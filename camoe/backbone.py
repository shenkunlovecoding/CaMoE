"""
RWKV-7 TimeMix Backbone (Turbo Edition with ClampW Kernel)
æ”¯æŒ BF16 åŸç”ŸåŠ é€Ÿ
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.utils.cpp_extension import load
import math
import os
from typing import Tuple

# ==================== CUDA Kernel ====================
HEAD_SIZE = 64
CHUNK_LEN = 16
USE_CUDA = False
RUN_CUDA_RWKV7 = None
CUDA_KERNEL_EXPECTS_FP32 = False
W_SCALE = -math.exp(-0.5)

def init_rwkv7_cuda():
    r"""init_rwkv7_cuda() -> None

    ç¼–è¯‘å¹¶æ³¨å†Œ RWKV7 è‡ªå®šä¹‰ CUDA ç®—å­ã€‚

    æˆåŠŸåä¼šè®¾ç½®å…¨å±€ ``USE_CUDA=True`` å¹¶ç»‘å®š ``RUN_CUDA_RWKV7``ã€‚
    """
    global USE_CUDA, RUN_CUDA_RWKV7, CUDA_KERNEL_EXPECTS_FP32
    
    try:
        curr_dir = os.path.dirname(os.path.abspath(__file__))
        cuda_dir = os.path.join(curr_dir, 'cuda')
        
        # æ–°æ–‡ä»¶å
        cu_file = os.path.join(cuda_dir, 'rwkv7_clampw.cu')
        cpp_file = os.path.join(cuda_dir, 'rwkv7_clampw.cpp')
        
        if not (os.path.exists(cu_file) and os.path.exists(cpp_file)):
            print(f"âš ï¸ CUDA files not found: {cu_file}")
            print("   Using Fallback (Slow FP32)...")
            return
        
        # ç¼–è¯‘æ ‡å¿—ï¼šå¦‚æœä¸åŠ  -D_FP32_ï¼Œé»˜è®¤å°±æ˜¯ BF16
        # æ³¨æ„ï¼šHEAD_SIZE å’Œ CHUNK_LEN ä¾ç„¶è¦ä¼ 
        use_fast_math = os.environ.get("CAMOE_DISABLE_FAST_MATH", "0") != "1"
        force_fp32_kernel = os.environ.get("CAMOE_FORCE_FP32_KERNEL", "0") == "1"

        flags = [
            '-res-usage', 
            f'-D_N_={HEAD_SIZE}', 
            f"-D_CHUNK_LEN_={CHUNK_LEN}", 
            "-O3",  
            "--extra-device-vectorization"
        ]
        if use_fast_math:
            flags.append("--use_fast_math")
        if force_fp32_kernel:
            flags.append("-D_FP32_")
        cxx_flags = ["-D_FP32_"] if force_fp32_kernel else []
        print(f"ğŸ”§ RWKV CUDA flags: fast_math={use_fast_math}, fp32_kernel={force_fp32_kernel}")
        
        ext_name = "rwkv7_clampw"
        load(
            name=ext_name, 
            sources=[cu_file, cpp_file], 
            is_python_module=False, 
            verbose=True, 
            extra_cuda_cflags=flags,
            extra_cflags=cxx_flags,
        )
        # TORCH_LIBRARY namespace is fixed in rwkv7_clampw.cpp
        op_ns = torch.ops.rwkv7_clampw
        
        class RWKV7_ClampW(torch.autograd.Function):
            @staticmethod
            def forward(ctx, r, w, k, v, a, b):
                B, T, H, C = r.shape
                assert T % CHUNK_LEN == 0
                expected_dtype = torch.float32 if force_fp32_kernel else torch.bfloat16
                assert all(i.dtype == expected_dtype for i in [r, w, k, v, a, b]), "RWKV7 kernel dtype mismatch"
                assert all(i.is_contiguous() for i in [r, w, k, v, a, b]), "RWKV7 kernel needs contiguous tensors"
                
                # åˆ›å»ºè¾“å‡ºå’ŒçŠ¶æ€å¼ é‡
                # æ³¨æ„ï¼šBF16 Kernel è¦æ±‚è¾“å…¥è¾“å‡ºæ˜¯ BF16/FP16ï¼Œä½† State æ˜¯ FP32
                y = torch.empty_like(v)
                s = torch.empty(B, H, T//CHUNK_LEN, C, C, dtype=torch.float32, device=r.device)
                sa = torch.empty(B, T, H, C, dtype=torch.float32, device=r.device)
                
                op_ns.forward(r, w, k, v, a, b, y, s, sa)
                ctx.save_for_backward(r, w, k, v, a, b, s, sa)
                return y
            
            @staticmethod
            def backward(ctx, dy):
                expected_dtype = torch.float32 if force_fp32_kernel else torch.bfloat16
                assert dy.dtype == expected_dtype, "RWKV7 backward dtype mismatch"
                assert dy.is_contiguous(), "RWKV7 backward needs contiguous dy"
                r, w, k, v, a, b, s, sa = ctx.saved_tensors
                dr, dw, dk, dv, da, db = [torch.empty_like(x) for x in [r, w, k, v, a, b]]
                op_ns.backward(r, w, k, v, a, b, dy, s, sa, dr, dw, dk, dv, da, db)
                return dr, dw, dk, dv, da, db
        
        def _run_cuda(r, w, k, v, a, b):
            B, T, HC = r.shape
            H = HC // HEAD_SIZE
            orig_dtype = r.dtype
            target_dtype = torch.float32 if CUDA_KERNEL_EXPECTS_FP32 else torch.bfloat16
            
            # View å˜æ¢
            # å…³é”®æ”¹åŠ¨ï¼šä¸å†å¼ºåˆ¶è½¬ .float() (FP32)
            # ä¿æŒåŸæœ‰çš„ dtype (BF16)ï¼Œåªåš contiguous å’Œ view
            r, w, k, v, a, b = [
                x.to(dtype=target_dtype).contiguous().view(B, T, H, HEAD_SIZE)
                for x in [r, w, k, v, a, b]
            ]
            
            out = RWKV7_ClampW.apply(r, w, k, v, a, b)
            out = out.view(B, T, HC).to(orig_dtype)
            return out
        
        RUN_CUDA_RWKV7 = _run_cuda
        USE_CUDA = True
        CUDA_KERNEL_EXPECTS_FP32 = force_fp32_kernel
        print(
            f"âœ… RWKV-7 CUDA Kernel (ClampW + {'FP32' if force_fp32_kernel else 'BF16'}) Ready"
        )
        
    except Exception as e:
        print(f"âš ï¸ CUDA init failed: {e}")


# ==================== RWKV-7 TimeMix ====================
class RWKV7_TimeMix(nn.Module):
    """
    RWKV-7 TimeMix (Attention equivalent)
    å®Œæ•´å®ç°åŠ¨æ€çŠ¶æ€æ¼”åŒ–
    """
    
    def __init__(self, n_embd: int, n_layer: int, layer_id: int, head_size: int = 64):
        super().__init__()
        self.layer_id = layer_id
        self.n_layer = n_layer
        self.head_size = head_size
        self.n_head = n_embd // head_size
        self.n_embd = n_embd
        
        assert n_embd % head_size == 0
        
        C = n_embd
        H = self.n_head
        N = head_size
        
        with torch.no_grad():
            ratio_0_to_1 = layer_id / max(n_layer - 1, 1)
            ratio_1_to_almost0 = 1.0 - (layer_id / n_layer)
            
            ddd = torch.ones(1, 1, C)
            for i in range(C):
                ddd[0, 0, i] = i / C
            
            self.x_r = nn.Parameter(1.0 - torch.pow(ddd, 0.2 * ratio_1_to_almost0))
            self.x_w = nn.Parameter(1.0 - torch.pow(ddd, 0.9 * ratio_1_to_almost0))
            self.x_k = nn.Parameter(1.0 - torch.pow(ddd, 0.7 * ratio_1_to_almost0))
            self.x_v = nn.Parameter(1.0 - torch.pow(ddd, 0.7 * ratio_1_to_almost0))
            self.x_a = nn.Parameter(1.0 - torch.pow(ddd, 0.9 * ratio_1_to_almost0))
            self.x_g = nn.Parameter(1.0 - torch.pow(ddd, 0.2 * ratio_1_to_almost0))
            
            def ortho_init(x, scale):
                shape = x.shape
                if len(shape) == 2:
                    gain = math.sqrt(shape[0] / shape[1]) if shape[0] > shape[1] else 1
                    nn.init.orthogonal_(x, gain=gain * scale)
                return x
            
            # æŒ‰RWKV-7è§„èŒƒè®¾ç½®LORAç»´åº¦
            D_DECAY_LORA = max(32, int(round((2.5 * (C ** 0.5)) / 32) * 32))
            D_AAA_LORA = max(32, int(round((2.5 * (C ** 0.5)) / 32) * 32))
            D_MV_LORA = max(32, int(round((1.7 * (C ** 0.5)) / 32) * 32))
            D_GATE_LORA = max(32, int(round((5 * (C ** 0.5)) / 32) * 32))
            
            # Decay
            www = torch.zeros(C)
            zigzag = torch.zeros(C)
            linear = torch.zeros(C)
            for n in range(C):
                linear[n] = n / (C - 1) - 0.5
                zigzag[n] = ((n % N) - ((N - 1) / 2)) / ((N - 1) / 2)
                zigzag[n] = zigzag[n] * abs(zigzag[n])
                www[n] = -6 + 6 * (n / (C - 1)) ** (1 + 1 * ratio_0_to_1 ** 0.3)
            
            self.w0 = nn.Parameter(www.reshape(1, 1, C) + 0.5 + zigzag * 2.5)
            self.w1 = nn.Parameter(torch.zeros(C, D_DECAY_LORA))
            self.w2 = nn.Parameter(ortho_init(torch.zeros(D_DECAY_LORA, C), 0.1))
            
            # AAA (in-context learning rate)
            self.a0 = nn.Parameter(torch.zeros(1, 1, C) - 0.19 + zigzag * 0.3 + linear * 0.4)
            self.a1 = nn.Parameter(torch.zeros(C, D_AAA_LORA))
            self.a2 = nn.Parameter(ortho_init(torch.zeros(D_AAA_LORA, C), 0.1))
            
            # Value residual
            self.v0 = nn.Parameter(torch.zeros(1, 1, C) + 0.73 - linear * 0.4)
            self.v1 = nn.Parameter(torch.zeros(C, D_MV_LORA))
            self.v2 = nn.Parameter(ortho_init(torch.zeros(D_MV_LORA, C), 0.1))
            
            # Gate
            self.g1 = nn.Parameter(torch.zeros(C, D_GATE_LORA))
            self.g2 = nn.Parameter(ortho_init(torch.zeros(D_GATE_LORA, C), 0.1))
            
            # Key normalization
            self.k_k = nn.Parameter(torch.zeros(1, 1, C) + 0.71 - linear * 0.1)
            self.k_a = nn.Parameter(torch.zeros(1, 1, C) + 1.02)
            self.r_k = nn.Parameter(torch.zeros(H, N) - 0.04)
        
        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))
        self.receptance = nn.Linear(C, C, bias=False)
        self.key = nn.Linear(C, C, bias=False)
        self.value = nn.Linear(C, C, bias=False)
        self.output = nn.Linear(C, C, bias=False)
        self.ln_x = nn.GroupNorm(H, C, eps=64e-5)
        
        # åˆå§‹åŒ–æƒé‡
        with torch.no_grad():
            self.receptance.weight.data.uniform_(-0.5/(C**0.5), 0.5/(C**0.5))
            self.key.weight.data.uniform_(-0.05/(C**0.5), 0.05/(C**0.5))
            self.value.weight.data.uniform_(-0.5/(C**0.5), 0.5/(C**0.5))
            self.output.weight.data.zero_()

    @staticmethod
    def _report_nonfinite(x: torch.Tensor, name: str, layer_id: int) -> None:
        if torch.isfinite(x).all():
            return
        with torch.no_grad():
            bad = ~torch.isfinite(x)
            bad_count = int(bad.sum().item())
            total = x.numel()
            finite_x = x[torch.isfinite(x)]
            if finite_x.numel() > 0:
                vmin = float(finite_x.min().item())
                vmax = float(finite_x.max().item())
            else:
                vmin = float("nan")
                vmax = float("nan")
            print(
                f"âŒ NaNDebug-TimeMix | layer={layer_id} | tensor={name} | "
                f"bad={bad_count}/{total} | finite_min={vmin:.6e} | finite_max={vmax:.6e}"
            )

    @staticmethod
    def _run_torch_fallback(
        r: torch.Tensor,
        w_raw: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        a: torch.Tensor,
        b: torch.Tensor,
        n_head: int,
        head_size: int,
    ) -> torch.Tensor:
        r"""çº¯ PyTorch çš„ TimeMix é€’æ¨å®ç°ï¼Œç”¨äº CUDA kernel å¼‚å¸¸æ—¶çš„åº”æ€¥è·¯å¾„ã€‚"""
        B, T, C = r.shape
        H, N = n_head, head_size
        r = r.view(B, T, H, N).float()
        w_raw = w_raw.view(B, T, H, N).float()
        k = k.view(B, T, H, N).float()
        v = v.view(B, T, H, N).float()
        a = a.view(B, T, H, N).float()
        b = b.view(B, T, H, N).float()

        # ä¸ CUDA kernel ä¿æŒåŒæ ·çš„ w å˜æ¢
        w = torch.exp(W_SCALE / (1.0 + torch.exp(-w_raw)))

        state = torch.zeros(B, H, N, N, device=r.device, dtype=torch.float32)
        ys = []
        for t in range(T):
            rt = r[:, t]  # [B,H,N]
            wt = w[:, t]
            kt = k[:, t]
            vt = v[:, t]
            at = a[:, t]
            bt = b[:, t]

            sa = (state * at.unsqueeze(-2)).sum(dim=-1)  # [B,H,N_i]
            state = (
                state * wt.unsqueeze(-2)
                + sa.unsqueeze(-1) * bt.unsqueeze(-2)
                + vt.unsqueeze(-1) * kt.unsqueeze(-2)
            )
            yt = (state * rt.unsqueeze(-2)).sum(dim=-1)  # [B,H,N_i]
            ys.append(yt)

        y = torch.stack(ys, dim=1).reshape(B, T, C)
        return y.to(r.dtype)
    
    def forward(
        self, x: torch.Tensor, v_first: torch.Tensor = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        r"""forward(x, v_first=None) -> Tuple[Tensor, Tensor, Tensor]

        æ‰§è¡Œ RWKV-7 TimeMix ä¸»å¹²å‰å‘ã€‚

        Args:
          x (Tensor): å½¢çŠ¶ ``[B, T, C]``ã€‚
          v_first (Tensor, optional): é¦–å±‚ value ç¼“å­˜ã€‚Default: ``None``ã€‚

        Returns:
          Tuple[Tensor, Tensor, Tensor]:
          è¾“å‡º ``out``ã€æ›´æ–°åçš„ ``v_first``ã€ä»¥åŠä¸­é—´çŠ¶æ€ ``state_representation``ã€‚
        """
        B, T, C = x.size()
        H = self.n_head
        N = self.head_size
        
        # Token shift
        xx = self.time_shift(x) - x
        
        xr = x + xx * self.x_r
        xw = x + xx * self.x_w
        xk = x + xx * self.x_k
        xv = x + xx * self.x_v
        xa = x + xx * self.x_a
        xg = x + xx * self.x_g
        
        r = self.receptance(xr)
        w = -F.softplus(-(self.w0 + torch.tanh(xw @ self.w1) @ self.w2)) - 0.5
        k = self.key(xk)
        v = self.value(xv)
        
        # Value residual (layer 0 stores, others use)
        if self.layer_id == 0:
            v_first = v.clone()
        else:
            if v_first is not None:
                v = v + (v_first - v) * torch.sigmoid(self.v0 + (xv @ self.v1) @ self.v2)
        
        a = torch.sigmoid(self.a0 + (xa @ self.a1) @ self.a2)
        g = torch.sigmoid(xg @ self.g1) @ self.g2
        
        kk = k * self.k_k
        kk = F.normalize(kk.view(B, T, H, -1), dim=-1, p=2.0).view(B, T, C)
        k = k * (1 + (a - 1) * self.k_a)
        
        # RWKV-7 åŠ¨æ€çŠ¶æ€æ¼”åŒ–
        use_fallback = os.environ.get("CAMOE_FORCE_TIMEMIX_FALLBACK", "0") == "1"
        if (not use_fallback) and USE_CUDA and RUN_CUDA_RWKV7 is not None:
            x_att = RUN_CUDA_RWKV7(r, w, k, v, -kk, kk * a)
        else:
            x_att = self._run_torch_fallback(r, w, k, v, -kk, kk * a, self.n_head, self.head_size)
        if os.environ.get("CAMOE_NAN_DEBUG", "0") == "1":
            self._report_nonfinite(x_att, "x_att_raw", self.layer_id)
        if os.environ.get("CAMOE_SANITIZE_TIMEMIX_OUT", "0") == "1":
            x_att = torch.nan_to_num(x_att, nan=0.0, posinf=0.0, neginf=0.0)

            
        state_representation = x_att.clone() 
        x_att = self.ln_x(x_att.view(B * T, C)).view(B, T, C)
        if os.environ.get("CAMOE_NAN_DEBUG", "0") == "1":
            self._report_nonfinite(x_att, "x_att_ln", self.layer_id)
        
        # Bonus term
        x_att = x_att + (
            (r.view(B, T, H, -1) * k.view(B, T, H, -1) * self.r_k)
            .sum(dim=-1, keepdim=True) * v.view(B, T, H, -1)
        ).view(B, T, C)
        
        out = self.output(x_att * g)
        if os.environ.get("CAMOE_NAN_DEBUG", "0") == "1":
            self._report_nonfinite(out, "att_out", self.layer_id)
        
        return out, v_first , state_representation


# ==================== DeepEmbed + DeepEmbedAttention (from RWKV-7 Goose) ====================
# æ€æƒ³ï¼šç”¨ã€Œè¯è¡¨çº§ã€åµŒå…¥å¯¹ K/V åšé€ token è°ƒåˆ¶ï¼Œå¹¶å¢åŠ ä¸€æ¡å› æœ Self-Attention åˆ†æ”¯ï¼ˆQ/K/V token-shift + soft-capï¼‰


class SharedDeepEmbed(nn.Module):
    """
    å…¨æ¨¡å‹å…±äº«çš„ DeepEmbed è¡¨ã€‚
    é€šè¿‡è·¨å±‚å…±äº«ï¼Œå°†è¯è¡¨å‚æ•°ä» O(n_layer * vocab * dim) é™ä¸º O(vocab * dim)ã€‚
    """

    def __init__(self, vocab_size: int, k_dim: int, v_dim: int):
        super().__init__()
        self.k_emb = nn.Embedding(vocab_size, k_dim)
        self.v_emb = nn.Embedding(vocab_size, v_dim)
        nn.init.normal_(self.k_emb.weight, std=0.02)
        nn.init.normal_(self.v_emb.weight, std=0.02)

    def forward(self, idx: torch.Tensor):
        r"""forward(idx) -> Tuple[Tensor, Tensor]

        Args:
          idx (Tensor): å½¢çŠ¶ ``[B, T]`` çš„ token idã€‚

        Returns:
          Tuple[Tensor, Tensor]: ``k_emb(idx)`` ä¸ ``v_emb(idx)``ã€‚
        """
        return self.k_emb(idx), self.v_emb(idx)


class DeepEmbedAttention(nn.Module):
    """
    DeepEmbedAttention (DEA)ï¼šä¸ TimeMix å¹¶è¡Œçš„å› æœ Self-Attention åˆ†æ”¯ã€‚
    - Q/K/V ç» DeepEmbedï¼ˆk_emb, v_emb æŒ‰ token id æŸ¥è¡¨ï¼‰è°ƒåˆ¶
    - Q/K/V åš token-shiftï¼ˆä¸ä¸Šä¸€ä½ç½®æ··åˆï¼‰
    - ä½¿ç”¨ soft-cap: 64 * tanh(scores / 1024) å† softmax
    """
    
    def __init__(
        self,
        n_embd: int,
        n_layer: int,
        layer_id: int,
        head_size: int,
        vocab_size: int,
        shared_deep_embed: nn.Module = None,
        q_dim: int = 256,
        kv_dim: int = 32,
        score_scale: float = 1024.0,
        cap_scale: float = 64.0,
    ):
        super().__init__()
        self.layer_id = layer_id
        self.n_embd = n_embd
        self.q_dim = min(q_dim, n_embd)
        self.kv_dim = min(kv_dim, self.q_dim)
        self.score_scale = float(score_scale)
        self.cap_scale = float(cap_scale)
        C = n_embd

        # RWKV-8 é£æ ¼ä½ç»´è·¯å¾„ï¼šQ(C->q_dim), K/V(C->kv_dim->up)
        self.qq = nn.Linear(C, self.q_dim, bias=False)
        self.k = nn.Linear(C, self.kv_dim, bias=False)
        self.k_up = nn.Linear(self.kv_dim, self.q_dim, bias=False)
        self.v = nn.Linear(C, self.kv_dim, bias=False)
        self.v_up = nn.Linear(self.kv_dim, C, bias=False)

        # DeepEmbedï¼šé»˜è®¤ä½¿ç”¨è·¨å±‚å…±äº«è¡¨ï¼›æœªæä¾›åˆ™å›é€€ä¸ºå±‚å†…è¡¨
        self.shared_deep_embed = shared_deep_embed
        if self.shared_deep_embed is None:
            self.k_emb = nn.Embedding(vocab_size, self.q_dim)
            self.v_emb = nn.Embedding(vocab_size, C)
            nn.init.normal_(self.k_emb.weight, std=0.02)
            nn.init.normal_(self.v_emb.weight, std=0.02)
        else:
            self.k_emb = None
            self.v_emb = None
        
        # Token-shift ç³»æ•°ï¼ˆä¸ backbone time_shift ç±»ä¼¼ï¼‰
        with torch.no_grad():
            ratio_1_to_almost0 = 1.0 - (layer_id / max(n_layer, 1))
            ddd_q = torch.ones(1, 1, self.q_dim)
            ddd_v = torch.ones(1, 1, C)
            for i in range(self.q_dim):
                ddd_q[0, 0, i] = i / max(self.q_dim - 1, 1)
            for i in range(C):
                ddd_v[0, 0, i] = i / max(C - 1, 1)
            self.x_q = nn.Parameter(1.0 - torch.pow(ddd_q, 0.2 * ratio_1_to_almost0))
            self.x_k = nn.Parameter(1.0 - torch.pow(ddd_q, 0.7 * ratio_1_to_almost0))
            self.x_v = nn.Parameter(1.0 - torch.pow(ddd_v, 0.7 * ratio_1_to_almost0))

        self.ln_q = nn.LayerNorm(self.q_dim)
        self.ln_k = nn.LayerNorm(self.q_dim)
        self.ln_v = nn.LayerNorm(C)
        
        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))
        
        with torch.no_grad():
            self.qq.weight.data.uniform_(-0.5 / (C ** 0.5), 0.5 / (C ** 0.5))
            self.k.weight.data.uniform_(-0.05 / (C ** 0.5), 0.05 / (C ** 0.5))
            self.k_up.weight.data.uniform_(-0.05 / (self.kv_dim ** 0.5), 0.05 / (self.kv_dim ** 0.5))
            self.v.weight.data.uniform_(-0.05 / (C ** 0.5), 0.05 / (C ** 0.5))
            self.v_up.weight.data.uniform_(-0.05 / (self.kv_dim ** 0.5), 0.05 / (self.kv_dim ** 0.5))
    
    def forward(self, x: torch.Tensor, idx: torch.Tensor) -> torch.Tensor:
        r"""forward(x, idx) -> Tensor

        Args:
          x (Tensor): å½¢çŠ¶ ``[B, T, C]``ã€‚
          idx (Tensor): å½¢çŠ¶ ``[B, T]`` çš„ token idã€‚

        Returns:
          Tensor: å½¢çŠ¶ ``[B, T, C]`` çš„ DEA åˆ†æ”¯è¾“å‡ºã€‚
        """
        B, T, C = x.shape
        if self.shared_deep_embed is not None:
            k_emb, v_emb = self.shared_deep_embed(idx)
        else:
            k_emb = self.k_emb(idx)
            v_emb = self.v_emb(idx)

        # Q
        q = self.qq(x)
        q_prev = self.time_shift(q) - q
        q = q + q_prev * self.x_q
        q = self.ln_q(q)
        
        # K: C -> kv_dim -> q_dimï¼Œå†ç”± token çº§ DeepEmbed è°ƒåˆ¶
        k = self.k_up(self.k(x)) * k_emb
        k_prev = self.time_shift(k) - k
        k = k + k_prev * self.x_k
        k = self.ln_k(k)
        
        # V: C -> kv_dim -> Cï¼Œä¿ç•™ tanh éçº¿æ€§åè°ƒåˆ¶
        v = torch.tanh(self.v_up(self.v(x))) * v_emb
        v_prev = self.time_shift(v) - v
        v = v + v_prev * self.x_v
        v = self.ln_v(v)
        
        # Causal self-attention with soft-cap
        scores = (q @ k.transpose(-2, -1)) / self.score_scale
        scores = self.cap_scale * torch.tanh(scores)
        causal_mask = torch.triu(
            torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1
        )
        scores = scores.masked_fill(causal_mask.unsqueeze(0), float("-inf"))
        attn = F.softmax(scores.float(), dim=-1).to(scores.dtype)
        out = attn @ v
        return out
