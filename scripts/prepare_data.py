"""
CaMoE v18 Data Preprocessor (Ultimate Edition)
åŠŸèƒ½:
1. åŠ è½½å¤šä¸ªæ•°æ®æº (TinyStories, Ultrachat, Cosmo, MiniPile)
2. æ¸…æ´— & æ ¼å¼åŒ– (User/Assistant)
3. é‡‡æ · & æ··åˆ (Interleave)
4. Tokenize & Packing (Rust RWKV Tokenizer)
5. ä¿å­˜ä¸ºå•ä¸€æ•°æ®é›†ï¼Œä¾› train.py ç›´æ¥è¯»å–
"""

import os
import argparse
import multiprocessing
from typing import Any, Dict
import re
from datasets import load_dataset, interleave_datasets
import pyrwkv_tokenizer

# ================= é…ç½® =================
# å®šä¹‰ä½ çš„é…æ–¹ (Recipe)
# æ ¼å¼: "name": (path_or_id, split, mode, probability)
# mode: "raw" (çº¯æ–‡æœ¬) æˆ– "chat" (å¯¹è¯)
DATA_RECIPE = {
    "tinystories": ("roneneldan/TinyStories", "train[:10%]", "raw", 0.4), # å–10%
    "cosmopedia":  ("HuggingFaceTB/cosmopedia-100k", "train", "raw", 0.3), # å…¨é‡
    "ultrachat":   ("HuggingFaceH4/ultrachat_200k", "train_sft", "chat", 0.2),
    "dailydialog": ("roskoN/dailydialog", "train", "chat", 0.1),
}

# å¦‚æœ Ultrachat è¿˜æ˜¯é‚£ä¸ª list æ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†
# è¿™é‡Œå‡è®¾ Ultrachat æ˜¯æ ‡å‡†çš„ HF æ ¼å¼

def get_args() -> argparse.Namespace:
    r"""get_args() -> argparse.Namespace

    è§£ææ•°æ®é¢„å¤„ç†å‘½ä»¤è¡Œå‚æ•°ã€‚

    Returns:
      argparse.Namespace: è§£æåçš„å‚æ•°å¯¹è±¡ã€‚
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--save_path", type=str, default="./data/camoe_mix_v1", help="ä¿å­˜è·¯å¾„")
    parser.add_argument("--ctx_len", type=int, default=1024)
    parser.add_argument("--num_proc", type=int, default=4, help="å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œå†…å­˜å°è®¾ä¸º2-4")
    parser.add_argument("--batch_size", type=int, default=100, help="Tokenizeæ‰¹æ¬¡å¤§å°ï¼Œå†…å­˜å°è®¾ä¸º50")
    return parser.parse_args()

def process_text(item: Dict[str, Any], mode: str = "raw") -> str:
    r"""process_text(item, mode="raw") -> str

    å°†ä¸åŒæ¥æºæ ·æœ¬æ ‡å‡†åŒ–ä¸ºè®­ç»ƒæ–‡æœ¬ã€‚

    Args:
      item (Dict[str, Any]): å•æ¡æ ·æœ¬ã€‚
      mode (str, optional): ``"raw"`` æˆ– ``"chat"``ã€‚Default: ``"raw"``ã€‚

    Returns:
      str: æ¸…æ´—åçš„æ–‡æœ¬ï¼›æ— æ•ˆæ ·æœ¬è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    text = ""
    
    # 1. å°è¯•è·å–å†…å®¹
    # Ultrachat 200k: 'messages' (list[{"role","content"}])
    # Ultrachat(old): 'data' (list)
    # DailyDialog: 'dialog' (list)
    # TinyStories/Cosmo: 'text' (str)
    
    raw = None
    if 'messages' in item:
        raw = item['messages']
    elif 'text' in item:
        raw = item['text']
    elif 'data' in item:
        raw = item['data']
    elif 'dialog' in item:
        raw = item['dialog']
    
    if raw is None: return ""

    # 2. æ ¼å¼åŒ–
    if isinstance(raw, list):
        # å¯¹è¯åˆ—è¡¨ -> Chat æ ¼å¼ï¼ˆå…¼å®¹ list[str] / list[dict]ï¼‰
        conversation = []
        for i, turn in enumerate(raw):
            if not turn:
                continue

            role = None
            content = None
            if isinstance(turn, dict):
                role = str(turn.get("role", "")).strip().lower()
                content = str(turn.get("content", "")).strip()
            else:
                content = str(turn).strip()

            if not content:
                continue

            content = content.replace('\r\n', '\n')
            content = re.sub(r'\n{2,}', '\n', content) # å»é™¤å¤šä½™æ¢è¡Œ

            if mode == "chat":
                if role in ("user", "assistant", "system"):
                    if role == "system":
                        # system ç»Ÿä¸€å¹¶å…¥æ–‡æœ¬ï¼Œä½†æ ‡æ³¨è§’è‰²ï¼Œä¾¿äºæ¨¡å‹æ„ŸçŸ¥
                        line = f"System: {content}"
                    elif role == "user":
                        line = f"User: {content}"
                    else:
                        line = f"Assistant: {content}"
                else:
                    # æ— è§’è‰²æ—¶å›é€€åˆ°äº¤æ›¿è§„åˆ™
                    guessed = "User" if i % 2 == 0 else "Assistant"
                    line = f"{guessed}: {content}"
                conversation.append(line)
            else:
                conversation.append(content)
        text = "\n\n".join(conversation)
        
    elif isinstance(raw, str):
        # çº¯æ–‡æœ¬
        text = raw.strip().replace(" __eou__ ", "\n")
        if mode == "chat":
            # å¦‚æœæ˜¯ Chat æ¨¡å¼ä½†åŸå§‹æ˜¯æ–‡æœ¬ï¼Œå°è¯•è½¬æ¢(ç®€æ˜“ç‰ˆ)
            pass 
            
    return text

# å…¨å±€ Tokenizer (Worker ç”¨)
tokenizer = None
def init_tokenizer():
    r"""init_tokenizer() -> None

    åˆå§‹åŒ–å…¨å±€ RWKV tokenizerï¼Œä¾›å¤šè¿›ç¨‹ worker è°ƒç”¨ã€‚
    """
    global tokenizer
    tokenizer = pyrwkv_tokenizer.RWKVTokenizer()

def tokenize_and_pack(batch: Dict[str, Any], ctx_len: int = 1024) -> Dict[str, Any]:
    r"""tokenize_and_pack(batch, ctx_len=1024) -> Dict[str, Any]

    å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶æ‰“åŒ…æˆå›ºå®šé•¿åº¦åºåˆ—ã€‚

    Args:
      batch (Dict[str, Any]): batched æ ·æœ¬å­—å…¸ï¼Œéœ€åŒ…å« ``text_processed``ã€‚
      ctx_len (int, optional): åºåˆ—é•¿åº¦ã€‚Default: ``1024``ã€‚

    Returns:
      Dict[str, Any]: åŒ…å« ``input_ids`` åˆ—çš„æ–°æ‰¹æ¬¡ã€‚
    """
    global tokenizer
    if tokenizer is None:
        # åœ¨ datasets çš„å¤šè¿›ç¨‹ map worker ä¸­åšæ‡’åŠ è½½åˆå§‹åŒ–ï¼Œé¿å… NoneType.encode æŠ¥é”™
        init_tokenizer()

    texts = batch['text_processed']
    if not texts: return {"input_ids": []}
    
    # æµå¼å¤„ç†ï¼šè¾¹tokenizeè¾¹packï¼Œä¸ç¼“å­˜å…¨éƒ¨token
    chunks = []
    current_chunk = []
    
    for text in texts:
        ids = tokenizer.encode(text)
        ids.append(0)  # EOS
        
        for token in ids:
            current_chunk.append(token)
            if len(current_chunk) == ctx_len:
                chunks.append(current_chunk)
                current_chunk = []
        
        # å†…å­˜ä¿æŠ¤ï¼šé™åˆ¶ç¼“å­˜chunkæ•°é‡
        if len(chunks) > 10000:
            break
    
    # ä¸¢å¼ƒå°¾éƒ¨ä¸è¶³ctx_lençš„token
    return {"input_ids": chunks}

def main() -> None:
    r"""main() -> None

    æ··åˆå¤šæºæ•°æ®å¹¶å¯¼å‡ºä¸ºå¯ç›´æ¥è®­ç»ƒçš„æ•°æ®é›†æ ¼å¼ã€‚
    """
    args = get_args()
    print(f"ğŸš€ Preparing Mixed Dataset -> {args.save_path}")
    
    datasets = []
    probs = []
    
    # 1. åŠ è½½å¹¶æ ‡å‡†åŒ–æ‰€æœ‰æº
    for name, (path, split, mode, prob) in DATA_RECIPE.items():
        print(f"  - Loading {name} ({split})...")
        try:
            ds = load_dataset(path, split=split)
            
            # Map: ç»Ÿä¸€è½¬æˆ text_processed åˆ—
            # è¿™é‡Œæˆ‘ä»¬ç”¨å•è¿›ç¨‹ map å¿«é€Ÿå¤„ç†æ–‡æœ¬æ ¼å¼åŒ–ï¼Œæˆ–è€…å¤šè¿›ç¨‹
            ds = ds.map(
                lambda x: {"text_processed": process_text(x, mode)},
                remove_columns=ds.column_names, # ç§»é™¤åŸå§‹åˆ—ï¼Œåªç•™ text_processed
                num_proc=args.num_proc,
                desc=f"Formatting {name}"
            )
            
            # è¿‡æ»¤ç©ºæ ·æœ¬
            ds = ds.filter(lambda x: len(x['text_processed']) > 0)
            
            datasets.append(ds)
            probs.append(prob)
            print(f"    -> {len(ds)} samples ready.")
            
        except Exception as e:
            print(f"âš ï¸ Failed to load {name}: {e}")
    
    if not datasets:
        print("âŒ No datasets loaded!")
        return

    # 2. æ··åˆ (Interleave)
    # å½’ä¸€åŒ–æ¦‚ç‡
    total_p = sum(probs)
    probs = [p/total_p for p in probs]
    
    print(f"ğŸ¥£ Mixing datasets with probs: {probs}...")
    mixed_ds = interleave_datasets(datasets, probabilities=probs, seed=42, stopping_strategy="first_exhausted")
    # æ³¨æ„ï¼šfirst_exhausted å¯èƒ½ä¼šä¸¢å¼ƒå¤§åŠå…¶æ•°æ®ï¼Œall_exhausted ä¼šè¿‡é‡‡æ ·å°æ•°æ®
    # å¯¹äºé¢„è®­ç»ƒï¼Œé€šå¸¸ç”¨ probabilities é‡‡æ ·å³å¯ï¼Œä¸ç”¨å¤ªåœ¨æ„ epoch è¾¹ç•Œ
    # å¦‚æœæƒ³â€œå­˜ä¸‹æ¥â€ï¼Œå»ºè®®ç”¨ stopping_strategy="first_exhausted" ç„¶åè®¾ä¸ª limit?
    # æˆ–è€…ç›´æ¥ç”± map å¤„ç†æ—¶å°±æ˜¯æµå¼çš„ã€‚
    
    # è¿™é‡Œçš„ mixed_ds æ˜¯ Lazy çš„ã€‚
    
    # 3. Tokenize & Pack (æœ€ç»ˆå¤„ç†)
    print("âš™ï¸  Tokenizing & Packing (This will take a while)...")
    final_ds = mixed_ds.map(
        lambda x: tokenize_and_pack(x, args.ctx_len),
        batched=True,
        batch_size=args.batch_size,
        num_proc=args.num_proc,
        remove_columns=["text_processed"],
        desc="Final Packing"
    )
    
    # 4. ä¿å­˜
    print(f"ğŸ’¾ Saving to disk...")
    final_ds.save_to_disk(args.save_path)
    
    total_tokens = len(final_ds) * args.ctx_len
    print(f"âœ… Done! Total Tokens: {total_tokens / 1e9:.4f} B")

if __name__ == "__main__":
    main()
