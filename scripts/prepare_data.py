"""
Preprocess Script for CaMoE v18 (Rust RWKV Tokenizer)
æ”¯æŒæœ¬åœ°æ–‡ä»¶ (json/txt/csv) å’Œ HF åœ¨çº¿æ•°æ®é›†
"""
import os
import argparse
from datasets import load_dataset
import pyrwkv_tokenizer 

def get_args():
    parser = argparse.ArgumentParser()
    # æ”¯æŒæœ¬åœ°è·¯å¾„æˆ– HF ID
    parser.add_argument("--dataset", type=str, required=True, 
                        help="Path to local file (.json/.txt) or HF Dataset ID")
    parser.add_argument("--save_path", type=str, required=True)
    parser.add_argument("--ctx_len", type=int, default=1024)
    parser.add_argument("--num_proc", type=int, default=16)
    
    # æ˜¾å¼æŒ‡å®šæ–‡æœ¬åˆ—å (å¯é€‰)
    parser.add_argument("--text_col", type=str, default=None)
    return parser.parse_args()

def process_batch(batch, text_col=None):
    tokenizer = pyrwkv_tokenizer.RWKVTokenizer()
    
    if text_col is None:
        keys = batch.keys()
        if "text" in keys: text_col = "text"
        elif "content" in keys: text_col = "content"
        elif "dialog" in keys: text_col = "dialog"
        else: text_col = list(keys)[0]
    
    raw_data = batch[text_col]
    
    texts = []
    for item in raw_data:
        if isinstance(item, str):
            # DailyDialog ä¿®å¤: æ›¿æ¢ __eou__ ä¸ºæ¢è¡Œ
            clean_text = item.replace(" __eou__ ", "\n").replace("__eou__", "\n").strip()
            texts.append(clean_text)
        elif isinstance(item, list):
            texts.append("\n".join(str(x) for x in item))
        else:
            texts.append(str(item))
            
    if not texts: return {"input_ids": []}
    
    encoded_batch = tokenizer.encode_batch(texts)
    
    flat_ids = []
    for ids in encoded_batch:
        flat_ids.extend(ids)
        flat_ids.append(0) # EOS
        
    chunks = []
    CTX_LEN = 1024
    for i in range(0, len(flat_ids), CTX_LEN):
        chunk = flat_ids[i:i+CTX_LEN]
        if len(chunk) == CTX_LEN:
            chunks.append(chunk)
            
    return {"input_ids": chunks}

def main():
    args = get_args()
    print(f"ğŸš€ Processing: {args.dataset}")
    
    # 1. æ™ºèƒ½åŠ è½½é€»è¾‘
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶
    if os.path.exists(args.dataset):
        ext = args.dataset.split('.')[-1]
        if ext in ['json', 'jsonl']:
            print("ğŸ“‚ Detected Local JSON/JSONL file")
            ds = load_dataset("json", data_files=args.dataset, split="train")
        elif ext == 'txt':
            print("ğŸ“‚ Detected Local TXT file")
            ds = load_dataset("text", data_files=args.dataset, split="train")
        elif os.path.isdir(args.dataset):
             print("ğŸ“‚ Detected Local Dataset Folder (Arrow/HF format)")
             from datasets import load_from_disk
             ds = load_from_disk(args.dataset)
        else:
            # å°è¯•ä½œä¸º CSV
            ds = load_dataset("csv", data_files=args.dataset, split="train")
    else:
        # å‡è®¾æ˜¯ HF Hub ID
        print("â˜ï¸  Loading from HF Hub...")
        # å³ä½¿è¿™é‡Œä¸åŠ  trust_remote_codeï¼Œå¯¹äºæ ‡å‡† dataset (text, json) ä¹Ÿæ²¡é—®é¢˜
        # å¦‚æœæ˜¯ç‰¹æ®Š script datasetï¼Œå¯èƒ½è¿˜ä¼šæŒ‚ï¼Œä½†æˆ‘ä»¬ä¸»è¦ç”¨ json/text
        ds = load_dataset(args.dataset, split="train", trust_remote_code=True)

    print(f"ğŸ“Š Rows: {len(ds)}")
    
    # 2. Map å¤„ç†
    # ä½¿ç”¨ lambda ä¼ å…¥ text_col å‚æ•°
    tokenized_ds = ds.map(
        lambda x: process_batch(x, args.text_col),
        batched=True,
        batch_size=1000,
        num_proc=args.num_proc,
        remove_columns=ds.column_names,
        desc="Tokenizing"
    )
    
    tokenized_ds.save_to_disk(args.save_path)
    print(f"âœ… Saved to {args.save_path}")

if __name__ == "__main__":
    main()