"""
Preprocess Script for CaMoE v18 (Rust RWKV Tokenizer)
"""
import os
import argparse
from datasets import load_dataset
import pyrwkv_tokenizer # Rust åŠ é€Ÿç‰ˆ

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default="roneneldan/TinyStories")
    parser.add_argument("--save_path", type=str, default="./data/TinyStories_rwkv_processed")
    parser.add_argument("--ctx_len", type=int, default=1024)
    parser.add_argument("--num_proc", type=int, default=16) # Rustæœ¬èº«æœ‰å¤šçº¿ç¨‹ï¼Œè¿™é‡ŒPythonè¿›ç¨‹æ•°å¯ä»¥å°‘ç‚¹
    return parser.parse_args()

def process_batch(batch):
    # Rust Tokenizer åˆå§‹åŒ–æå¿«ï¼Œç›´æ¥åœ¨å‡½æ•°é‡Œæ
    tokenizer = pyrwkv_tokenizer.RWKVTokenizer()
    
    texts = batch["text"]
    # æ‰¹é‡ç¼–ç  (Rust å†…éƒ¨è‡ªå¸¦å¤šçº¿ç¨‹ä¼˜åŒ–)
    # æ³¨æ„ï¼špyrwkv_tokenizer çš„ encode_batch è¿”å›çš„æ˜¯ list of lists
    encoded_batch = tokenizer.encode_batch(texts)
    
    # Flatten & Add EOS (0 for RWKV?) 
    # RWKV world tokenizeré€šå¸¸æ²¡æœ‰æ˜¾å¼çš„EOSï¼Œæˆ–è€…ç”¨ 0ã€‚
    # æ£€æŸ¥ vocab å‘ç° 0 æ˜¯ <|endoftext|> ?? éœ€ç¡®è®¤ã€‚
    # å‡è®¾ 0 æ˜¯ EOSã€‚
    
    flat_ids = []
    for ids in encoded_batch:
        flat_ids.extend(ids)
        flat_ids.append(0) # EOS
        
    # Chunking
    chunks = []
    CTX_LEN = 1024 # éœ€ä»å¤–éƒ¨ä¼ å…¥æˆ–å†™æ­»
    for i in range(0, len(flat_ids), CTX_LEN):
        chunk = flat_ids[i:i+CTX_LEN]
        if len(chunk) == CTX_LEN:
            chunks.append(chunk)
            
    return {"input_ids": chunks}

def main():
    args = get_args()
    print(f"ğŸš€ Preprocessing {args.dataset} with Rust RWKV Tokenizer...")
    
    ds = load_dataset(args.dataset, split="train")
    
    # æ³¨æ„ï¼šå› ä¸º Rust tokenizer å†…éƒ¨æœ‰å¤šçº¿ç¨‹ï¼ŒPython å±‚é¢çš„ num_proc å¯ä»¥è®¾å°ä¸€ç‚¹ï¼Œæˆ–è€…è®¾ä¸º 1
    # å®é™…ä¸Š datasets çš„ map å¤šè¿›ç¨‹æ˜¯ process çº§ï¼ŒRust æ˜¯ thread çº§ï¼Œä¸¤è€…ç»“åˆå¯èƒ½æ›´å¥½ã€‚
    # å»ºè®® Python num_proc = cpu_count // 2
    
    tokenized_ds = ds.map(
        process_batch,
        batched=True,
        batch_size=1000,
        num_proc=args.num_proc,
        remove_columns=ds.column_names,
        desc="Tokenizing"
    )
    
    tokenized_ds.save_to_disk(args.save_path)
    print("âœ… Done!")

if __name__ == "__main__":
    main()