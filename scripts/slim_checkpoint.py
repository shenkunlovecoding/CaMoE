import argparse
import os
import tempfile
import shutil
from typing import Dict, Tuple

import torch


def parse_args() -> argparse.Namespace:
    r"""parse_args() -> argparse.Namespace

    è§£æžå‘½ä»¤è¡Œå‚æ•°ã€‚

    Returns:
      argparse.Namespace: å‚æ•°å¯¹è±¡ã€‚
    """
    parser = argparse.ArgumentParser(description="Slim CaMoE checkpoint for inference.")
    parser.add_argument(
        "--input",
        "-i",
        type=str,
        required=True,
        help="è¾“å…¥ checkpoint è·¯å¾„ï¼ˆè®­ç»ƒäº§ç‰©æˆ–çº¯ state_dictï¼‰ã€‚",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        required=True,
        help="è¾“å‡ºæŽ¨ç† pth è·¯å¾„ã€‚",
    )
    parser.add_argument(
        "--dtype",
        type=str,
        default="bf16",
        choices=["keep", "bf16", "fp16", "fp32"],
        help="æµ®ç‚¹æƒé‡å¯¼å‡ºç²¾åº¦ï¼ˆé»˜è®¤ bf16ï¼Œå¯æ˜¾è‘—å‡å°ä½“ç§¯ï¼‰ã€‚",
    )
    parser.add_argument(
        "--format",
        type=str,
        default="state_dict",
        choices=["state_dict", "checkpoint"],
        help="å¯¼å‡ºæ ¼å¼ï¼šstate_dict(æ›´å°) æˆ– checkpoint({'model': ...})ã€‚",
    )
    parser.add_argument(
        "--strip-prefix",
        type=str,
        nargs="*",
        default=[],
        help="å¯é€‰ï¼šç§»é™¤æŒ‡å®šå‰ç¼€çš„æƒé‡é”®ï¼ˆè°¨æ…Žä½¿ç”¨ï¼‰ã€‚ç¤ºä¾‹ --strip-prefix optimizer",
    )
    return parser.parse_args()


def _target_dtype(name: str):
    if name == "bf16":
        return torch.bfloat16
    if name == "fp16":
        return torch.float16
    if name == "fp32":
        return torch.float32
    return None


def _extract_state_dict(ckpt_obj) -> Dict[str, torch.Tensor]:
    if isinstance(ckpt_obj, dict) and "model" in ckpt_obj:
        return ckpt_obj["model"]
    if isinstance(ckpt_obj, dict):
        return ckpt_obj
    raise ValueError("Unsupported checkpoint format: expected dict or {'model': state_dict}.")


def _slim_state_dict(
    state_dict: Dict[str, torch.Tensor],
    dtype_name: str,
    strip_prefix: Tuple[str, ...],
) -> Dict[str, torch.Tensor]:
    target_dtype = _target_dtype(dtype_name)
    out: Dict[str, torch.Tensor] = {}

    for k, v in state_dict.items():
        if any(k.startswith(p) for p in strip_prefix):
            continue

        if torch.is_tensor(v):
            t = v.detach().cpu()
            if target_dtype is not None and torch.is_floating_point(t):
                t = t.to(target_dtype)
            out[k] = t.contiguous()
        else:
            out[k] = v

    return out


def _file_size_mb(path: str) -> float:
    return os.path.getsize(path) / (1024 * 1024)


def main() -> None:
    args = parse_args()

    if not os.path.exists(args.input):
        print(f"âŒ Input not found: {args.input}")
        return

    print(f"ðŸ“¦ Loading: {args.input}")
    ckpt = torch.load(args.input, map_location="cpu", weights_only=False)
    state_dict = _extract_state_dict(ckpt)
    print(f"âœ… Loaded state_dict keys: {len(state_dict)}")

    slim_state = _slim_state_dict(
        state_dict=state_dict,
        dtype_name=args.dtype,
        strip_prefix=tuple(args.strip_prefix),
    )
    print(f"ðŸ§¹ Slimmed keys: {len(slim_state)} | dtype={args.dtype}")

    if args.format == "checkpoint":
        export_obj = {"model": slim_state, "info": "Slim checkpoint for inference"}
    else:
        export_obj = slim_state

    out_dir = os.path.dirname(args.output)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)

    # å…ˆä¿å­˜åˆ°ä¸Žç›®æ ‡åŒç›®å½•çš„ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…è·¨è®¾å¤‡æ›¿æ¢å¤±è´¥
    temp_dir = out_dir if out_dir else "."
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pth", dir=temp_dir) as f:
        tmp_path = f.name

    torch.save(export_obj, tmp_path)
    try:
        os.replace(tmp_path, args.output)
    except OSError:
        # å…œåº•ï¼šæŸäº›çŽ¯å¢ƒ replace ä»å¯èƒ½å¤±è´¥ï¼Œé€€åŒ–ä¸º moveï¼ˆå¯è·¨è®¾å¤‡ï¼‰
        shutil.move(tmp_path, args.output)

    src_mb = _file_size_mb(args.input)
    dst_mb = _file_size_mb(args.output)
    ratio = (dst_mb / src_mb) if src_mb > 0 else 0.0

    print("-" * 50)
    print(f"âœ… Exported inference pth: {args.output}")
    print(f"ðŸ“ Size: {src_mb:.2f} MB -> {dst_mb:.2f} MB (x{ratio:.3f})")
    print("ðŸ’¡ å»ºè®® eval/infer ç›´æŽ¥åŠ è½½è¯¥æ–‡ä»¶ä½œä¸º state_dictã€‚")
    print("-" * 50)


if __name__ == "__main__":
    main()
