import argparse
from datasets import load_from_disk
import pyrwkv_tokenizer

def inspect_data():
    parser = argparse.ArgumentParser()
    parser.add_argument("--path", type=str, required=True, help="Path to processed dataset")
    parser.add_argument("--n", type=int, default=3, help="Number of samples to inspect")
    args = parser.parse_args()
    
    print(f"ğŸ•µï¸ Inspecting: {args.path}")
    
    # 1. åŠ è½½æ•°æ®
    try:
        ds = load_from_disk(args.path)
        if 'train' in ds: ds = ds['train']
    except Exception as e:
        print(f"âŒ Load failed: {e}")
        return

    # 2. åŠ è½½ Tokenizer
    print("ğŸ”¤ Loading Rust Tokenizer...")
    tokenizer = pyrwkv_tokenizer.RWKVTokenizer()
    
    # 3. æŠ½æ ·è§£ç 
    print(f"ğŸ” Sampling {args.n} entries...")
    for i in range(args.n):
        print(f"\n--- Sample {i} ---")
        try:
            ids = ds[i]['input_ids']
            # æˆªå–å‰ 200 ä¸ª token ä»¥é˜²å¤ªé•¿
            preview_ids = ids[:200]
            
            text = tokenizer.decode(preview_ids)
            print(f"[Decoded Text]:\n{text}")
            print(f"\n[Raw IDs (first 10)]: {preview_ids[:10]}")
            
        except Exception as e:
            print(f"âŒ Decode failed: {e}")

if __name__ == "__main__":
    inspect_data()