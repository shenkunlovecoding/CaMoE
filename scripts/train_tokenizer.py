import os
from datasets import load_dataset
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors

def train_custom_tokenizer():
    # 1. å‡†å¤‡è·¯å¾„
    save_path = "tokenizer/minipile_32k"
    os.makedirs(save_path, exist_ok=True)
    
    # 2. åˆå§‹åŒ– Tokenizer (BPE æ¨¡å¼ï¼Œç±»ä¼¼ GPT-2/RoBERTa)
    print("âš™ï¸  Initializing Tokenizer...")
    tokenizer = Tokenizer(models.BPE())
    
    # é¢„å¤„ç†ï¼šæŒ‰å­—èŠ‚åˆ‡åˆ† (ByteLevel)ï¼Œè¿™å¯¹ä»£ç å’Œå¤šè¯­è¨€æ”¯æŒå¾ˆå¥½
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    
    # è§£ç å™¨
    tokenizer.decoder = decoders.ByteLevel()
    
    # åå¤„ç† (RoBERTa é£æ ¼)
    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
    
    # 3. è®¾ç½®è®­ç»ƒå™¨
    trainer = trainers.BpeTrainer(
        vocab_size=32000,           # âœ… ä½ çš„ç›®æ ‡ï¼š32k è¯è¡¨
        min_frequency=2,            # è¿‡æ»¤æ‰åªå‡ºç°ä¸€æ¬¡çš„è¯
        special_tokens=["<|endoftext|>", "<|padding|>"], # ç‰¹æ®Š Token
        show_progress=True
    )
    
    # 4. åŠ è½½æ•°æ®è¿­ä»£å™¨ (æµå¼åŠ è½½ï¼Œä¸å å†…å­˜)
    print("â˜ï¸  Loading MiniPile dataset (streaming)...")
    dataset = load_dataset("JeanKaddour/minipile", split="train", streaming=True)
    
    def batch_iterator(batch_size=10000):
        for i, item in enumerate(dataset):
            yield item["text"]
            if i > 200_000: # åªç”¨å‰ 20w æ¡æ ·æœ¬è®­ç»ƒå°±è¶³å¤Ÿäº†ï¼Œä¸ç”¨è·‘å…¨é‡
                break
                
    # 5. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ Training Tokenizer (this may take 2-3 minutes)...")
    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)
    
    # 6. ä¿å­˜ (ä¿å­˜ä¸º HF æ ¼å¼)
    print(f"ğŸ’¾ Saving to {save_path}...")
    tokenizer.save(os.path.join(save_path, "tokenizer.json"))
    
    # ä¸ºäº†è®© AutoTokenizer èƒ½ç›´æ¥åŠ è½½ï¼Œæˆ‘ä»¬éœ€è¦è¡¥å…… config
    from transformers import PreTrainedTokenizerFast
    
    fast_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        bos_token="<|endoftext|>",
        eos_token="<|endoftext|>",
        pad_token="<|padding|>",
        unk_token="<|endoftext|>"
    )
    fast_tokenizer.save_pretrained(save_path)
    
    print("âœ… Done! You can now load it via AutoTokenizer.from_pretrained('./tokenizer/minipile_32k')")

if __name__ == "__main__":
    train_custom_tokenizer()