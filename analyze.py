import torch
import torch.nn as nn
from camoe import CaMoE_System

# ==========================================
# 1. ä½ çš„é…ç½® (æ‰‹åŠ¨å¡«å…¥ä½  train.py é‡Œç”¨çš„ config)
# ==========================================
# æˆ‘æ ¹æ® 0.1B RWKV çš„æ ‡å‡†é…ç½®å¡«äº†ä¸€ä¸ªé»˜è®¤çš„ï¼Œ
# å¦‚æœä½ æ”¹è¿‡ train.py é‡Œçš„ configï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹ï¼
config = {
    'n_embd': 768,       # 0.1B æ ‡å‡†æ˜¯ 768
    'n_layer': 12,       # 0.1B æ ‡å‡†æ˜¯ 12
    'head_size': 64,
    'vocab_size': 65536, # æˆ–è€… 50277
    
    # å…³é”®å«Œç–‘äººï¼šä¸“å®¶æ•°é‡
    'num_rwkv_experts': 3, # å¦‚æœè¿™é‡Œå¾ˆå¤§ï¼Œå‚æ•°ä¼šçˆ†ç‚¸
    
    # å…¶ä»–
    'ctx_len': 1024,
    'total_capital': 10000.0,
}

def analyze():
    print(f"ğŸ” æ­£åœ¨åˆ†æ CaMoE æ¨¡å‹é…ç½®...")
    print(f"ğŸ“‹ Config: {config}")
    
    # å®ä¾‹åŒ–æ¨¡å‹ (ä¸åŠ è½½æƒé‡ï¼Œåªçœ‹éª¨æ¶)
    model = CaMoE_System(config)
    
    # 1. ç»Ÿè®¡æ€»å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\n" + "="*40)
    print(f"ğŸ“Š å‚æ•°é‡ç»Ÿè®¡")
    print(f"="*40)
    print(f"Total Params:     {total_params / 1e6:.2f} M ({total_params / 1e9:.3f} B)")
    print(f"Trainable Params: {trainable_params / 1e6:.2f} M")
    
    # 2. æ˜¾å­˜ä¼°ç®— (é™æ€)
    # BF16 = 2 bytes
    model_mem_gb = total_params * 2 / (1024**3)
    # AdamW ä¼˜åŒ–å™¨çŠ¶æ€ (m, v) = 8 bytes (FP32) æˆ–è€… 2 bytes (8-bit)
    optim_mem_8bit = total_params * 2 / (1024**3) # state + weight copy
    grad_mem = total_params * 2 / (1024**3) # gradients (BF16)
    
    print(f"\nğŸ’¾ é™æ€æ˜¾å­˜éœ€æ±‚ä¼°ç®— (ä¸å«æ¿€æ´»å€¼)")
    print(f"--------------------------------")
    print(f"Model Weights (BF16): {model_mem_gb:.2f} GB")
    print(f"Gradients     (BF16): {grad_mem:.2f} GB")
    print(f"Optimizer (8-bit):    {optim_mem_8bit:.2f} GB")
    print(f"--------------------------------")
    print(f"ğŸ”¥ ä»…å¯åŠ¨å°±éœ€è¦:      {model_mem_gb + grad_mem + optim_mem_8bit:.2f} GB")
    print(f"   (å¦‚æœè¿™æ˜¯ 1.7B æ¨¡å‹ï¼Œå¯åŠ¨å°±è¦ 10GB+ï¼Œè¿˜æ²¡å¼€å§‹è·‘æ•°æ®)")

    # 3. å‚æ•°åˆ†å¸ƒåˆ†æ (è°æ˜¯èƒ–å­ï¼Ÿ)
    print(f"\nğŸ¥© å‚æ•°åˆ†å¸ƒè§£å‰–")
    print(f"--------------------------------")
    
    backbone_params = 0
    experts_params = 0
    bridge_params = 0
    
    for name, module in model.named_modules():
        # ç»Ÿè®¡ Block é‡Œçš„å…·ä½“åˆ†å¸ƒ
        if isinstance(module, nn.ModuleList) and name == 'blocks':
            first_block = module[0]
            
            # ç»Ÿè®¡ Attn (Backbone)
            attn_p = sum(p.numel() for p in first_block.att.parameters())
            print(f"Layer 0 - RWKV TimeMix: {attn_p/1e6:.2f} M")
            
            # ç»Ÿè®¡ Experts
            exp_p_total = 0
            for i, exp in enumerate(first_block.experts):
                this_exp_p = sum(p.numel() for p in exp.parameters())
                if i < len(first_block.experts) - 1:
                    print(f"   â”œâ”€ RWKV Expert {i}:  {this_exp_p/1e6:.2f} M")
                else:
                    print(f"   â””â”€ Trans Expert:    {this_exp_p/1e6:.2f} M")
                exp_p_total += this_exp_p
            
            print(f"Layer 0 - Total Experts: {exp_p_total/1e6:.2f} M")
            
            # ç»Ÿè®¡ Bridge
            bridge_p = sum(p.numel() for p in first_block.bridge.parameters())
            print(f"Layer 0 - Bridge:        {bridge_p/1e6:.2f} M")
            
            # å®è§‚æ¨ç®—
            total_experts_all_layers = exp_p_total * config['n_layer']
            print(f"\nğŸ‘‰ ç»“è®ºï¼šå…¨æ¨¡å‹ Expert å‚æ•°æ€»å’Œ â‰ˆ {total_experts_all_layers/1e6:.2f} M")
            if total_experts_all_layers > total_params * 0.8:
                print("âš ï¸ è­¦å‘Šï¼šç»å¤§éƒ¨åˆ†å‚æ•°éƒ½åœ¨ä¸“å®¶å±‚ï¼")
                print("   MoE æå¤§åœ°è†¨èƒ€äº†æ˜¾å­˜éœ€æ±‚ï¼Œè™½ç„¶è®¡ç®—é‡(FLOPs)æ²¡å˜ï¼Œä½†æ˜¾å­˜å¿…é¡»å­˜ä¸‹æ‰€æœ‰ä¸“å®¶ã€‚")
            break

    # 4. æ¨¡æ‹Ÿä¸€æ¬¡å‰å‘ä¼ æ’­ (æ£€æŸ¥æ˜¯å¦ä¼šç¬é—´ OOM)
    print(f"\nğŸ§ª æ­£åœ¨å°è¯• Dummy Forward (æ£€æŸ¥ä¸­é—´æ¿€æ´»)...")
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        # æ¨¡æ‹Ÿä¸€ä¸ª Batch
        x = torch.randint(0, config['vocab_size'], (4, config['ctx_len'])).to(device) # Batch=4
        
        torch.cuda.reset_peak_memory_stats()
        with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
            logits, info = model(x)
            
        peak_mem = torch.cuda.max_memory_allocated() / (1024**3)
        print(f"âœ… Forward æˆåŠŸï¼")
        print(f"ğŸ“ˆ å³°å€¼æ˜¾å­˜ (Batch=4, ctx={config['ctx_len']}): {peak_mem:.2f} GB")
        
        # æ£€æŸ¥æœ‰æ²¡æœ‰ Broadcasting ç‚¸è£‚
        print(f"   å¦‚æœè¿™é‡Œæ²¡æŠ¥é”™ï¼Œè¯´æ˜ [Batch, Batch] çš„ Bug ä¿®å¥½äº†ã€‚")
        
    except Exception as e:
        print(f"âŒ Forward å¤±è´¥: {e}")
        print("   å¯èƒ½æ˜¯æ˜¾å­˜ä¸è¶³ï¼Œæˆ–è€…ç»´åº¦ä¸åŒ¹é…ã€‚")

if __name__ == "__main__":
    analyze()