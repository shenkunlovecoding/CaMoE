train.py:
"""
CaMoE v20 ËÆ≠ÁªÉËÑöÊú¨
ÊîØÊåÅ: ‰∏ÉÈò∂ÊÆµË∞ÉÂ∫¶ / ÂàÜÁªÑÂ≠¶‰π†Áéá / ÂàÜÈò∂ÊÆµÊï∞ÊçÆ profile / ÁªèÊµéÁ≥ªÁªüÂ¢ûÂº∫ / Eval Loss
"""

import os
import gc
import time
import argparse
from typing import Dict, Iterator, List, Tuple, Any
import torch
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from datasets import load_from_disk, Dataset, DatasetDict, interleave_datasets
import bitsandbytes as bnb
from CaMoE.backbone import init_rwkv7_cuda
try:
    import swanlab
    HAS_SWANLAB = True
except ImportError:
    HAS_SWANLAB = False

from CaMoE.system import CaMoE_System
from CaMoE.config import get_config, VERSION


def load_backbone(model: CaMoE_System, path: str) -> None:
    r"""load_backbone(model, path) -> None

    ‰ªé RWKV Â∫ïÊ®°ËøÅÁßªÂèØÂØπÈΩêÊùÉÈáçÂà∞ CaMoE Êû∂ÊûÑ„ÄÇ

    Args:
      model (CaMoE_System): ÂΩìÂâç CaMoE Ê®°Âûã„ÄÇ
      path (str): RWKV Â∫ïÊ®°ÊùÉÈáçË∑ØÂæÑ„ÄÇ
    """
    if not os.path.exists(path):
        print(f"‚ö†Ô∏è Weights not found: {path} (Starting from scratch)")
        return
    
    print(f"üì¶ Loading backbone from {path}...")
    official = torch.load(path, map_location='cpu', weights_only=True)
    my_dict = model.state_dict()
    loaded = 0
    
    for k, v in official.items():
        if k in my_dict and my_dict[k].shape == v.shape:
            my_dict[k].copy_(v)
            loaded += 1
            continue
        
        if 'blocks' in k:
            try:
                parts = k.split('.')
                lid = int(parts[1])
                layer_type = parts[2]
                
                if layer_type == 'att':
                    target_name = f"blocks.{lid}.att.{'.'.join(parts[3:])}"
                    if target_name in my_dict and my_dict[target_name].shape == v.shape:
                        my_dict[target_name].copy_(v)
                        loaded += 1
                
                elif layer_type == 'ffn':
                    param_name = '.'.join(parts[3:])
                    for i in range(model.num_rwkv_experts):
                        target = f"blocks.{lid}.experts.{i}.{param_name}"
                        if target in my_dict and my_dict[target].shape == v.shape:
                            noise = torch.randn_like(v) * 0.01
                            my_dict[target].copy_(v + noise)
                            if i == 0: loaded += 1
            except Exception as e:
                pass
    
    model.load_state_dict(my_dict, strict=False)
    print(f"‚úÖ Loaded matching tensors (~{loaded})")

def _build_phase_plan(config: Dict) -> List[Dict[str, Any]]:
    schedule = config.get("phase_schedule")
    if schedule:
        plan = []
        cursor = 0
        for phase in schedule:
            item = dict(phase)
            steps = max(0, int(item.get("steps", 0)))
            item["steps"] = steps
            item["start_step"] = cursor
            item["end_step"] = cursor + steps
            plan.append(item)
            cursor += steps
        return plan

    # ÂÖºÂÆπÊóßÈÖçÁΩÆÔºö‰∏âÈò∂ÊÆµ
    pre = int(config.get("prewarm_steps", 100))
    warm_end = int(config.get("warmup_steps", 500))
    warm = max(0, warm_end - pre)
    total = int(config.get("total_steps", warm_end + 1000))
    normal = max(0, total - warm_end)
    return [
        {
            "name": "prewarm",
            "steps": pre,
            "start_step": 0,
            "end_step": pre,
            "data_profile": "default",
            "train_groups": ["all"],
            "lr_mult": {g: 1.0 for g in config.get("param_groups", [])},
            "market_update": False,
        },
        {
            "name": "warmup",
            "steps": warm,
            "start_step": pre,
            "end_step": pre + warm,
            "data_profile": "default",
            "train_groups": ["all"],
            "lr_mult": {g: 1.0 for g in config.get("param_groups", [])},
            "market_update": False,
        },
        {
            "name": "normal",
            "steps": normal,
            "start_step": pre + warm,
            "end_step": pre + warm + normal,
            "data_profile": "default",
            "train_groups": ["all"],
            "lr_mult": {g: 1.0 for g in config.get("param_groups", [])},
            "market_update": True,
        },
    ]


def _phase_total_steps(phase_plan: List[Dict[str, Any]]) -> int:
    return int(sum(max(0, int(p.get("steps", 0))) for p in phase_plan))


def get_phase(step: int, phase_plan: List[Dict[str, Any]]) -> Dict[str, Any]:
    r"""get_phase(step, phase_plan) -> Dict"""
    for phase in phase_plan:
        if step < phase["end_step"]:
            return phase
    # Ë∂ÖËåÉÂõ¥Êó∂ÂõûÈÄÄÊúÄÂêé‰∏Ä‰∏™ÊúâÊ≠•Êï∞ÁöÑÈò∂ÊÆµ
    for phase in reversed(phase_plan):
        if phase.get("steps", 0) > 0:
            return phase
    return phase_plan[-1] if phase_plan else {"name": "normal", "data_profile": "default", "market_update": True}


def _classify_param_group(name: str, num_rwkv: int) -> str:
    if ".critic." in name:
        return "critic"
    if name.startswith("bridge."):
        return "bridge"
    if name.startswith("emb.") or name.startswith("ln_out.") or name.startswith("head.") or name.startswith("deep_embed."):
        return "emb_head"

    if name.startswith("blocks."):
        parts = name.split(".")
        if len(parts) > 3 and parts[2] == "experts":
            try:
                expert_idx = int(parts[3])
                return "rwkv_experts" if expert_idx < num_rwkv else "trans_experts"
            except ValueError:
                pass
        return "rwkv_backbone"

    return "rwkv_backbone"


def build_param_groups(model: CaMoE_System, config: Dict) -> Tuple[List[Dict[str, Any]], Dict[str, List[torch.nn.Parameter]]]:
    r"""build_param_groups(model, config) -> (optimizer_param_groups, group_map)"""
    groups = {g: [] for g in config.get("param_groups", [
        "rwkv_backbone", "rwkv_experts", "trans_experts", "bridge", "critic", "emb_head"
    ])}
    num_rwkv = int(config.get("num_rwkv_experts", 6))

    for name, p in model.named_parameters():
        g = _classify_param_group(name, num_rwkv)
        if g not in groups:
            groups[g] = []
        groups[g].append(p)

    base_lr = float(config.get("base_lr", 1e-4))
    optim_groups = []
    for gname, params in groups.items():
        if not params:
            continue
        optim_groups.append({
            "params": params,
            "lr": base_lr,
            "name": gname,
        })
    return optim_groups, groups


def apply_phase_policy(
    optimizer,
    phase: Dict[str, Any],
    config: Dict,
    group_map: Dict[str, List[torch.nn.Parameter]],
) -> None:
    r"""apply_phase_policy(optimizer, phase, config, group_map) -> None"""
    base_lr = float(config.get("base_lr", 1e-4))
    train_groups = set(phase.get("train_groups", ["all"]))
    train_all = "all" in train_groups
    lr_mult = phase.get("lr_mult", {})

    for gname, params in group_map.items():
        active = train_all or (gname in train_groups)
        for p in params:
            p.requires_grad = active

    for pg in optimizer.param_groups:
        gname = pg.get("name", "")
        mult = float(lr_mult.get(gname, 1.0 if (train_all or gname in train_groups) else 0.0))
        if not (train_all or gname in train_groups):
            mult = 0.0
        pg["lr"] = base_lr * mult


def apply_route_grad_policy(model: CaMoE_System, phase_name: str, config: Dict) -> None:
    r"""apply_route_grad_policy(model, phase_name, config) -> None

    criticwarm ÊúüÈó¥ÂºÄÂêØ route gradÔºàËÆ≠ÁªÉ criticÔºâÔºåÂÖ∂ÂÆÉÈò∂ÊÆµÊÅ¢Â§çÈªòËÆ§ route_no_grad„ÄÇ
    """
    default_route_no_grad = bool(config.get("route_no_grad", True))
    route_no_grad = default_route_no_grad
    if phase_name == "criticwarm":
        route_no_grad = False

    for block in model.blocks:
        block.route_no_grad = route_no_grad


def _load_profile_datasets(config: Dict, profile_name: str) -> Tuple[Dataset, Dataset]:
    data_profiles = config.get("data_profiles") or {}
    profile = data_profiles.get(profile_name, {})

    if profile_name == "default" and not profile:
        profile = {
            "mix": config.get("mix"),
            "data_path": config.get("data_path"),
        }

    mix = profile.get("mix")
    data_path = profile.get("data_path", config.get("data_path"))
    data_roots = config.get("data_roots") or {}

    if mix and data_roots:
        train_datasets = []
        val_datasets = []
        probs = []
        loaded_names = []

        for name, prob in mix.items():
            if prob <= 0:
                continue
            path = data_roots.get(name)
            if not path or not os.path.exists(path):
                print(f"‚ö†Ô∏è Dataset not found: {path}, skipping {name}.")
                continue

            ds = load_from_disk(path)
            if isinstance(ds, DatasetDict):
                tr = ds["train"]
                va = ds.get("validation") or ds.get("test")
                if va is None:
                    split = tr.train_test_split(test_size=0.01, seed=42)
                    tr, va = split["train"], split["test"]
            else:
                split = ds.train_test_split(test_size=0.01, seed=42)
                tr, va = split["train"], split["test"]

            tr.set_format(type="torch", columns=["input_ids"])
            va.set_format(type="torch", columns=["input_ids"])
            train_datasets.append(tr)
            val_datasets.append(va)
            probs.append(float(prob))
            loaded_names.append(name)
            print(f"  - [{profile_name}] {name}: train={len(tr)}, val={len(va)} (prob={prob})")

        if not train_datasets:
            raise ValueError(f"No valid datasets in profile '{profile_name}'.")

        total_p = sum(probs)
        probs = [p / total_p for p in probs]
        train_data = interleave_datasets(
            train_datasets,
            probabilities=probs,
            seed=42,
            stopping_strategy="all_exhausted",
        )
        val_data = interleave_datasets(
            val_datasets,
            probabilities=probs,
            seed=42,
            stopping_strategy="first_exhausted",
        )
        print(f"üìä Profile={profile_name} Mix: {dict(zip(loaded_names, probs))} -> Train={len(train_data)}, Val={len(val_data)}")
        return train_data, val_data

    # ÂçïÊï∞ÊçÆÈõÜ
    if not data_path:
        raise ValueError(f"Profile '{profile_name}' has no mix and no data_path.")
    raw_dataset = load_from_disk(data_path)
    if isinstance(raw_dataset, DatasetDict):
        if "validation" in raw_dataset:
            train_data, val_data = raw_dataset["train"], raw_dataset["validation"]
        elif "test" in raw_dataset:
            train_data, val_data = raw_dataset["train"], raw_dataset["test"]
        else:
            split = raw_dataset["train"].train_test_split(test_size=0.05, seed=42)
            train_data, val_data = split["train"], split["test"]
    elif isinstance(raw_dataset, Dataset):
        split = raw_dataset.train_test_split(test_size=0.05, seed=42)
        train_data, val_data = split["train"], split["test"]
    else:
        raise ValueError("Unknown dataset type")

    train_data.set_format(type="torch", columns=["input_ids"])
    val_data.set_format(type="torch", columns=["input_ids"])
    print(f"üìä Profile={profile_name}: Train={len(train_data)}, Val={len(val_data)}")
    return train_data, val_data


def build_loader_for_profile(
    profile_name: str,
    config: Dict,
    collate_fn,
) -> Tuple[DataLoader, DataLoader]:
    r"""build_loader_for_profile(profile_name, config, collate_fn) -> (train_loader, val_loader)"""
    train_data, val_data = _load_profile_datasets(config, profile_name)
    train_loader = DataLoader(
        train_data,
        batch_size=config["micro_batch_size"],
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_data,
        batch_size=config["micro_batch_size"],
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    return train_loader, val_loader

def log_gpu() -> str:
    r"""log_gpu() -> str

    ËøîÂõûÂΩìÂâç GPU ÊòæÂ≠òÂç†Áî®ÊëòË¶ÅÂ≠óÁ¨¶‰∏≤„ÄÇ

    Returns:
      str: ÊòæÂ≠ò‰ø°ÊÅØÔºõÊó† CUDA Êó∂ËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤„ÄÇ
    """
    if torch.cuda.is_available():
        alloc = torch.cuda.memory_allocated() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return f"GPU: {alloc:.1f}/{total:.1f}GB"
    return ""

def infinite_loader(loader: DataLoader) -> Iterator:
    r"""infinite_loader(loader) -> Iterator

    Â∞ÜÊúâÈôê DataLoader ÂåÖË£Ö‰∏∫Êó†ÈôêËø≠‰ª£Âô®„ÄÇ

    Args:
      loader (DataLoader): ÂéüÂßãÊï∞ÊçÆÂä†ËΩΩÂô®„ÄÇ

    Returns:
      Iterator: Âæ™ÁéØ‰∫ßÂá∫ batch ÁöÑËø≠‰ª£Âô®„ÄÇ
    """
    while True:
        for batch in loader:
            yield batch

def main() -> None:
    r"""main() -> None

    ËÆ≠ÁªÉ‰∏ªÂÖ•Âè£ÔºåÂåÖÂê´Êï∞ÊçÆÂä†ËΩΩ„ÄÅÊñ≠ÁÇπÁª≠ËÆ≠„ÄÅÈò∂ÊÆµËÆ≠ÁªÉ„ÄÅËØÑ‰º∞‰∏é‰øùÂ≠ò„ÄÇ
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--scale", default="0.4b")
    parser.add_argument(
        "--diag",
        default="baseline",
        choices=["baseline", "no_amp", "no_fast_math", "fp32_kernel"],
        help="Diagnostic switches for isolating NaN source",
    )
    parser.add_argument("--resume", type=str, default=None, help="Path to checkpoint to resume from")
    args = parser.parse_args()
    
    config = get_config(args.scale)

    # Diagnostic overrides (switch-only; no model structure changes)
    if args.diag == "no_amp":
        config["train_use_amp"] = False
    elif args.diag == "no_fast_math":
        config["cuda_use_fast_math"] = False
    elif args.diag == "fp32_kernel":
        config["cuda_use_fast_math"] = False
        config["cuda_force_fp32_kernel"] = True

    # Pass diagnostic kernel switches through env for backbone.init_rwkv7_cuda()
    os.environ["CAMOE_DISABLE_FAST_MATH"] = "1" if not config.get("cuda_use_fast_math", True) else "0"
    os.environ["CAMOE_FORCE_FP32_KERNEL"] = "1" if config.get("cuda_force_fp32_kernel", False) else "0"
    os.environ["CAMOE_NAN_DEBUG"] = "1" if config.get("nan_debug", False) else "0"
    os.environ["CAMOE_SANITIZE_TIMEMIX_OUT"] = "1" if config.get("sanitize_timemix_output", False) else "0"
    os.environ["CAMOE_FORCE_TIMEMIX_FALLBACK"] = "1" if config.get("force_timemix_fallback", False) else "0"
    init_rwkv7_cuda()
    
    # Âº∫Âà∂ËÆæÁΩÆ Eval È¢ëÁéá
    eval_interval = config.get('eval_interval', 1000)  # ÊØè500Ê≠•ËØÑÊµã‰∏ÄÊ¨°
    eval_iters = config.get('eval_iters', 50)         # ÊØèÊ¨°ËØÑÊµãË∑ë50‰∏™batch
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    torch.set_float32_matmul_precision('high')

    train_use_amp = bool(config.get("train_use_amp", True)) and device == "cuda"
    amp_dtype_name = str(config.get("amp_dtype", "bfloat16"))
    amp_dtype = torch.bfloat16 if amp_dtype_name == "bfloat16" else torch.float16
    print(
        f"üß™ Diag={args.diag} | AMP={train_use_amp}({amp_dtype_name}) | "
        f"fast_math={config.get('cuda_use_fast_math', True)} | "
        f"fp32_kernel={config.get('cuda_force_fp32_kernel', False)}"
    )

    phase_plan = _build_phase_plan(config)
    config["phase_schedule"] = phase_plan
    total_steps_from_schedule = _phase_total_steps(phase_plan)
    if total_steps_from_schedule > 0:
        config["total_steps"] = total_steps_from_schedule
    phase_to_id = {p.get("name", f"phase_{i}"): i for i, p in enumerate(phase_plan)}

    # 2. DataLoader Collate
    def simple_collate(batch) -> torch.Tensor:
        r"""simple_collate(batch) -> Tensor

        Â∞ÜÂèòÈïøÊ†∑Êú¨ÊãºÊé•‰∏∫Âõ∫ÂÆöÈïøÂ∫¶ batchÔºåÂπ∂Êåâ CUDA kernel Á∫¶ÊùüËøõË°åÈïøÂ∫¶ÂØπÈΩê„ÄÇ

        Args:
          batch: ÂéüÂßãÊ†∑Êú¨ÂàóË°®ÔºåÊØèÈ°πÂåÖÂê´ ``input_ids``„ÄÇ

        Returns:
          Tensor: ÂΩ¢Áä∂ ``[B, T]`` ÁöÑ long Âº†Èáè„ÄÇ
        """
        input_ids = [item["input_ids"] for item in batch]
        max_len = max(len(ids) for ids in input_ids)
        max_len = min(max_len, config['ctx_len'] + 1)
        
        # [CUDA Kernel Ë¶ÅÊ±Ç] ÂØπÈΩêÂà∞ 16 ÁöÑÂÄçÊï∞ + 1
        CHUNK_LEN = 16
        input_len = ((max_len - 1 + CHUNK_LEN - 1) // CHUNK_LEN) * CHUNK_LEN
        target_len = max(input_len + 1, CHUNK_LEN + 1)
        
        padded_batch = torch.full((len(batch), target_len), -100, dtype=torch.long)
        for i, ids in enumerate(input_ids):
            l = min(len(ids), target_len)
            padded_batch[i, :l] = ids[:l]
        return padded_batch

    # 3. Model & Optimizer
    print("üèóÔ∏è Building model...")
    model = CaMoE_System(config).to(device)

    optimizer_groups, group_map = build_param_groups(model, config)
    optimizer = bnb.optim.AdamW8bit(optimizer_groups, lr=config.get("base_lr", 1e-4))

    # ==========================================
    # Êñ≠ÁÇπÁª≠ËÆ≠ÈÄªËæë
    # ==========================================
        # ==========================================
    # ÊùÉÈáçÂä†ËΩΩÈÄªËæë (ÈÄÇÈÖç MiniPile Init)
    # ==========================================
    start_step = 0
    
    # 1. ‰ºòÂÖàÊ£ÄÊü•ÊòØÂê¶ÊúâÊòæÂºèÊåáÂÆöÁöÑ Resume Ë∑ØÂæÑ
    resume_path = args.resume
    
    # 2. Â¶ÇÊûúÊ≤°ÊåáÂÆö resumeÔºåÊ£ÄÊü•ÊòØÂê¶Êúâ MiniPile ÂàùÂßãÂåñÊùÉÈáç (Ê∏ÖÊ¥óÁâà)
    if not resume_path:
        # ÂÅáËÆæ‰Ω†ÊääÊ∏ÖÊ¥óÂêéÁöÑÊùÉÈáçÊîæÂú®ËøôÈáåÔºåÂêçÂ≠óÂõ∫ÂÆö
        minipile_init_path = f"checkpoints/{config['version']}_{config['scale']}/init.pth"
        if os.path.exists(minipile_init_path):
            print(f"‚ú® Found init checkpoint: {minipile_init_path}")
            resume_path = minipile_init_path
    
    checkpoint = None
    if resume_path and os.path.exists(resume_path):
        print(f"üì¶ Loading checkpoint from {resume_path}...")
        checkpoint = torch.load(resume_path, map_location='cpu')
        
        # Âä†ËΩΩÊ®°ÂûãÊùÉÈáç
        if isinstance(checkpoint, dict) and 'model' in checkpoint:
            # strict=False ÂÖÅËÆ∏‰∏Ä‰∫õÂæÆÂ∞èÁöÑ key Â∑ÆÂºÇÔºå‰ΩÜ‰∏ªË¶ÅÊùÉÈáçÂøÖÈ°ªÂåπÈÖç
            model.load_state_dict(checkpoint['model'], strict=False)
            print("‚úÖ Model weights loaded.")
            
            # Â∞ùËØïÂä†ËΩΩ‰ºòÂåñÂô® (Â¶ÇÊûúÊúâ)
            if 'optimizer' in checkpoint:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer'])
                    print("‚úÖ Optimizer state restored.")
                except Exception as e:
                    print(f"‚ö†Ô∏è Optimizer load failed (expected for init weights): {e}")
            else:
                print("‚ÑπÔ∏è No optimizer state found (Fresh start).")
            
            # Â∞ùËØïÊÅ¢Â§çÊ≠•Êï∞ (Â¶ÇÊûúÊòØ init ÊùÉÈáçÔºåstep Â∫îËØ•ÊòØ 0)
            if 'step' in checkpoint:
                start_step = checkpoint['step']
                # Â¶ÇÊûúÊòØ step 40000 ËøôÁßçÁªìÊùüÁÇπÔºåÊàë‰ª¨Ë¶ÅÂº∫Ë°åÈáçÁΩÆ‰∏∫ 0
                # Âè™ÊúâÂΩìÂÆÉÊòØ‰∏≠Èó¥Â≠òÊ°£Êó∂ÊâçÁªßÁª≠
                if "init" in resume_path or start_step >= config['total_steps']:
                    print(f"üîÑ Resetting step from {start_step} to 0 for new training phase.")
                    start_step = 0
                else:
                    start_step += 1
                    print(f"üîÑ Resuming from step {start_step}")
        else:
            # ÊóßÊ†ºÂºè
            model.load_state_dict(checkpoint, strict=False)
            print("‚ö†Ô∏è Loaded weights only (Legacy format).")
            
    else:
        # 3. Êó¢Ê≤° Resume ‰πüÊ≤° InitÔºåÊâçÂéªÂä†ËΩΩ RWKV Â∫ïÊ®°
        print("üå± No checkpoint found. Loading RWKV backbone...")
        load_backbone(model, config['weights_path'])

    # ==========================================
    # ÊåâÂΩìÂâç step ÂØπÈΩêÈò∂ÊÆµ‰∏éÊï∞ÊçÆ profile
    # ==========================================
    current_phase = get_phase(start_step, phase_plan)
    current_profile = current_phase.get("data_profile", "default")
    print(f"üöÄ Loading datasets for phase={current_phase.get('name')} profile={current_profile} ...")
    try:
        train_loader, val_loader = build_loader_for_profile(current_profile, config, simple_collate)
    except Exception as e:
        print(f"‚ùå Error loading dataset profile '{current_profile}': {e}")
        return
    train_iter = infinite_loader(train_loader)
    
    # ==========================================
    # [Êñ∞Â¢û] ËØÑ‰º∞ÂáΩÊï∞
    # ==========================================
    @torch.no_grad()
    def estimate_loss(model: CaMoE_System, loader: DataLoader, eval_steps: int) -> float:
        r"""estimate_loss(model, loader, eval_steps) -> float

        Âú®È™åËØÅÈõÜ‰∏ä‰º∞ÁÆóÂπ≥Âùá‰∫§ÂèâÁÜµÊçüÂ§±„ÄÇ

        Args:
          model (CaMoE_System): ÂæÖËØÑ‰º∞Ê®°Âûã„ÄÇ
          loader (DataLoader): È™åËØÅÈõÜÂä†ËΩΩÂô®„ÄÇ
          eval_steps (int): ËØÑ‰º∞ÊâπÊ¨°Êï∞„ÄÇ

        Returns:
          float: Âπ≥ÂùáÈ™åËØÅÊçüÂ§±ÔºõËã•Êó†ÊúâÊïàÂÄºÂàôËøîÂõû ``inf``„ÄÇ
        """
        model.eval()
        losses = []
        
        # ‰ΩøÁî® itertools.cycle Êó†ÈôêÂæ™ÁéØÈ™åËØÅÈõÜÔºåÈÅøÂÖç StopIteration
        from itertools import cycle
        
        for i, batch in enumerate(cycle(loader)):
            if i >= eval_steps:
                break
            
            batch = batch.to(device)
            if batch.shape[1] <= 1: 
                continue
            
            x, y = batch[:, :-1], batch[:, 1:]
            
            # Eval Êó∂‰ΩøÁî® Normal Ê®°ÂºèÔºåÊµãËØïÂÖ®Á≥ªÁªü
            with torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=train_use_amp):
                logits, info = model(x, step=100000, phase="normal")
                # Âè™ÁÆó Main Loss
                loss = torch.nn.functional.cross_entropy(
                    logits.reshape(-1, model.vocab_size),
                    y.reshape(-1),
                    ignore_index=-100,
                )
            
            losses.append(loss.item())
            
            # ÂÆâÂÖ®Ê£ÄÊü•ÔºöÂ¶ÇÊûúÊçüÂ§±‰∏∫ NaN Êàñ InfÔºåÁ´ãÂç≥Êä•Âëä
            if not torch.isfinite(torch.tensor(loss.item())):
                print(f"‚ö†Ô∏è Invalid loss detected at eval step {i}: {loss.item()}")
                continue
        
        model.train()
        if len(losses) == 0:
            print("‚ö†Ô∏è Warning: No valid losses collected during evaluation!")
            return float('inf')
        return sum(losses) / len(losses)

    print(f"üìä Model params: {sum(p.numel() for p in model.parameters())/1e6:.1f}M")
    
    # ==========================================
    # SwanLab ÂàùÂßãÂåñ (Â∏¶ÂõæË°®Áª≠Êé•ÂäüËÉΩ)
    # ==========================================
    current_run_id = None
    run_id = None
    
    # 1. Â¶ÇÊûúÊòØ ResumeÔºåÂ∞ùËØï‰ªé checkpoint Êâæ run_id
    if args.resume and isinstance(checkpoint, dict) and 'swanlab_run_id' in checkpoint:
        run_id = checkpoint['swanlab_run_id']
        print(f"üîÑ Resuming SwanLab run: {run_id}")
    
    # 2. ÂàùÂßãÂåñ SwanLab
    if HAS_SWANLAB:
        experiment = swanlab.init(
            project=config['project'],
            name=config['run_name'],
            config=config,
            id=run_id,
            resume="allow"
        )
        # Ëé∑ÂèñÂΩìÂâçÁöÑ run_id (Â¶ÇÊûúÊòØÊñ∞ÁöÑÔºåËøôÈáå‰ºöÁîüÊàêÊñ∞ÁöÑ)
        current_run_id = experiment.public.run_id
    
    os.makedirs(config['save_dir'], exist_ok=True)
    
    print(f"üöÄ Training start from step {start_step}...")
    
    # ==========================================
    # Logging ÈÄªËæë (ÂõûÊªöÂà∞Áû¨Êó∂ÂÄº + ‰øÆÂ§çStepÊòæÁ§∫)
    # ==========================================
    log_interval = config.get('log_interval', 10)
    last_phase_name = None
    active_profile = current_profile
    
    # 5. Training Loop
    for step in range(start_step, config['total_steps']):
        t0 = time.time()
        
        phase = get_phase(step, phase_plan)
        phase_name = phase.get("name", "normal")
        if phase_name != last_phase_name:
            apply_phase_policy(optimizer, phase, config, group_map)
            apply_route_grad_policy(model, phase_name, config)
            last_phase_name = phase_name
            print(f"üîÅ Phase switched -> {phase_name} [{phase.get('start_step', step)}:{phase.get('end_step', step)}]")

            phase_profile = phase.get("data_profile", "default")
            if phase_profile != active_profile:
                print(f"üß≠ Rebuilding dataloaders for profile={phase_profile}")
                train_loader, val_loader = build_loader_for_profile(phase_profile, config, simple_collate)
                train_iter = infinite_loader(train_loader)
                active_profile = phase_profile
        
        try:
            x_batch = next(train_iter)
        except StopIteration:
            train_iter = infinite_loader(train_loader)
            x_batch = next(train_iter)
            
        x_batch = x_batch.to(device)
        if x_batch.shape[1] <= 1: continue
            
        x, y = x_batch[:, :-1], x_batch[:, 1:]
        
        try:
            with torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=train_use_amp):
                logits, info = model(x, step=step, phase=phase_name)
                total_loss, token_losses, main_loss, critic_loss, _bridge_loss = model.compute_losses(logits, y, info)
                loss_to_backward = total_loss / config['grad_accum']
        except RuntimeError as e:
            print(f"üí• Forward/LOSS failed at step={step}, phase={phase_name}: {e}")
            raise

        if not torch.isfinite(loss_to_backward):
            print(f"‚ö†Ô∏è Non-finite loss at step {step}: {float(loss_to_backward)} (skip batch)")
            optimizer.zero_grad(set_to_none=True)
            continue

        if not loss_to_backward.requires_grad:
            critic_req = critic_loss.requires_grad if isinstance(critic_loss, torch.Tensor) else False
            print(
                f"‚ö†Ô∏è No grad graph at step={step}, phase={phase_name} "
                f"(total_loss.requires_grad={total_loss.requires_grad}, critic_loss.requires_grad={critic_req})."
            )
            optimizer.zero_grad(set_to_none=True)
            continue

        loss_to_backward.backward()
        
        if (step + 1) % config['grad_accum'] == 0:
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
            
            if phase.get("market_update", True) and step > 100:
                model.update_market(info, token_losses, step, phase=phase_name, critic_loss=critic_loss)
        
        # [‰øÆÊîπ] Êó•Âøó‰∏éËØÑ‰º∞ÈÄªËæë
        if step % log_interval == 0:
            dt = time.time() - t0
            tps = config['micro_batch_size'] * x.shape[1] / dt
            
            # --- ËØÑ‰º∞ ---
            val_loss = None
            if step > 0 and step % eval_interval == 0:
                print(f"üîç Evaluating at step {step}...")
                val_loss = estimate_loss(model, val_loader, eval_iters)
            
            # ÁªüËÆ°
            stats = model.log_market_health()
            
            # ÊâìÂç∞ (Áû¨Êó∂ Loss)
            log_str = f"Step {step} | Loss: {main_loss.item():.3f}"
            if val_loss:
                log_str += f" | ValLoss: {val_loss:.3f}"
            log_str += f" | TPS: {tps:.0f} | [{phase_name.upper()}]"
            print(log_str)
            
            # SwanLab Log (ÂÖ≥ÈîÆ‰øÆÊ≠£Ôºö‰º†ÂÖ• step ÂèÇÊï∞)
            if HAS_SWANLAB:
                logs = {
                    "Loss/Train_Main": main_loss.item(),
                    "Loss/Train_Critic": critic_loss.item() if isinstance(critic_loss, torch.Tensor) else critic_loss,
                    "Speed/TPS": tps,
                    "Phase/ID": float(phase_to_id.get(phase_name, -1)),
                    f"Phase/{phase_name}": 1.0,
                    **stats
                }
                if val_loss:
                    logs["Loss/Validation"] = val_loss
                
                # [ÂÖ≥ÈîÆ] ÊòæÂºèÊåáÂÆö stepÔºåËøôÊ†∑ step 1000 Â∞±‰ºöÁîªÂú® X=1000 Â§Ñ
                swanlab.log(logs, step=step)
        
        # ‰øùÂ≠òÂÆåÊï¥ Checkpoint (È°∫‰æø‰øùÂ≠ò run_id)
        if step > 0 and step % 2000 == 0:
            gc.collect()
            torch.cuda.empty_cache()
            print("üßπ Cache cleared")
            path = os.path.join(config['save_dir'], f"{config['version']}_step{step}.pth")
        
            checkpoint_data = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'step': step,
                'config': config,
                'swanlab_run_id': current_run_id,
                'version': config['version']  # È¢ùÂ§ñËÆ∞ÂΩïÁâàÊú¨
            }
            torch.save(checkpoint_data, path)
            print(f"üíæ Saved: {path}")
    
    final_path = os.path.join(config['save_dir'], f"{config['version']}_final.pth")
    torch.save(
        {
            'model': model.state_dict(),
            'step': config['total_steps'],
            'config': config,
            'swanlab_run_id': current_run_id,
            'version': config['version'],
        },
        final_path
    )
    print("üéâ Done!")

if __name__ == "__main__":
    main()


CaMoE/system.py:
"""
CaMoE v18 ‰∏ªÁ≥ªÁªü (Final Fix)
Changes:
1. Âº∫Âà∂ÂÖ®Á®ãÂºÄÂêØ Router (use_market=True)ÔºåÊãíÁªùÈöèÊú∫Ë∑ØÁî±„ÄÇ
2. ‰øÆÂ§ç LinearTransformerExpert ÂàùÂßãÂåñ„ÄÇ
3. ‰øùÊåÅ Rescale Trick Âíå GC„ÄÇ
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
from typing import Dict, Tuple, List
from contextlib import nullcontext
from torch.utils.checkpoint import checkpoint

from .backbone import RWKV7_TimeMix, DeepEmbedAttention, SharedDeepEmbed
from .bridge import UltimateBridge
from .experts import SparseRWKVFFN, LinearTransformerExpert
from .critic import CriticVC
from .market import CapitalManager, SparseRouter

class CaMoE_Block(nn.Module):
    r"""Âçï‰∏™ CaMoE BlockÔºåÂåÖÂê´ TimeMix„ÄÅDEA ‰∏é Top-2 ‰∏ìÂÆ∂Ë∑ØÁî±„ÄÇ"""
    
    def __init__(
        self,
        n_embd: int,
        n_layer: int,
        layer_id: int,
        head_size: int,
        config: Dict,
        bridge: nn.Module,
        shared_deep_embed: nn.Module = None,
    ) -> None:
        r"""ÂàùÂßãÂåñÂçïÂ±Ç CaMoE Block„ÄÇ"""
        super().__init__()
        
        self.layer_id = layer_id
        self.num_rwkv = config.get('num_rwkv_experts', 6)
        self.num_trans = config.get('num_trans_experts', 2)
        self.num_experts = self.num_rwkv + self.num_trans
        self.n_embd = n_embd
        self.bridge = bridge
        self.nan_debug = config.get("nan_debug", False)
        use_gc = config.get("use_gradient_checkpoint", True)
        self.checkpoint_att_stage = use_gc and config.get("checkpoint_att_stage", True)
        self.checkpoint_expert_stage = use_gc and config.get("checkpoint_expert_stage", True)
        self.route_no_grad = config.get("route_no_grad", True)
        self.lazy_prefix_union = config.get("lazy_prefix_union", True)
        
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        
        # RWKV-7 TimeMix (Backbone)
        self.att = RWKV7_TimeMix(n_embd, n_layer, layer_id, head_size)
        
        # DeepEmbedAttention (v18.5-test): ‰∏é TimeMix Âπ∂Ë°åÁöÑÂõ†Êûú Attention ÂàÜÊîØ
        self.use_deep_embed_attention = config.get("use_deep_embed_attention", False)
        vocab_size = config.get("vocab_size", 65536)
        if self.use_deep_embed_attention:
            self.dea = DeepEmbedAttention(
                n_embd=n_embd,
                n_layer=n_layer,
                layer_id=layer_id,
                head_size=head_size,
                vocab_size=vocab_size,
                shared_deep_embed=shared_deep_embed,
                q_dim=config.get("dea_q_dim", 256),
                kv_dim=config.get("dea_kv_dim", 32),
                score_scale=config.get("dea_score_scale", 1024.0),
                cap_scale=config.get("dea_cap_scale", 64.0),
            )
        else:
            self.dea = None
        
        # ‰∏ìÂÆ∂ÁªÑ
        self.experts = nn.ModuleList()
        
        # RWKV FFN Experts
        for _ in range(self.num_rwkv):
            self.experts.append(SparseRWKVFFN(n_embd))
        
        # Transformer Experts
        n_head = n_embd // head_size
        for _ in range(self.num_trans):
            self.experts.append(LinearTransformerExpert(n_embd, n_head))
        
        # Critic
        self.critic = CriticVC(n_embd, self.num_experts)

    def _assert_finite(self, x: torch.Tensor, name: str, step: int) -> None:
        if (not self.nan_debug) or (x is None):
            return
        if not torch.is_floating_point(x):
            return
        if torch.isfinite(x).all():
            return
        with torch.no_grad():
            bad = ~torch.isfinite(x)
            bad_count = int(bad.sum().item())
            total = x.numel()
            finite_x = x[torch.isfinite(x)]
            if finite_x.numel() > 0:
                vmin = float(finite_x.min().item())
                vmax = float(finite_x.max().item())
            else:
                vmin = float("nan")
                vmax = float("nan")
            print(
                f"‚ùå NaNDebug-Block | step={step} | block={self.layer_id} | tensor={name} | "
                f"bad={bad_count}/{total} | finite_min={vmin:.6e} | finite_max={vmax:.6e}"
            )
        raise RuntimeError(f"NaN/Inf in block {self.layer_id}, tensor={name}, step={step}")
    
    def _forward_att_stage(
        self,
        x: torch.Tensor,
        v_first: torch.Tensor,
        idx: torch.Tensor,
        step: int,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        r"""ÊâßË°å TimeMix(+DEA) ‰∏é ln2ÔºåËøîÂõû x_after_att/h/v_first/rwkv_state„ÄÇ"""
        self._assert_finite(x, "x_in", step)
        x_ln = self.ln1(x)
        self._assert_finite(x_ln, "x_ln", step)
        att_out, v_first, rwkv_state = self.att(x_ln, v_first)
        self._assert_finite(att_out, "att_out", step)
        self._assert_finite(v_first, "v_first_att", step)
        self._assert_finite(rwkv_state, "rwkv_state", step)
        if self.dea is not None and idx is not None:
            dea_out = self.dea(x_ln, idx)
            self._assert_finite(dea_out, "dea_out", step)
            x_after_att = x + att_out + dea_out
        else:
            x_after_att = x + att_out
        self._assert_finite(x_after_att, "x_after_att", step)
        h = self.ln2(x_after_att)
        self._assert_finite(h, "h_ln2", step)
        return x_after_att, h, v_first, rwkv_state

    def _forward_route_stage(
        self,
        h: torch.Tensor,
        capital_shares: torch.Tensor,
        router: SparseRouter,
        use_market: bool,
        training: bool,
        step: int,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        r"""ÊâßË°å confidence/critic/routerÔºåËøîÂõû winners/weights/costs/difficulty/affinity„ÄÇ"""
        route_ctx = torch.no_grad if self.route_no_grad else nullcontext
        with route_ctx():
            route_h = h.detach() if self.route_no_grad else h
            conf_list = [exp.get_confidence(route_h) for exp in self.experts]
            confidences = torch.stack(conf_list, dim=-1)  # [B, T, E]
            self._assert_finite(confidences, "confidences", step)

            if not use_market:
                B, T, _E = confidences.shape
                winners = torch.randint(0, self.num_experts, (B, T, 2), device=h.device)
                weights = torch.ones(B, T, 2, device=h.device) * 0.5
                costs = torch.zeros(B, T, device=h.device)
                difficulty = torch.ones(B, T, 1, device=h.device)
                affinity = torch.zeros(B, T, self.num_experts, device=h.device)
            else:
                difficulty, affinity = self.critic(route_h)
                self._assert_finite(difficulty, "difficulty", step)
                self._assert_finite(affinity, "affinity", step)
                critic_subsidy = self.critic.subsidy_from_affinity(affinity)
                self._assert_finite(critic_subsidy, "critic_subsidy", step)
                winners, weights, costs, bids = router.route(
                    confidences, capital_shares, difficulty, critic_subsidy, training
                )
                self._assert_finite(weights, "weights", step)
                self._assert_finite(costs, "costs", step)
                self._assert_finite(bids, "bids", step)

        if self.route_no_grad:
            diff_out = difficulty.detach()
        else:
            # criticwarm ÈúÄË¶Å difficulty ‰øùÁïôËÆ°ÁÆóÂõæÊù•ËÆ≠ÁªÉ critic
            diff_out = difficulty

        return winners.detach(), weights.detach(), costs.detach(), diff_out, affinity.detach()

    def _build_trans_prefix_union(
        self,
        h: torch.Tensor,
        rwkv_state: torch.Tensor,
        winners: torch.Tensor,
        step: int,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        r"""‰ªÖ‰∏∫ Transformer ÂëΩ‰∏≠ token ÊûÑÂª∫ prefixÔºõËøîÂõû prefix_union ‰∏éÁ¥¢ÂºïÊò†Â∞Ñ„ÄÇ"""
        B, T, C = h.shape
        flat_bt = B * T
        flat_h = h.reshape(flat_bt, C)
        flat_state = rwkv_state.reshape(flat_bt, C)

        if not self.lazy_prefix_union:
            bridge_prefix = self.bridge(flat_h, flat_state)  # [B*T, P, C]
            self._assert_finite(bridge_prefix, "bridge_prefix_full", step)
            prefix_indices = torch.arange(flat_bt, device=h.device, dtype=torch.long)
            return bridge_prefix, prefix_indices

        trans_mask = (winners[:, :, 0] >= self.num_rwkv) | (winners[:, :, 1] >= self.num_rwkv)
        flat_mask = trans_mask.reshape(-1)
        prefix_indices = torch.full((flat_bt,), -1, device=h.device, dtype=torch.long)

        if not flat_mask.any():
            empty_prefix = torch.empty(
                0,
                self.bridge.max_prefix_len,
                C,
                device=h.device,
                dtype=h.dtype,
            )
            return empty_prefix, prefix_indices

        prefix_union = self.bridge(flat_h[flat_mask], flat_state[flat_mask])  # [N_u, P, C]
        self._assert_finite(prefix_union, "bridge_prefix_union", step)
        prefix_indices[flat_mask] = torch.arange(prefix_union.shape[0], device=h.device, dtype=torch.long)
        return prefix_union, prefix_indices

    def _forward_expert_stage(
        self,
        x_after_att: torch.Tensor,
        h: torch.Tensor,
        rwkv_state: torch.Tensor,
        winners: torch.Tensor,
        weights: torch.Tensor,
        step: int,
    ) -> torch.Tensor:
        r"""ÊâßË°å Top-2 ‰∏ìÂÆ∂Ê∑∑ÂêàÂπ∂ËøîÂõû block ËæìÂá∫„ÄÇ"""
        B, T, C = h.shape
        prefix_union, prefix_indices = self._build_trans_prefix_union(h, rwkv_state, winners, step)
        final_out = torch.zeros_like(h)  # [B, T, C]

        for rank in range(2):
            rank_winners = winners[:, :, rank]  # [B, T]
            rank_weights = weights[:, :, rank].unsqueeze(-1)  # [B, T, 1]

            for e in range(self.num_experts):
                mask = (rank_winners == e)  # [B, T]
                if not mask.any():
                    continue

                selected_h = h[mask]  # [N, C]
                selected_weights = rank_weights[mask]  # [N, 1]

                if e >= self.num_rwkv:
                    flat_mask = mask.reshape(-1)
                    if self.lazy_prefix_union:
                        sel_idx = prefix_indices[flat_mask]
                        valid = sel_idx >= 0
                        if not valid.any():
                            continue
                        expert_out = torch.zeros_like(selected_h)
                        expert_valid = self.experts[e](selected_h[valid], prefix_union[sel_idx[valid]])
                        if expert_valid.dtype != expert_out.dtype:
                            expert_valid = expert_valid.to(expert_out.dtype)
                        expert_out[valid] = expert_valid
                    else:
                        expert_out = self.experts[e](selected_h, prefix_union[flat_mask])
                else:
                    expert_out = self.experts[e](selected_h, None)
                if expert_out.dtype != selected_h.dtype:
                    expert_out = expert_out.to(selected_h.dtype)
                self._assert_finite(expert_out, f"expert_out_e{e}", step)

                weighted_out = expert_out * selected_weights
                self._assert_finite(weighted_out, f"weighted_out_e{e}", step)
                final_out[mask] += weighted_out

        self._assert_finite(final_out, "final_out", step)
        x_out = x_after_att + final_out
        self._assert_finite(x_out, "x_out", step)
        return x_out

    def forward(self, 
                x: torch.Tensor, 
                v_first: torch.Tensor,
                capital_shares: torch.Tensor,
                router: SparseRouter,
                step: int,
                warmup_steps: int,
                use_market: bool = True,
                training: bool = True,
                idx: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        r"""forward(x, v_first, capital_shares, router, step, warmup_steps, use_market=True, training=True, idx=None) -> Tuple[Tensor, Tensor, Dict]"""
        del warmup_steps

        if self.training and self.checkpoint_att_stage:
            x_after_att, h, v_first, rwkv_state = checkpoint(
                lambda xx, vv, ii: self._forward_att_stage(xx, vv, ii, step),
                x,
                v_first,
                idx,
                use_reentrant=False,
            )
        else:
            x_after_att, h, v_first, rwkv_state = self._forward_att_stage(x, v_first, idx, step)

        winners, weights, costs, difficulty, affinity = self._forward_route_stage(
            h, capital_shares, router, use_market, training, step
        )

        if self.training and self.checkpoint_expert_stage:
            x = checkpoint(
                lambda x0, h0, s0, w0, wt0: self._forward_expert_stage(x0, h0, s0, w0, wt0, step),
                x_after_att,
                h,
                rwkv_state,
                winners,
                weights,
                use_reentrant=False,
            )
        else:
            x = self._forward_expert_stage(x_after_att, h, rwkv_state, winners, weights, step)

        info = {
            "winners": winners,
            "costs": costs,
            "difficulty": difficulty,
            "affinity": affinity,
        }
        return x, v_first, info


class CaMoE_System(nn.Module):
    r"""CaMoE ‰∏ªÁ≥ªÁªüÔºåÂ∞ÅË£ÖÂ§öÂ±Ç Block„ÄÅÂ∏ÇÂú∫Áä∂ÊÄÅ‰∏éÊçüÂ§±ËÆ°ÁÆó„ÄÇ"""

    def __init__(self, config: Dict) -> None:
        r"""ÂàùÂßãÂåñÁ≥ªÁªüÁ∫ßÊ®°Âùó‰∏éÂÖ±‰∫´ÁªÑ‰ª∂„ÄÇ"""
        super().__init__()
        self.config = config
        self.n_embd = config['n_embd']
        self.n_layer = config['n_layer']
        self.vocab_size = config['vocab_size']
        self.use_gradient_checkpoint = config.get("use_gradient_checkpoint", True)
        self.nan_debug = config.get("nan_debug", False)
        self.economy_cfg = config.get("economy", {})
        
        self.num_rwkv_experts = config.get('num_rwkv_experts', 6)
        self.num_trans_experts = config.get('num_trans_experts', 2)
        self.num_experts = self.num_rwkv_experts + self.num_trans_experts
        
        # Embedding
        self.emb = nn.Embedding(self.vocab_size, self.n_embd)

        # Shared DeepEmbed table (optional, recommended for VRAM efficiency)
        self.deep_embed = None
        if config.get("use_deep_embed_attention", False) and config.get("use_shared_deep_embed", True):
            self.deep_embed = SharedDeepEmbed(
                vocab_size=self.vocab_size,
                k_dim=min(config.get("dea_q_dim", 256), self.n_embd),
                v_dim=self.n_embd,
            )
        
        # ÂÖ±‰∫´ Bridge
        self.bridge = UltimateBridge(
            self.n_embd, 
            config.get('prefix_len', 64),
            config.get('low_rank_dim', 64)
        )
        
        # Blocks
        self.blocks = nn.ModuleList()
        for i in range(self.n_layer):
            self.blocks.append(CaMoE_Block(
                self.n_embd,
                self.n_layer,
                i,
                config['head_size'],
                config,
                bridge=self.bridge,
                shared_deep_embed=self.deep_embed,
            ))
        
        self.ln_out = nn.LayerNorm(self.n_embd)
        
        # Head (ÂèØÈÄâ Tied Embedding)
        if config.get('tied_embeddings', False):
            self.head = None  # ‰ΩøÁî® emb.weight
        else:
            self.head = nn.Linear(self.n_embd, self.vocab_size, bias=False)
        
        # Market
        self.capital_manager = CapitalManager(
            self.n_layer, self.num_experts,
            total_capital=config.get('total_capital', 10000.0),
            min_share=config.get('min_capital_share', 0.05),
            tax_threshold=config.get('tax_threshold', 2.0),
            tax_rate=config.get('tax_rate', 0.1),
            economy=self.economy_cfg,
        )
        
        self.router = SparseRouter()
        self.register_buffer("layer_performance_ema", torch.zeros(self.n_layer))
        self._critic_loss_ema = None
        self._last_critic_bonus_signal = 0.0

    def _assert_finite(self, x: torch.Tensor, name: str, step: int, layer_id: int = -1) -> None:
        r"""_assert_finite(x, name, step, layer_id=-1) -> None

        Âú®Ë∞ÉËØïÊ®°Âºè‰∏ãÊ†°È™åÂº†ÈáèÊï∞ÂÄºÂêàÊ≥ïÊÄßÔºåÂá∫Áé∞ NaN/Inf Á´ãÂç≥ÊäõÈîôÂπ∂ËæìÂá∫ÂÆö‰Ωç‰ø°ÊÅØ„ÄÇ
        """
        if (not self.nan_debug) or (x is None):
            return
        if not torch.is_floating_point(x):
            return
        if torch.isfinite(x).all():
            return

        with torch.no_grad():
            bad = ~torch.isfinite(x)
            bad_count = int(bad.sum().item())
            total = x.numel()
            finite_x = x[torch.isfinite(x)]
            if finite_x.numel() > 0:
                vmin = float(finite_x.min().item())
                vmax = float(finite_x.max().item())
            else:
                vmin = float("nan")
                vmax = float("nan")
            print(
                f"‚ùå NaNDebug | step={step} | layer={layer_id} | tensor={name} | "
                f"bad={bad_count}/{total} | finite_min={vmin:.6e} | finite_max={vmax:.6e}"
            )
        raise RuntimeError(f"NaN/Inf detected at step={step}, layer={layer_id}, tensor={name}")
    
    def forward(self, idx: torch.Tensor, step: int = 0, 
                phase: str = "normal") -> Tuple[torch.Tensor, Dict]:
        r"""forward(idx, step=0, phase="normal") -> Tuple[Tensor, Dict]

        ÊâßË°åÊï¥ÁΩëÂâçÂêëÂπ∂Êî∂ÈõÜÂêÑÂ±ÇË∑ØÁî±‰ø°ÊÅØ„ÄÇ

        Args:
          idx (Tensor): ÂΩ¢Áä∂ ``[B, T]`` ÁöÑ token id„ÄÇ
          step (int, optional): ÂΩìÂâçÊ≠•Êï∞„ÄÇDefault: ``0``„ÄÇ
          phase (str, optional): ËÆ≠ÁªÉÈò∂ÊÆµÊ†áÁ≠æ„ÄÇDefault: ``"normal"``„ÄÇ

        Returns:
          Tuple[Tensor, Dict]: ``logits`` ‰∏éÂêÑÂ±Ç ``info``„ÄÇ
        """
        x = self.emb(idx)
        self._assert_finite(x, "emb_out", step, -1)
        v_first = None
        
        # [CRITICAL FIX] ÂßãÁªàÂºÄÂêØ Market Routing
        # Âç≥‰ΩøÂú® Prewarm/WarmupÔºåÊàë‰ª¨‰πüÈúÄË¶Å Router ÈÄâÂá∫ÊúÄÂ•ΩÁöÑ‰∏ìÂÆ∂ÔºåËÆ©‰∏ìÂÆ∂Ëé∑ÂæóÊ≠£Á°ÆÁöÑÊ¢ØÂ∫¶
        # ËµÑÊú¨ÁöÑÊõ¥Êñ∞ (Update) Áî± train.py ÊéßÂà∂ÔºåËøôÈáåÂè™ÁÆ°Ë∑ØÁî± (Selection)
        use_market = True 
        
        all_info = {
            "winners": [], "costs": [], "difficulties": [], "affinities": []
        }
        warmup_steps = self.config.get('warmup_steps', 2000)

        for i, block in enumerate(self.blocks):
            shares = self.capital_manager.get_shares(i)
            x, v_first, info = block(
                x, v_first, shares, self.router,
                step, warmup_steps, use_market, self.training, idx
            )

            self._assert_finite(x, "block_out", step, i)
            self._assert_finite(v_first, "v_first", step, i)
            self._assert_finite(info["costs"], "costs", step, i)
            self._assert_finite(info["difficulty"], "difficulty", step, i)
            self._assert_finite(info["affinity"], "affinity", step, i)
            
            all_info["winners"].append(info["winners"].detach())
            all_info["costs"].append(info["costs"].detach())
            # criticwarm ÈúÄË¶Å difficulty ‰øùÁïô autograd ÂõæÊù•ËÆ≠ÁªÉ Critic
            # ÂÖ∂ÂÆÉÈò∂ÊÆµÂú® block ÂÜÖ route_no_grad=True Êó∂ difficulty Â∑≤ÊòØ detached tensor
            all_info["difficulties"].append(info["difficulty"])
            all_info["affinities"].append(info["affinity"].detach())
        
        x = self.ln_out(x)
        self._assert_finite(x, "ln_out", step, self.n_layer)
        
        # Output (Tied Embedding Rescale Trick)
        if self.head is not None:
            logits = self.head(x)
        else:
            # Tied embedding ÈúÄÁº©ÊîæÔºåÈÅøÂÖç logits ÂπÖÂ∫¶ËøáÂ§ßÂØºËá¥ CE ÈáèÁ∫≤ÂºÇÂ∏∏
            x = x * (self.n_embd ** -0.5)
            logits = F.linear(x, self.emb.weight)
        self._assert_finite(logits, "logits", step, self.n_layer)
        
        return logits, all_info
    
    def compute_losses(
        self,
        logits: torch.Tensor,
        targets: torch.Tensor,
        all_info: Dict,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, float]:
        r"""compute_losses(logits, targets, all_info) -> Tuple[Tensor, Tensor, Tensor, Tensor, float]

        ËÆ°ÁÆó‰∏ªÊçüÂ§±„ÄÅtoken Á∫ßÊçüÂ§±‰ª•Âèä Critic ÊçüÂ§±„ÄÇ

        Args:
          logits (Tensor): ÂΩ¢Áä∂ ``[B, T, V]``„ÄÇ
          targets (Tensor): ÂΩ¢Áä∂ ``[B, T]``„ÄÇ
          all_info (Dict): ÂêÑÂ±ÇÈöæÂ∫¶/Ë∑ØÁî±‰ø°ÊÅØ„ÄÇ

        Returns:
          Tuple[Tensor, Tensor, Tensor, Tensor, float]:
          ``total_loss``„ÄÅ``token_losses``„ÄÅ``main_loss``„ÄÅ``critic_loss``„ÄÅ``bridge_loss``„ÄÇ
        """
        if self.config.get("stabilize_logits", False):
            # ËÆ≠ÁªÉÁ®≥ÂÆöÊÄß‰øùÊä§ÔºöÈÅøÂÖç‰∏äÊ∏∏ÊûÅÁ´ØÂÄºÂØºËá¥ CE Áõ¥Êé• NaN/Inf
            logits = torch.nan_to_num(logits, nan=0.0, posinf=30.0, neginf=-30.0)

        B, T = targets.shape
        
        # Main Loss
        main_loss = F.cross_entropy(
            logits.reshape(-1, self.vocab_size),
            targets.reshape(-1),
            ignore_index=-100,
        )
        
        # Token Losses (for Market Update)
        with torch.no_grad():
            token_losses = F.cross_entropy(
                logits.reshape(-1, self.vocab_size),
                targets.reshape(-1),
                reduction='none',
                ignore_index=-100,
            ).reshape(B, T)
            if self.config.get("stabilize_logits", False):
                token_losses = torch.nan_to_num(token_losses, nan=0.0, posinf=100.0, neginf=0.0)
            # ignore_index ‰ΩçÁΩÆÊú¨Ë∫´‰∏∫ 0 lossÔºåËøôÈáåÂÜçÊòæÂºèÂΩíÈõ∂ÔºåÈÅøÂÖçÂêéÁª≠Â∏ÇÂú∫Êõ¥Êñ∞ËØØÁî®
            token_losses = token_losses.masked_fill(targets.eq(-100), 0.0)
        
        # Critic Loss
        critic_loss = 0.0
        for i, diff in enumerate(all_info.get("difficulties", [])):
            baseline = self.capital_manager.baseline_losses[i]
            target = F.relu(token_losses - baseline)
            critic_loss += F.smooth_l1_loss(diff.squeeze(-1), target)
        
        if len(all_info.get("difficulties", [])) > 0:
            critic_loss /= len(all_info["difficulties"])

        total_loss = main_loss + 0.1 * critic_loss
        bridge_loss = 0.0 # No longer used
        
        return total_loss, token_losses, main_loss, critic_loss, bridge_loss
    
    def _compute_critic_bonus_signal(self, critic_loss) -> float:
        if critic_loss is None:
            return 0.0
        curr = float(critic_loss.detach().item() if isinstance(critic_loss, torch.Tensor) else critic_loss)
        eps = 1e-6
        momentum = float(self.economy_cfg.get("critic_bonus_ema_momentum", 0.95))

        if self._critic_loss_ema is None:
            self._critic_loss_ema = curr
            return 0.0

        prev = self._critic_loss_ema
        signal = (prev - curr) / (abs(prev) + eps)
        self._critic_loss_ema = momentum * prev + (1.0 - momentum) * curr
        return float(signal)

    def _build_donor_state(self, donor_indices: torch.Tensor) -> Dict[str, torch.Tensor]:
        donor_state = {}
        if donor_indices.numel() == 0:
            return donor_state

        weights = []
        for idx in donor_indices.tolist():
            cap = torch.clamp(self.blocks[idx].critic.capital, min=1.0)
            weights.append(cap)
        weight_tensor = torch.stack(weights)
        norm_w = weight_tensor / (weight_tensor.sum() + 1e-6)

        for name, _p in self.blocks[0].critic.named_parameters():
            acc = None
            for w, idx in zip(norm_w, donor_indices.tolist()):
                src = dict(self.blocks[idx].critic.named_parameters())[name].detach()
                term = src * w.to(src.dtype)
                acc = term if acc is None else (acc + term)
            donor_state[name] = acc
        return donor_state

    def _handle_critic_bankruptcy_and_restructure(self, layer_idx: int, step: int) -> None:
        critic = self.blocks[layer_idx].critic
        threshold = float(self.economy_cfg.get("critic_bankrupt_threshold_ratio", 0.2)) * critic.init_capital
        bailout_base = float(self.economy_cfg.get("bailout_base", 1000.0))
        bailout_decay = float(self.economy_cfg.get("bailout_decay", 0.65))
        bailout_min = float(self.economy_cfg.get("bailout_min", 200.0))
        alpha = float(self.economy_cfg.get("restructure_alpha", 0.12))
        donor_topk = int(self.economy_cfg.get("donor_topk", 2))

        if float(critic.capital.item()) >= threshold:
            return

        bcount = float(critic.bailout_count.item())
        bailout = max(bailout_min, bailout_base * (bailout_decay ** bcount))
        critic.capital = critic.capital + bailout
        critic.debt = critic.debt + bailout
        critic.bailout_count = critic.bailout_count + 1

        if self.n_layer <= 1:
            return

        scores = self.layer_performance_ema.clone()
        scores[layer_idx] = -1e9
        k = min(donor_topk, self.n_layer - 1)
        donor_indices = torch.topk(scores, k=k).indices
        donor_state = self._build_donor_state(donor_indices)
        if donor_state:
            critic.restructure_from_donors(donor_state, alpha=alpha)
            print(
                f"üîÅ CriticRestructure | step={step} | layer={layer_idx} | "
                f"donors={donor_indices.tolist()} | alpha={alpha:.3f} | "
                f"bailout={bailout:.2f} | debt={float(critic.debt.item()):.2f}"
            )

        if step % 100 == 0:
            print(f"üèõÔ∏è Layer {layer_idx}: Critic bailout={bailout:.2f}, debt={float(critic.debt.item()):.2f}")

    def update_market(
        self,
        all_info: Dict,
        token_losses: torch.Tensor,
        step: int,
        phase: str = "normal",
        critic_loss=None,
    ) -> None:
        r"""update_market(all_info, token_losses, step, phase="normal", critic_loss=None) -> None"""
        with torch.no_grad():
            bonus_signal = self._compute_critic_bonus_signal(critic_loss)
            bonus_clip = self.economy_cfg.get("critic_bonus_clip", (-0.1, 0.3))
            clipped_signal = float(max(float(bonus_clip[0]), min(float(bonus_clip[1]), bonus_signal)))
            self._last_critic_bonus_signal = clipped_signal

            if phase == "criticwarm":
                reward_scale = float(self.economy_cfg.get("criticwarm_reward_scale", 2.0))
                penalty_scale = float(self.economy_cfg.get("criticwarm_penalty_scale", 0.4))
                critic_bonus_scale = float(self.economy_cfg.get("critic_bonus_scale", 0.2))
            else:
                reward_scale = 1.0
                penalty_scale = 1.0
                critic_bonus_scale = 0.0

            base_commission = float(self.economy_cfg.get("base_commission", 0.8))
            dividend_scale = float(self.economy_cfg.get("dividend_scale", 0.6))
            dividend_std_factor = float(self.economy_cfg.get("dividend_std_factor", 0.5))
            repay_ratio = float(self.economy_cfg.get("repay_ratio", 0.25))

            for i in range(self.n_layer):
                if i >= len(all_info.get("winners", [])):
                    continue

                _cap_stats = self.capital_manager.update(
                    i, all_info["winners"][i], token_losses, all_info["costs"][i]
                )

                baseline = float(self.capital_manager.baseline_losses[i].item())
                perf = baseline - float(token_losses.mean().item())
                self.layer_performance_ema[i] = 0.95 * self.layer_performance_ema[i] + 0.05 * perf

                self.blocks[i].critic.settle(
                    all_info["affinities"][i],
                    all_info["winners"][i],
                    token_losses,
                    baseline,
                    reward_scale=reward_scale,
                    penalty_scale=penalty_scale,
                    critic_bonus_scale=critic_bonus_scale,
                    bonus_clip=bonus_clip,
                    critic_loss_signal=clipped_signal,
                    base_commission=base_commission,
                    dividend_scale=dividend_scale,
                    dividend_std_factor=dividend_std_factor,
                )

                # ÂÄ∫Âä°Ëá™Âä®ÂÅøËøò
                critic = self.blocks[i].critic
                if float(critic.debt.item()) > 0:
                    repay_base = torch.clamp(critic.capital - critic.init_capital * 0.2, min=0.0)
                    repay = torch.clamp(repay_base * repay_ratio, min=0.0, max=critic.debt)
                    critic.capital = critic.capital - repay
                    critic.debt = critic.debt - repay

                self._handle_critic_bankruptcy_and_restructure(i, step)
    
    def log_market_health(self) -> Dict:
        r"""log_market_health() -> Dict

        Ê±áÊÄªÊâÄÊúâÂ±ÇÁöÑÂ∏ÇÂú∫ÂÅ•Â∫∑ÊåáÊ†á„ÄÇ

        Returns:
          Dict: ÂåÖÂê´ RWKV/Transformer ‰ªΩÈ¢ù„ÄÅGini„ÄÅCritic ËµÑÊú¨Á≠âÊåáÊ†á„ÄÇ
        """
        metrics = {}
        for i in range(self.n_layer):
            caps = self.capital_manager.capitals[i]
            total_cap = caps.sum() + 1e-6
            
            rwkv_share = caps[:self.blocks[i].num_rwkv].sum() / total_cap * 100
            trans_share = caps[self.blocks[i].num_rwkv:].sum() / total_cap * 100
            
            sorted_caps, _ = torch.sort(caps)
            n = self.num_experts
            idx = torch.arange(1, n + 1, device=caps.device, dtype=caps.dtype)
            gini = ((2 * idx - n - 1) * sorted_caps).sum() / (n * total_cap + 1e-6)
            
            metrics[f"L{i}/TransShare"] = trans_share.item()
            metrics[f"L{i}/RWKVShare"] = rwkv_share.item()
            metrics[f"L{i}/Gini"] = gini.item()
            metrics[f"L{i}/CriticCap"] = self.blocks[i].critic.capital.item()
            metrics[f"L{i}/CriticDebt"] = self.blocks[i].critic.debt.item()
            metrics[f"L{i}/BailoutCount"] = self.blocks[i].critic.bailout_count.item()
            metrics[f"L{i}/AssetVelocity"] = self.capital_manager.get_asset_velocity(i).item()
            metrics[f"L{i}/QEInject"] = self.capital_manager.last_qe_inject[i].item()
            metrics[f"L{i}/QEDrain"] = self.capital_manager.last_qe_drain[i].item()
            metrics[f"L{i}/IdleTax"] = self.capital_manager.last_idle_tax[i].item()
            metrics[f"L{i}/Depreciation"] = self.capital_manager.last_depreciation[i].item()
            metrics[f"L{i}/ProfitFlow"] = self.capital_manager.last_profit_flow[i].item()
            metrics[f"L{i}/WealthTax"] = self.capital_manager.last_wealth_tax[i].item()
            metrics[f"L{i}/PerfEMA"] = self.layer_performance_ema[i].item()
            metrics[f"L{i}/CriticBonusSignal"] = float(self._last_critic_bonus_signal)
        
        return metrics

