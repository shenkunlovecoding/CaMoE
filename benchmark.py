"""
CaMoE v12 Benchmark Script (Folder Version)
ÈÄÇÈÖç: 2 RWKV + 2 Trans Êû∂ÊûÑ
ÊîØÊåÅ: ÊâπÈáèËØªÂèñÊñá‰ª∂Â§πÂÜÖÊâÄÊúâÊñáÊú¨Êñá‰ª∂
"""

import torch
import torch.nn.functional as F
import time
import os
import math
from pathlib import Path
from tqdm import tqdm
from camoe import CaMoE_System
from config import *
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER

# ================= ÈÖçÁΩÆÂå∫Âüü =================
DATA_FOLDER = "data/dev"  # Êîπ‰∏∫Êñá‰ª∂Â§πË∑ØÂæÑ
MODEL_PATH = "checkpoints/babylm/v12_step16000.pth"
SCALE = "0.1b"
DEVICE = "cuda"
CTX_LEN = 512
BATCH_SIZE = 16
CHUNK_LEN = 16

# Êñá‰ª∂ËøáÊª§ÈÖçÁΩÆ
FILE_EXTENSIONS = ['.txt', '.dev', '.train', '.json']  # ÊîØÊåÅÁöÑÊñá‰ª∂ÂêéÁºÄ
RECURSIVE = False  # ÊòØÂê¶ÈÄíÂΩíËØªÂèñÂ≠êÊñá‰ª∂Â§π
MAX_FILES = None   # ÈôêÂà∂ÊúÄÂ§ßËØªÂèñÊñá‰ª∂Êï∞ (None=Êó†ÈôêÂà∂)

# ===========================================

def load_data_generator(folder_path, tokenizer, ctx_len, batch_size):
    """
    ‰ªéÊñá‰ª∂Â§πÂä†ËΩΩÊâÄÊúâÊñáÊú¨Êñá‰ª∂Âπ∂ÁîüÊàêÊï∞ÊçÆÂä†ËΩΩÂô®
    ÊîØÊåÅÂ§öÊñá‰ª∂ÂêàÂπ∂„ÄÅËá™Âä®ËøáÊª§„ÄÅËøõÂ∫¶ÊòæÁ§∫
    """
    if not os.path.exists(folder_path):
        print(f"‚ùå Error: Folder not found at {folder_path}")
        return None, 0

    # Êî∂ÈõÜÊâÄÊúâÁ¨¶ÂêàÊù°‰ª∂ÁöÑÊñá‰ª∂
    folder = Path(folder_path)
    files = []
    
    if RECURSIVE:
        # ÈÄíÂΩíÊêúÁ¥¢
        for ext in FILE_EXTENSIONS:
            files.extend(folder.rglob(f"*{ext}"))
    else:
        # ‰ªÖÂΩìÂâçÊñá‰ª∂Â§π
        for ext in FILE_EXTENSIONS:
            files.extend(folder.glob(f"*{ext}"))
    
    # ÂéªÈáçÂπ∂ÊéíÂ∫è
    files = sorted(list(set(files)))
    
    if MAX_FILES:
        files = files[:MAX_FILES]
    
    if not files:
        print(f"‚ùå Error: No {FILE_EXTENSIONS} files found in {folder_path}")
        return None, 0
    
    print(f"üìÇ Found {len(files)} files in {folder_path}")
    if RECURSIVE:
        print(f"   (Recursive mode enabled)")
    
    # ÈÄêÊñá‰ª∂ËØªÂèñÂπ∂ tokenize
    all_tokens = []
    file_stats = []
    
    for fpath in tqdm(files, desc="üî§ Tokenizing files", unit="file"):
        try:
            # Ê†πÊçÆÂêéÁºÄÈÄâÊã©ËØªÂèñÊñπÂºè
            suffix = fpath.suffix.lower()
            
            if suffix == '.jsonl':
                # JSON Lines Ê†ºÂºèÔºöÊØèË°å‰∏Ä‰∏™jsonÔºåÂèñ"text"Â≠óÊÆµ
                import json
                texts = []
                with open(fpath, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            if isinstance(data, dict) and 'text' in data:
                                texts.append(data['text'])
                            elif isinstance(data, str):
                                texts.append(data)
                        except json.JSONDecodeError:
                            continue
                text = '\n'.join(texts)
            elif suffix == '.json':
                # Ê†áÂáÜ JSONÔºöÂ∞ùËØïËØªÂèñ text Êàñ content Â≠óÊÆµÔºåÂê¶ÂàôËØªÂèñÊï¥‰∏™Êñá‰ª∂
                import json
                with open(fpath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list) and len(data) > 0:
                        if isinstance(data[0], dict):
                            text = '\n'.join([item.get('text', item.get('content', str(item))) for item in data])
                        else:
                            text = '\n'.join([str(item) for item in data])
                    elif isinstance(data, dict):
                        text = data.get('text', data.get('content', str(data)))
                    else:
                        text = str(data)
            else:
                # ÊôÆÈÄöÊñáÊú¨Êñá‰ª∂
                with open(fpath, 'r', encoding='utf-8') as f:
                    text = f.read()
            
            # Tokenize
            tokens = tokenizer.encode(text)
            all_tokens.extend(tokens)
            file_stats.append((fpath.name, len(tokens)))
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Failed to process {fpath}: {e}")
            continue
    
    if not all_tokens:
        print("‚ùå Error: No valid tokens extracted from files")
        return None, 0
    
    # ÊâìÂç∞Êñá‰ª∂ÁªüËÆ°
    print(f"\nüìä Dataset Statistics:")
    print(f"   Total files processed: {len(file_stats)}")
    print(f"   Total tokens: {len(all_tokens):,}")
    
    # ÊòæÁ§∫Êñá‰ª∂Â§ßÂ∞èÂàÜÂ∏ÉÔºàÂâç5Â§ßÔºâ
    if len(file_stats) > 0:
        file_stats.sort(key=lambda x: x[1], reverse=True)
        print(f"\nüìÅ Top 5 largest files:")
        for fname, tok_count in file_stats[:5]:
            print(f"   - {fname}: {tok_count:,} tokens")
    
    # ÊûÑÂª∫ batch Êï∞ÊçÆ
    total_tokens = len(all_tokens)
    stride = ctx_len
    num_batches = total_tokens // (batch_size * stride)
    
    if num_batches == 0:
        print(f"‚ùå Error: Not enough tokens ({total_tokens}) for one batch (need {batch_size * stride})")
        return None, 0
    
    # Êà™Êñ≠Âà∞ batch Êï¥Êï∞ÂÄç
    limit = num_batches * batch_size * stride
    data = torch.tensor(all_tokens[:limit], dtype=torch.long)
    data = data.view(num_batches, batch_size, stride)
    
    print(f"üì¶ Batches created: {num_batches} (batch_size={batch_size}, ctx_len={ctx_len})")
    print(f"   Actual tokens used: {limit:,} ({limit/total_tokens*100:.1f}% of total)\n")
    
    return data, num_batches

def main():
    # 1. Load Config & Model
    config = CONFIG_BABYLM if SCALE == "0.1b" else CONFIG_04B
    config['ctx_len'] = CTX_LEN
    
    NUM_RWKV = config.get('num_rwkv_experts', 2)
    NUM_TRANS = config.get('num_trans_experts', 2)
    NUM_EXPERTS = NUM_RWKV + NUM_TRANS
    
    print(f"üèóÔ∏è Loading model from {MODEL_PATH}...")
    print(f"‚öôÔ∏è Config: {NUM_RWKV} RWKV + {NUM_TRANS} Trans")
    
    model = CaMoE_System(config).to(DEVICE)
    
    checkpoint = torch.load(MODEL_PATH, map_location='cpu')
    if 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    
    # 2. Tokenizer
    tokenizer = TRIE_TOKENIZER(config['vocab_file'])

    # Âú® Benchmark ËÑöÊú¨ÂºÄÂ§¥Âä†ÂÖ•Ôºö
    test_str = "Once upon a time"
    tokens = tokenizer.encode(test_str)
    print(f"Tokens: {tokens}")
    print(f"Vocab size in config: {config['vocab_size']}")
    print(f"Max token ID in this sample: {max(tokens)}")
    assert max(tokens) < config['vocab_size'], "Vocab size mismatch!"
    
    # 3. Data Loader (Êñá‰ª∂Â§πÁâàÊú¨)
    data_loader, num_batches = load_data_generator(DATA_FOLDER, tokenizer, CTX_LEN, BATCH_SIZE)
    if data_loader is None:
        return

    # 4. Stats Init
    total_nll = 0.0
    total_tokens_processed = 0
    total_characters_processed = 0
    print("üìè Calculating character count for the evaluation set...")
    
    # Ëé∑Âèñ data_loader Ê∂µÁõñÁöÑÊâÄÊúâ tokens
    all_eval_tokens = data_loader.view(-1).tolist()
    
    # ‰∏∫‰∫ÜÈÅøÂÖçÂ§ßÂÜÖÂ≠òÂç†Áî®ÔºåÂàÜÂùó decode
    chunk_size = 10000
    total_characters_processed = 0
    
    for i in tqdm(range(0, len(all_eval_tokens), chunk_size), desc="Decoding to chars", leave=False):
        chunk = all_eval_tokens[i:i+chunk_size]
        text_chunk = tokenizer.decode(chunk)
        total_characters_processed += len(text_chunk)
    
    print(f"üìä Total tokens: {len(all_eval_tokens)}")
    print(f"üìä Total characters: {total_characters_processed}")
    # ËÆ°ÁÆóÂéãÁº©ÁéáÔºöÂπ≥ÂùáÊØè‰∏™ token ‰ª£Ë°®Â§öÂ∞ë‰∏™Â≠óÁ¨¶
    char_per_token = total_characters_processed / len(all_eval_tokens)
    print(f"üìä Ratio: {char_per_token:.3f} characters per token")

    start_time = time.time()
    
    # ‰∏ìÂÆ∂‰ΩøÁî®ÁªüËÆ° (ÊØèÂ±ÇÊØè‰∏ìÂÆ∂)
    layer_expert_counts = {l: {e: 0 for e in range(NUM_EXPERTS)} for l in range(config['n_layer'])}
    total_decisions = 0
    
    print(f"\nüöÄ Start Benchmarking (Batches: {num_batches})...")
    
    # 5. Eval Loop
    with torch.no_grad():
        pbar = tqdm(data_loader, total=num_batches, desc="Benchmarking")
        for batch in pbar:
            batch = batch.to(DEVICE)
            B, T = batch.shape
            
            if T % CHUNK_LEN != 0:
                T_new = (T // CHUNK_LEN) * CHUNK_LEN
                batch = batch[:, :T_new]
            
            logits, info = model(batch, step=30000, phase="normal")
            
            # Loss
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = batch[:, 1:].contiguous()
            mask = (shift_labels != 0).float() 
            loss = F.cross_entropy(
                shift_logits.view(-1, config['vocab_size']), 
                shift_labels.view(-1), 
                reduction='none'
            )
            loss = (loss * mask.view(-1)).sum()
            total_tokens_processed += mask.sum().item()
            total_nll += loss.item()
            
            # ‰∏ìÂÆ∂‰ΩøÁî®ÁªüËÆ°
            for layer_idx, winners in enumerate(info['winners']):
                for e in range(NUM_EXPERTS):
                    count = (winners == e).sum().item()
                    layer_expert_counts[layer_idx][e] += count
            
            total_decisions += winners.numel() * config['n_layer']
            
            curr_ppl = math.exp(total_nll / total_tokens_processed)
            pbar.set_postfix({'PPL': f"{curr_ppl:.3f}"})

    # 6. Final Report
    end_time = time.time()
    duration = end_time - start_time
    tps = total_tokens_processed / duration
    final_ppl = math.exp(total_nll / total_tokens_processed)
    
    # 2. ËÆ°ÁÆó BPC (Bits Per Character) - Ë°å‰∏öÊ†áÂáÜÊåáÊ†á
    # ‰ΩøÁî® log2 Â∞Ü NLL ËΩ¨Êç¢‰∏∫ bits
    bpc = (total_nll / math.log(2)) / total_characters_processed
    
    # 3. ËÆ°ÁÆó‚ÄúÁ≠âÊïà Per-Character PPL‚Äù
    # ÂÅáËÆæÂ¶ÇÊûúÊ®°ÂûãÊòØÊåâÂ≠óÁ¨¶È¢ÑÊµãÁöÑÔºåÂÆÉÁöÑ PPL ‰ºöÊòØÂ§öÂ∞ë
    ppl_char = math.exp(total_nll / total_characters_processed)

    print("-" * 60)
    print(f"üìâ BPC (Bits Per Character): {bpc:.4f}")
    print(f"üìâ Equivalent Char-PPL:    {ppl_char:.4f}")
    
    def get_expert_name(e):
        return f"R{e}" if e < NUM_RWKV else f"T{e - NUM_RWKV}"
    
    print("\n" + "="*60)
    print(f"üèÜ BENCHMARK RESULT (CaMoE v12 - {SCALE.upper()})")
    print(f"üìÅ Data Source: {DATA_FOLDER}")
    print("="*60)
    print(f"‚úÖ Final PPL:        {final_ppl:.4f}")
    print(f"‚è±Ô∏è  Speed (TPS):      {tps:.0f} tokens/s")
    print(f"üî¢ Total Tokens:     {total_tokens_processed:,}")
    print("-" * 60)
    
    # ‰∏ìÂÆ∂Ë°®Â§¥
    header = "Layer |"
    for e in range(NUM_EXPERTS):
        header += f" {get_expert_name(e):>6} |"
    print(header)
    print("-" * 60)
    
    # ÊØèÂ±ÇÁªüËÆ°
    rwkv_total = 0
    trans_total = 0
    
    for l in range(config['n_layer']):
        layer_total = sum(layer_expert_counts[l].values())
        row = f" L{l:02d}  |"
        for e in range(NUM_EXPERTS):
            pct = layer_expert_counts[l][e] / layer_total * 100 if layer_total > 0 else 0
            row += f" {pct:5.1f}% |"
            
            if e < NUM_RWKV:
                rwkv_total += layer_expert_counts[l][e]
            else:
                trans_total += layer_expert_counts[l][e]
        print(row)
    
    print("-" * 60)
    
    # Ê±áÊÄª
    grand_total = rwkv_total + trans_total
    print(f"üìä RWKV Total: {rwkv_total/grand_total*100:.1f}%")
    print(f"üìä Trans Total: {trans_total/grand_total*100:.1f}%")
    print(f"‚è±Ô∏è  Total Time: {duration:.2f}s")
    print("="*60)

if __name__ == "__main__":
    main()