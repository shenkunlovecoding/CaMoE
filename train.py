"""
CaMoE v21.1 è®­ç»ƒè„šæœ¬
æ”¯æŒ: ä¸ƒé˜¶æ®µè°ƒåº¦ / åˆ†ç»„å­¦ä¹ ç‡ / åˆ†é˜¶æ®µæ•°æ® profile / ç»æµç³»ç»Ÿå¢å¼º / Eval Loss
"""

import os
import gc
import time
import argparse
from typing import Dict, Iterator, List, Tuple, Any
import torch
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from datasets import load_from_disk, Dataset, DatasetDict, interleave_datasets
import bitsandbytes as bnb
from CaMoE.backbone import init_rwkv7_cuda
try:
    import swanlab
    HAS_SWANLAB = True
except ImportError:
    HAS_SWANLAB = False

from CaMoE.system import CaMoE_System
from CaMoE.config import get_config, VERSION


def load_backbone(model: CaMoE_System, path: str) -> None:
    r"""load_backbone(model, path) -> None

    ä» RWKV åº•æ¨¡è¿ç§»å¯å¯¹é½æƒé‡åˆ° CaMoE æ¶æ„ã€‚

    Args:
      model (CaMoE_System): å½“å‰ CaMoE æ¨¡å‹ã€‚
      path (str): RWKV åº•æ¨¡æƒé‡è·¯å¾„ã€‚
    """
    if not os.path.exists(path):
        print(f"âš ï¸ Weights not found: {path} (Starting from scratch)")
        return
    
    print(f"ğŸ“¦ Loading backbone from {path}...")
    official = torch.load(path, map_location='cpu', weights_only=True)
    my_dict = model.state_dict()
    loaded = 0
    
    for k, v in official.items():
        if k in my_dict and my_dict[k].shape == v.shape:
            my_dict[k].copy_(v)
            loaded += 1
            continue
        
        if 'blocks' in k:
            try:
                parts = k.split('.')
                lid = int(parts[1])
                layer_type = parts[2]
                
                if layer_type == 'att':
                    target_name = f"blocks.{lid}.att.{'.'.join(parts[3:])}"
                    if target_name in my_dict and my_dict[target_name].shape == v.shape:
                        my_dict[target_name].copy_(v)
                        loaded += 1
                
                elif layer_type == 'ffn':
                    param_name = '.'.join(parts[3:])
                    for i in range(model.num_rwkv_experts):
                        target = f"blocks.{lid}.experts.{i}.{param_name}"
                        if target in my_dict and my_dict[target].shape == v.shape:
                            noise = torch.randn_like(v) * 0.01
                            my_dict[target].copy_(v + noise)
                            if i == 0: loaded += 1
            except Exception as e:
                pass
    
    model.load_state_dict(my_dict, strict=False)
    print(f"âœ… Loaded matching tensors (~{loaded})")

def _build_phase_plan(config: Dict) -> List[Dict[str, Any]]:
    schedule = config.get("phase_schedule")
    if schedule:
        plan = []
        cursor = 0
        for phase in schedule:
            item = dict(phase)
            steps = max(0, int(item.get("steps", 0)))
            item["steps"] = steps
            item["start_step"] = cursor
            item["end_step"] = cursor + steps
            item.setdefault("use_market", True)
            item.setdefault("route_grad", True)
            plan.append(item)
            cursor += steps
        return plan

    # å…¼å®¹æ—§é…ç½®ï¼šä¸‰é˜¶æ®µ
    pre = int(config.get("prewarm_steps", 100))
    warm_end = int(config.get("warmup_steps", 500))
    warm = max(0, warm_end - pre)
    total = int(config.get("total_steps", warm_end + 1000))
    normal = max(0, total - warm_end)
    return [
        {
            "name": "prewarm",
            "steps": pre,
            "start_step": 0,
            "end_step": pre,
            "data_profile": "default",
            "train_groups": ["all"],
            "lr_mult": {g: 1.0 for g in config.get("param_groups", [])},
            "market_update": False,
            "use_market": False,
            "route_grad": True,
        },
        {
            "name": "warmup",
            "steps": warm,
            "start_step": pre,
            "end_step": pre + warm,
            "data_profile": "default",
            "train_groups": ["all"],
            "lr_mult": {g: 1.0 for g in config.get("param_groups", [])},
            "market_update": False,
            "use_market": True,
            "route_grad": True,
        },
        {
            "name": "normal",
            "steps": normal,
            "start_step": pre + warm,
            "end_step": pre + warm + normal,
            "data_profile": "default",
            "train_groups": ["all"],
            "lr_mult": {g: 1.0 for g in config.get("param_groups", [])},
            "market_update": True,
            "use_market": True,
            "route_grad": True,
        },
    ]


def _phase_total_steps(phase_plan: List[Dict[str, Any]]) -> int:
    return int(sum(max(0, int(p.get("steps", 0))) for p in phase_plan))


def get_phase(step: int, phase_plan: List[Dict[str, Any]]) -> Dict[str, Any]:
    r"""get_phase(step, phase_plan) -> Dict"""
    for phase in phase_plan:
        if step < phase["end_step"]:
            return phase
    # è¶…èŒƒå›´æ—¶å›é€€æœ€åä¸€ä¸ªæœ‰æ­¥æ•°çš„é˜¶æ®µ
    for phase in reversed(phase_plan):
        if phase.get("steps", 0) > 0:
            return phase
    return phase_plan[-1] if phase_plan else {
        "name": "normal",
        "data_profile": "default",
        "market_update": True,
        "use_market": True,
        "route_grad": True,
    }


def _classify_param_group(name: str, num_rwkv: int) -> str:
    if ".experts." in name and ".confidence." in name:
        return "router_conf"
    if ".critic." in name:
        return "critic"
    if name.startswith("bridge."):
        return "bridge"
    if name.startswith("emb.") or name.startswith("ln_out.") or name.startswith("head.") or name.startswith("deep_embed."):
        return "emb_head"

    if name.startswith("blocks."):
        parts = name.split(".")
        if len(parts) > 3 and parts[2] == "experts":
            try:
                expert_idx = int(parts[3])
                return "rwkv_experts" if expert_idx < num_rwkv else "trans_experts"
            except ValueError:
                pass
        return "rwkv_backbone"

    return "rwkv_backbone"


def build_param_groups(model: CaMoE_System, config: Dict) -> Tuple[List[Dict[str, Any]], Dict[str, List[torch.nn.Parameter]]]:
    r"""build_param_groups(model, config) -> (optimizer_param_groups, group_map)"""
    groups = {g: [] for g in config.get("param_groups", [
        "rwkv_backbone", "router_conf", "rwkv_experts", "trans_experts", "bridge", "critic", "emb_head"
    ])}
    num_rwkv = int(config.get("num_rwkv_experts", 6))

    for name, p in model.named_parameters():
        g = _classify_param_group(name, num_rwkv)
        if g not in groups:
            groups[g] = []
        groups[g].append(p)

    base_lr = float(config.get("base_lr", 1e-4))
    optim_groups = []
    for gname, params in groups.items():
        if not params:
            continue
        optim_groups.append({
            "params": params,
            "lr": base_lr,
            "name": gname,
        })
    return optim_groups, groups


def apply_phase_policy(
    optimizer,
    phase: Dict[str, Any],
    config: Dict,
    group_map: Dict[str, List[torch.nn.Parameter]],
) -> None:
    r"""apply_phase_policy(optimizer, phase, config, group_map) -> None"""
    base_lr = float(config.get("base_lr", 1e-4))
    train_groups = set(phase.get("train_groups", ["all"]))
    train_all = "all" in train_groups
    lr_mult = phase.get("lr_mult", {})

    for gname, params in group_map.items():
        active = train_all or (gname in train_groups)
        for p in params:
            p.requires_grad = active

    for pg in optimizer.param_groups:
        gname = pg.get("name", "")
        mult = float(lr_mult.get(gname, 1.0 if (train_all or gname in train_groups) else 0.0))
        if not (train_all or gname in train_groups):
            mult = 0.0
        pg["lr"] = base_lr * mult


def apply_route_grad_policy(model: CaMoE_System, phase: Dict[str, Any], config: Dict) -> None:
    r"""apply_route_grad_policy(model, phase, config) -> None"""
    default_route_no_grad = bool(config.get("route_no_grad", True))
    route_grad = bool(phase.get("route_grad", not default_route_no_grad))
    route_no_grad = not route_grad

    for block in model.blocks:
        block.route_no_grad = route_no_grad


def _load_profile_datasets(config: Dict, profile_name: str) -> Tuple[Dataset, Dataset]:
    data_profiles = config.get("data_profiles") or {}
    profile = data_profiles.get(profile_name, {})

    if profile_name == "default" and not profile:
        profile = {
            "mix": config.get("mix"),
            "data_path": config.get("data_path"),
        }

    mix = profile.get("mix")
    data_path = profile.get("data_path", config.get("data_path"))
    data_roots = config.get("data_roots") or {}

    if mix and data_roots:
        train_datasets = []
        val_datasets = []
        probs = []
        loaded_names = []

        for name, prob in mix.items():
            if prob <= 0:
                continue
            path = data_roots.get(name)
            if not path or not os.path.exists(path):
                print(f"âš ï¸ Dataset not found: {path}, skipping {name}.")
                continue

            ds = load_from_disk(path)
            if isinstance(ds, DatasetDict):
                tr = ds["train"]
                va = ds.get("validation") or ds.get("test")
                if va is None:
                    split = tr.train_test_split(test_size=0.01, seed=42)
                    tr, va = split["train"], split["test"]
            else:
                split = ds.train_test_split(test_size=0.01, seed=42)
                tr, va = split["train"], split["test"]

            tr.set_format(type="torch", columns=["input_ids"])
            va.set_format(type="torch", columns=["input_ids"])
            train_datasets.append(tr)
            val_datasets.append(va)
            probs.append(float(prob))
            loaded_names.append(name)
            print(f"  - [{profile_name}] {name}: train={len(tr)}, val={len(va)} (prob={prob})")

        if not train_datasets:
            raise ValueError(f"No valid datasets in profile '{profile_name}'.")

        total_p = sum(probs)
        probs = [p / total_p for p in probs]
        train_data = interleave_datasets(
            train_datasets,
            probabilities=probs,
            seed=42,
            stopping_strategy="all_exhausted",
        )
        val_data = interleave_datasets(
            val_datasets,
            probabilities=probs,
            seed=42,
            stopping_strategy="first_exhausted",
        )
        print(f"ğŸ“Š Profile={profile_name} Mix: {dict(zip(loaded_names, probs))} -> Train={len(train_data)}, Val={len(val_data)}")
        return train_data, val_data

    # å•æ•°æ®é›†
    if not data_path:
        raise ValueError(f"Profile '{profile_name}' has no mix and no data_path.")
    raw_dataset = load_from_disk(data_path)
    if isinstance(raw_dataset, DatasetDict):
        if "validation" in raw_dataset:
            train_data, val_data = raw_dataset["train"], raw_dataset["validation"]
        elif "test" in raw_dataset:
            train_data, val_data = raw_dataset["train"], raw_dataset["test"]
        else:
            split = raw_dataset["train"].train_test_split(test_size=0.05, seed=42)
            train_data, val_data = split["train"], split["test"]
    elif isinstance(raw_dataset, Dataset):
        split = raw_dataset.train_test_split(test_size=0.05, seed=42)
        train_data, val_data = split["train"], split["test"]
    else:
        raise ValueError("Unknown dataset type")

    train_data.set_format(type="torch", columns=["input_ids"])
    val_data.set_format(type="torch", columns=["input_ids"])
    print(f"ğŸ“Š Profile={profile_name}: Train={len(train_data)}, Val={len(val_data)}")
    return train_data, val_data


def build_loader_for_profile(
    profile_name: str,
    config: Dict,
    collate_fn,
) -> Tuple[DataLoader, DataLoader]:
    r"""build_loader_for_profile(profile_name, config, collate_fn) -> (train_loader, val_loader)"""
    train_data, val_data = _load_profile_datasets(config, profile_name)
    train_loader = DataLoader(
        train_data,
        batch_size=config["micro_batch_size"],
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_data,
        batch_size=config["micro_batch_size"],
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn,
        pin_memory=True,
    )
    return train_loader, val_loader

def log_gpu() -> str:
    r"""log_gpu() -> str

    è¿”å›å½“å‰ GPU æ˜¾å­˜å ç”¨æ‘˜è¦å­—ç¬¦ä¸²ã€‚

    Returns:
      str: æ˜¾å­˜ä¿¡æ¯ï¼›æ—  CUDA æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    if torch.cuda.is_available():
        alloc = torch.cuda.memory_allocated() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return f"GPU: {alloc:.1f}/{total:.1f}GB"
    return ""

def infinite_loader(loader: DataLoader) -> Iterator:
    r"""infinite_loader(loader) -> Iterator

    å°†æœ‰é™ DataLoader åŒ…è£…ä¸ºæ— é™è¿­ä»£å™¨ã€‚

    Args:
      loader (DataLoader): åŸå§‹æ•°æ®åŠ è½½å™¨ã€‚

    Returns:
      Iterator: å¾ªç¯äº§å‡º batch çš„è¿­ä»£å™¨ã€‚
    """
    while True:
        for batch in loader:
            yield batch


def _detach_market_info(info: Dict[str, List[torch.Tensor]]) -> Dict[str, List[torch.Tensor]]:
    r"""_detach_market_info(info) -> Dict

    å°† market ç›¸å…³å¼ é‡ä» autograd å›¾ä¸­åˆ†ç¦»ï¼Œé¿å… grad_accum é˜¶æ®µé¢å¤–ä¿å›¾ã€‚
    """
    detached = {}
    for k, v in info.items():
        if isinstance(v, list):
            detached[k] = [x.detach() if torch.is_tensor(x) else x for x in v]
        else:
            detached[k] = v.detach() if torch.is_tensor(v) else v
    return detached


def _merge_market_infos(info_chunks: List[Dict[str, List[torch.Tensor]]]) -> Dict[str, List[torch.Tensor]]:
    r"""_merge_market_infos(info_chunks) -> Dict

    å°†å¤šä¸ª micro-batch çš„è·¯ç”±ä¿¡æ¯æ²¿ batch ç»´æ‹¼æ¥ï¼Œä¾›ä¸€æ¬¡ market_update ä½¿ç”¨ã€‚
    """
    if not info_chunks:
        return {}

    merged: Dict[str, List[torch.Tensor]] = {}
    keys = info_chunks[0].keys()
    for k in keys:
        first = info_chunks[0][k]
        if not isinstance(first, list):
            merged[k] = first
            continue
        nlayers = len(first)
        merged[k] = []
        for lid in range(nlayers):
            parts = [chunk[k][lid] for chunk in info_chunks if lid < len(chunk[k])]
            merged[k].append(torch.cat(parts, dim=0) if len(parts) > 1 else parts[0])
    return merged

def main() -> None:
    r"""main() -> None

    è®­ç»ƒä¸»å…¥å£ï¼ŒåŒ…å«æ•°æ®åŠ è½½ã€æ–­ç‚¹ç»­è®­ã€é˜¶æ®µè®­ç»ƒã€è¯„ä¼°ä¸ä¿å­˜ã€‚
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--scale", default="0.4b")
    parser.add_argument(
        "--diag",
        default="baseline",
        choices=["baseline", "no_amp", "no_fast_math", "fp32_kernel"],
        help="Diagnostic switches for isolating NaN source",
    )
    parser.add_argument("--resume", type=str, default=None, help="Path to checkpoint to resume from")
    args = parser.parse_args()
    
    config = get_config(args.scale)

    # Diagnostic overrides (switch-only; no model structure changes)
    if args.diag == "no_amp":
        config["train_use_amp"] = False
    elif args.diag == "no_fast_math":
        config["cuda_use_fast_math"] = False
    elif args.diag == "fp32_kernel":
        config["cuda_use_fast_math"] = False
        config["cuda_force_fp32_kernel"] = True

    # Pass diagnostic kernel switches through env for backbone.init_rwkv7_cuda()
    os.environ["CAMOE_DISABLE_FAST_MATH"] = "1" if not config.get("cuda_use_fast_math", True) else "0"
    os.environ["CAMOE_FORCE_FP32_KERNEL"] = "1" if config.get("cuda_force_fp32_kernel", False) else "0"
    os.environ["CAMOE_NAN_DEBUG"] = "1" if config.get("nan_debug", False) else "0"
    os.environ["CAMOE_SANITIZE_TIMEMIX_OUT"] = "1" if config.get("sanitize_timemix_output", False) else "0"
    os.environ["CAMOE_FORCE_TIMEMIX_FALLBACK"] = "1" if config.get("force_timemix_fallback", False) else "0"
    init_rwkv7_cuda()
    
    # å¼ºåˆ¶è®¾ç½® Eval é¢‘ç‡
    eval_interval = config.get('eval_interval', 1000)  # æ¯500æ­¥è¯„æµ‹ä¸€æ¬¡
    eval_iters = config.get('eval_iters', 50)         # æ¯æ¬¡è¯„æµ‹è·‘50ä¸ªbatch
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    torch.set_float32_matmul_precision('high')

    train_use_amp = bool(config.get("train_use_amp", True)) and device == "cuda"
    amp_dtype_name = str(config.get("amp_dtype", "bfloat16"))
    amp_dtype = torch.bfloat16 if amp_dtype_name == "bfloat16" else torch.float16
    print(
        f"ğŸ§ª Diag={args.diag} | AMP={train_use_amp}({amp_dtype_name}) | "
        f"fast_math={config.get('cuda_use_fast_math', True)} | "
        f"fp32_kernel={config.get('cuda_force_fp32_kernel', False)}"
    )

    phase_plan = _build_phase_plan(config)
    config["phase_schedule"] = phase_plan
    total_steps_from_schedule = _phase_total_steps(phase_plan)
    if total_steps_from_schedule > 0:
        config["total_steps"] = total_steps_from_schedule
    phase_to_id = {p.get("name", f"phase_{i}"): i for i, p in enumerate(phase_plan)}

    # 2. DataLoader Collate
    def simple_collate(batch) -> torch.Tensor:
        r"""simple_collate(batch) -> Tensor

        å°†å˜é•¿æ ·æœ¬æ‹¼æ¥ä¸ºå›ºå®šé•¿åº¦ batchï¼Œå¹¶æŒ‰ CUDA kernel çº¦æŸè¿›è¡Œé•¿åº¦å¯¹é½ã€‚

        Args:
          batch: åŸå§‹æ ·æœ¬åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« ``input_ids``ã€‚

        Returns:
          Tensor: å½¢çŠ¶ ``[B, T]`` çš„ long å¼ é‡ã€‚
        """
        input_ids = [item["input_ids"] for item in batch]
        max_len = max(len(ids) for ids in input_ids)
        max_len = min(max_len, config['ctx_len'] + 1)
        
        # [CUDA Kernel è¦æ±‚] å¯¹é½åˆ° 16 çš„å€æ•° + 1
        CHUNK_LEN = 16
        input_len = ((max_len - 1 + CHUNK_LEN - 1) // CHUNK_LEN) * CHUNK_LEN
        target_len = max(input_len + 1, CHUNK_LEN + 1)
        
        padded_batch = torch.full((len(batch), target_len), -100, dtype=torch.long)
        for i, ids in enumerate(input_ids):
            l = min(len(ids), target_len)
            padded_batch[i, :l] = ids[:l]
        return padded_batch

    # 3. Model & Optimizer
    print("ğŸ—ï¸ Building model...")
    model = CaMoE_System(config).to(device)

    optimizer_groups, group_map = build_param_groups(model, config)
    optimizer = bnb.optim.AdamW8bit(optimizer_groups, lr=config.get("base_lr", 1e-4))

    # ==========================================
    # æ–­ç‚¹ç»­è®­é€»è¾‘
    # ==========================================
        # ==========================================
    # æƒé‡åŠ è½½é€»è¾‘ (é€‚é… MiniPile Init)
    # ==========================================
    start_step = 0
    
    # 1. ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ˜¾å¼æŒ‡å®šçš„ Resume è·¯å¾„
    resume_path = args.resume
    
    # 2. å¦‚æœæ²¡æŒ‡å®š resumeï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ MiniPile åˆå§‹åŒ–æƒé‡ (æ¸…æ´—ç‰ˆ)
    if not resume_path:
        # å‡è®¾ä½ æŠŠæ¸…æ´—åçš„æƒé‡æ”¾åœ¨è¿™é‡Œï¼Œåå­—å›ºå®š
        minipile_init_path = f"checkpoints/{config['version']}_{config['scale']}/init.pth"
        if os.path.exists(minipile_init_path):
            print(f"âœ¨ Found init checkpoint: {minipile_init_path}")
            resume_path = minipile_init_path
    
    checkpoint = None
    if resume_path and os.path.exists(resume_path):
        print(f"ğŸ“¦ Loading checkpoint from {resume_path}...")
        checkpoint = torch.load(resume_path, map_location='cpu')
        
        # åŠ è½½æ¨¡å‹æƒé‡
        if isinstance(checkpoint, dict) and 'model' in checkpoint:
            # strict=False å…è®¸ä¸€äº›å¾®å°çš„ key å·®å¼‚ï¼Œä½†ä¸»è¦æƒé‡å¿…é¡»åŒ¹é…
            model.load_state_dict(checkpoint['model'], strict=False)
            print("âœ… Model weights loaded.")
            
            # å°è¯•åŠ è½½ä¼˜åŒ–å™¨ (å¦‚æœæœ‰)
            if 'optimizer' in checkpoint:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer'])
                    print("âœ… Optimizer state restored.")
                except Exception as e:
                    print(f"âš ï¸ Optimizer load failed (expected for init weights): {e}")
            else:
                print("â„¹ï¸ No optimizer state found (Fresh start).")
            
            # å°è¯•æ¢å¤æ­¥æ•° (å¦‚æœæ˜¯ init æƒé‡ï¼Œstep åº”è¯¥æ˜¯ 0)
            if 'step' in checkpoint:
                start_step = checkpoint['step']
                # å¦‚æœæ˜¯ step 40000 è¿™ç§ç»“æŸç‚¹ï¼Œæˆ‘ä»¬è¦å¼ºè¡Œé‡ç½®ä¸º 0
                # åªæœ‰å½“å®ƒæ˜¯ä¸­é—´å­˜æ¡£æ—¶æ‰ç»§ç»­
                if "init" in resume_path or start_step >= config['total_steps']:
                    print(f"ğŸ”„ Resetting step from {start_step} to 0 for new training phase.")
                    start_step = 0
                else:
                    start_step += 1
                    print(f"ğŸ”„ Resuming from step {start_step}")
        else:
            # æ—§æ ¼å¼
            model.load_state_dict(checkpoint, strict=False)
            print("âš ï¸ Loaded weights only (Legacy format).")
            
    else:
        # 3. æ—¢æ²¡ Resume ä¹Ÿæ²¡ Initï¼Œæ‰å»åŠ è½½ RWKV åº•æ¨¡
        print("ğŸŒ± No checkpoint found. Loading RWKV backbone...")
        load_backbone(model, config['weights_path'])

    # ==========================================
    # æŒ‰å½“å‰ step å¯¹é½é˜¶æ®µä¸æ•°æ® profile
    # ==========================================
    current_phase = get_phase(start_step, phase_plan)
    current_profile = current_phase.get("data_profile", "default")
    print(f"ğŸš€ Loading datasets for phase={current_phase.get('name')} profile={current_profile} ...")
    try:
        train_loader, val_loader = build_loader_for_profile(current_profile, config, simple_collate)
    except Exception as e:
        print(f"âŒ Error loading dataset profile '{current_profile}': {e}")
        return
    train_iter = infinite_loader(train_loader)
    
    # ==========================================
    # [æ–°å¢] è¯„ä¼°å‡½æ•°
    # ==========================================
    @torch.no_grad()
    def estimate_loss(model: CaMoE_System, loader: DataLoader, eval_steps: int) -> float:
        r"""estimate_loss(model, loader, eval_steps) -> float

        åœ¨éªŒè¯é›†ä¸Šä¼°ç®—å¹³å‡äº¤å‰ç†µæŸå¤±ã€‚

        Args:
          model (CaMoE_System): å¾…è¯„ä¼°æ¨¡å‹ã€‚
          loader (DataLoader): éªŒè¯é›†åŠ è½½å™¨ã€‚
          eval_steps (int): è¯„ä¼°æ‰¹æ¬¡æ•°ã€‚

        Returns:
          float: å¹³å‡éªŒè¯æŸå¤±ï¼›è‹¥æ— æœ‰æ•ˆå€¼åˆ™è¿”å› ``inf``ã€‚
        """
        model.eval()
        losses = []
        
        # ä½¿ç”¨ itertools.cycle æ— é™å¾ªç¯éªŒè¯é›†ï¼Œé¿å… StopIteration
        from itertools import cycle
        
        for i, batch in enumerate(cycle(loader)):
            if i >= eval_steps:
                break
            
            batch = batch.to(device)
            if batch.shape[1] <= 1: 
                continue
            
            x, y = batch[:, :-1], batch[:, 1:]
            
            # Eval æ—¶ä½¿ç”¨ Normal æ¨¡å¼ï¼Œæµ‹è¯•å…¨ç³»ç»Ÿ
            with torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=train_use_amp):
                logits, info = model(x, step=100000, phase="normal")
                # åªç®— Main Loss
                loss = torch.nn.functional.cross_entropy(
                    logits.reshape(-1, model.vocab_size),
                    y.reshape(-1),
                    ignore_index=-100,
                )
            
            losses.append(loss.item())
            
            # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæŸå¤±ä¸º NaN æˆ– Infï¼Œç«‹å³æŠ¥å‘Š
            if not torch.isfinite(torch.tensor(loss.item())):
                print(f"âš ï¸ Invalid loss detected at eval step {i}: {loss.item()}")
                continue
        
        model.train()
        if len(losses) == 0:
            print("âš ï¸ Warning: No valid losses collected during evaluation!")
            return float('inf')
        return sum(losses) / len(losses)

    print(f"ğŸ“Š Model params: {sum(p.numel() for p in model.parameters())/1e6:.1f}M")
    
    # ==========================================
    # SwanLab åˆå§‹åŒ– (å¸¦å›¾è¡¨ç»­æ¥åŠŸèƒ½)
    # ==========================================
    current_run_id = None
    run_id = None
    
    # 1. å¦‚æœæ˜¯ Resumeï¼Œå°è¯•ä» checkpoint æ‰¾ run_id
    if args.resume and isinstance(checkpoint, dict) and 'swanlab_run_id' in checkpoint:
        run_id = checkpoint['swanlab_run_id']
        print(f"ğŸ”„ Resuming SwanLab run: {run_id}")
    
    # 2. åˆå§‹åŒ– SwanLab
    if HAS_SWANLAB:
        experiment = swanlab.init(
            project=config['project'],
            name=config['run_name'],
            config=config,
            id=run_id,
            resume="allow"
        )
        # è·å–å½“å‰çš„ run_id (å¦‚æœæ˜¯æ–°çš„ï¼Œè¿™é‡Œä¼šç”Ÿæˆæ–°çš„)
        current_run_id = experiment.public.run_id
    
    os.makedirs(config['save_dir'], exist_ok=True)
    
    print(f"ğŸš€ Training start from step {start_step}...")
    optimizer.zero_grad(set_to_none=True)
    
    # ==========================================
    # Logging é€»è¾‘ (å›æ»šåˆ°ç¬æ—¶å€¼ + ä¿®å¤Stepæ˜¾ç¤º)
    # ==========================================
    log_interval = config.get('log_interval', 10)
    last_phase_name = None
    active_profile = current_profile
    
    # 5. Training Loop
    accum_market_infos: List[Dict[str, List[torch.Tensor]]] = []
    accum_token_losses: List[torch.Tensor] = []
    accum_critic_losses: List[float] = []
    accum_micro_steps = 0

    for step in range(start_step, config['total_steps']):
        t0 = time.time()
        
        phase = get_phase(step, phase_plan)
        phase_name = phase.get("name", "normal")
        phase_use_market = bool(phase.get("use_market", True))
        if phase_name != last_phase_name:
            apply_phase_policy(optimizer, phase, config, group_map)
            apply_route_grad_policy(model, phase, config)
            last_phase_name = phase_name
            print(
                f"ğŸ” Phase switched -> {phase_name} [{phase.get('start_step', step)}:{phase.get('end_step', step)}] "
                f"| use_market={phase_use_market} | route_grad={phase.get('route_grad', True)}"
            )

            phase_profile = phase.get("data_profile", "default")
            if phase_profile != active_profile:
                print(f"ğŸ§­ Rebuilding dataloaders for profile={phase_profile}")
                train_loader, val_loader = build_loader_for_profile(phase_profile, config, simple_collate)
                train_iter = infinite_loader(train_loader)
                active_profile = phase_profile
        
        try:
            x_batch = next(train_iter)
        except StopIteration:
            train_iter = infinite_loader(train_loader)
            x_batch = next(train_iter)
            
        x_batch = x_batch.to(device)
        if x_batch.shape[1] <= 1: continue
            
        x, y = x_batch[:, :-1], x_batch[:, 1:]
        
        try:
            with torch.amp.autocast(device_type='cuda', dtype=amp_dtype, enabled=train_use_amp):
                logits, info = model(
                    x,
                    step=step,
                    phase=phase_name,
                    use_market_override=phase_use_market,
                )
                total_loss, token_losses, main_loss, critic_loss, aux_loss = model.compute_losses(logits, y, info)
                loss_to_backward = total_loss / config['grad_accum']
        except RuntimeError as e:
            print(f"ğŸ’¥ Forward/LOSS failed at step={step}, phase={phase_name}: {e}")
            raise

        if not torch.isfinite(loss_to_backward):
            print(f"âš ï¸ Non-finite loss at step {step}: {float(loss_to_backward)} (skip batch)")
            optimizer.zero_grad(set_to_none=True)
            accum_market_infos.clear()
            accum_token_losses.clear()
            accum_critic_losses.clear()
            accum_micro_steps = 0
            continue

        if not loss_to_backward.requires_grad:
            critic_req = critic_loss.requires_grad if isinstance(critic_loss, torch.Tensor) else False
            print(
                f"âš ï¸ No grad graph at step={step}, phase={phase_name} "
                f"(total_loss.requires_grad={total_loss.requires_grad}, critic_loss.requires_grad={critic_req})."
            )
            optimizer.zero_grad(set_to_none=True)
            accum_market_infos.clear()
            accum_token_losses.clear()
            accum_critic_losses.clear()
            accum_micro_steps = 0
            continue

        loss_to_backward.backward()
        accum_market_infos.append(_detach_market_info(info))
        accum_token_losses.append(token_losses.detach())
        if isinstance(critic_loss, torch.Tensor):
            accum_critic_losses.append(float(critic_loss.detach().item()))
        else:
            accum_critic_losses.append(float(critic_loss))
        accum_micro_steps += 1
        
        if (step + 1) % config['grad_accum'] == 0:
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)
            
            if phase.get("market_update", True) and step > 100 and accum_micro_steps > 0:
                merged_info = _merge_market_infos(accum_market_infos)
                merged_token_losses = torch.cat(accum_token_losses, dim=0)
                merged_critic_loss = (
                    sum(accum_critic_losses) / max(1, len(accum_critic_losses))
                    if accum_critic_losses else None
                )
                model.update_market(
                    merged_info,
                    merged_token_losses,
                    step,
                    phase=phase_name,
                    critic_loss=merged_critic_loss,
                )

            accum_market_infos.clear()
            accum_token_losses.clear()
            accum_critic_losses.clear()
            accum_micro_steps = 0
        
        # [ä¿®æ”¹] æ—¥å¿—ä¸è¯„ä¼°é€»è¾‘
        if step % log_interval == 0:
            dt = time.time() - t0
            tps = config['micro_batch_size'] * x.shape[1] / dt
            
            # --- è¯„ä¼° ---
            val_loss = None
            if step > 0 and step % eval_interval == 0:
                print(f"ğŸ” Evaluating at step {step}...")
                val_loss = estimate_loss(model, val_loader, eval_iters)
            
            # ç»Ÿè®¡
            stats = model.log_market_health()
            
            # æ‰“å° (ç¬æ—¶ Loss)
            log_str = f"Step {step} | Loss: {main_loss.item():.3f}"
            if val_loss:
                log_str += f" | ValLoss: {val_loss:.3f}"
            log_str += f" | TPS: {tps:.0f} | [{phase_name.upper()}]"
            print(log_str)
            
            # SwanLab Log (å…³é”®ä¿®æ­£ï¼šä¼ å…¥ step å‚æ•°)
            if HAS_SWANLAB:
                logs = {
                    "Loss/Train_Main": main_loss.item(),
                    "Loss/Train_Critic": critic_loss.item() if isinstance(critic_loss, torch.Tensor) else critic_loss,
                    "Loss/Aux_Balance": aux_loss.item() if isinstance(aux_loss, torch.Tensor) else aux_loss,
                    "Speed/TPS": tps,
                    "Phase/ID": float(phase_to_id.get(phase_name, -1)),
                    f"Phase/{phase_name}": 1.0,
                    **stats
                }
                if val_loss:
                    logs["Loss/Validation"] = val_loss
                
                # [å…³é”®] æ˜¾å¼æŒ‡å®š stepï¼Œè¿™æ · step 1000 å°±ä¼šç”»åœ¨ X=1000 å¤„
                swanlab.log(logs, step=step)
        
        # ä¿å­˜å®Œæ•´ Checkpoint (é¡ºä¾¿ä¿å­˜ run_id)
        if step > 0 and step % 2000 == 0:
            gc.collect()
            torch.cuda.empty_cache()
            print("ğŸ§¹ Cache cleared")
            path = os.path.join(config['save_dir'], f"{config['version']}_step{step}.pth")
        
            checkpoint_data = {
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'step': step,
                'config': config,
                'swanlab_run_id': current_run_id,
                'version': config['version']  # é¢å¤–è®°å½•ç‰ˆæœ¬
            }
            torch.save(checkpoint_data, path)
            print(f"ğŸ’¾ Saved: {path}")
    
    final_path = os.path.join(config['save_dir'], f"{config['version']}_final.pth")
    torch.save(
        {
            'model': model.state_dict(),
            'step': config['total_steps'],
            'config': config,
            'swanlab_run_id': current_run_id,
            'version': config['version'],
        },
        final_path
    )
    print("ğŸ‰ Done!")

if __name__ == "__main__":
    main()
