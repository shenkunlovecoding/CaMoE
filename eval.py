"""
CaMoE v12.0 å¯è§†åŒ–æ·±åº¦è¯„æµ‹è„šæœ¬ (Sherlock Edition)
é€‚é…: 2 RWKV + 2 Trans æ¶æ„
åŠŸèƒ½ï¼š
1. ç”Ÿæˆå¸¦é¢œè‰²é«˜äº®çš„æ•…äº‹ (äººç±»çœ‹)
2. ç”Ÿæˆ Token çº§çš„è¯¦ç»†å±‚çº§è·¯ç”±æ—¥å¿— (AI åˆ†æç”¨)
3. è‡ªåŠ¨ç»Ÿè®¡ Transformer çš„â€œå£å‘³åå¥½â€
"""

import torch
import torch.nn.functional as F
import os
import json
from termcolor import colored
from collections import Counter
from CaMoE.system import CaMoE_System
from CaMoE.config import *
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER

# ================= é…ç½® =================
# [è¯·ç¡®è®¤] æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
MODEL_PATH = "checkpoints/minipile/v16_step12000.pth" 
# æˆ–è€…ç”¨æœ€æ–°çš„ step: "checkpoints/v12/v12_step10000.pth"

SCALE = "0.1b"
DEVICE = "cuda"
ctx_len = 512
CHUNK_LEN = 16  

# ================= åŠ è½½é€»è¾‘ =================
config = CONFIG_MINIPILE if SCALE == "0.1b" else CONFIG_04B

# [é‡è¦] å¿…é¡»åŒ¹é… v12 è®­ç»ƒé…ç½®ï¼
config['num_rwkv_experts'] = 3
config['num_trans_experts'] = 1
config['micro_batch_size'] = 1 # æ¨ç†æ—¶ BS=1

print(f"ğŸ”„ Loading model from {MODEL_PATH}...")
print(f"âš™ï¸ Config: {config['num_rwkv_experts']} RWKV + {config['num_trans_experts']} Trans Experts")

model = CaMoE_System(config).to(DEVICE)

# å°è¯•åŠ è½½
if os.path.exists(MODEL_PATH):
    checkpoint = torch.load(MODEL_PATH, map_location='cpu')
    state_dict = checkpoint['model'] if isinstance(checkpoint, dict) and 'model' in checkpoint else checkpoint

    try:
        model.load_state_dict(state_dict, strict=True)
        print("âœ… Full strict load success.")
    except Exception as e:
        print(f"âš ï¸ Strict load failed, trying non-strict... ({str(e)[:100]}...)")
        model.load_state_dict(state_dict, strict=False)
        print("âœ… Non-strict load success.")
else:
    print(f"âŒ Checkpoint not found: {MODEL_PATH}")
    exit()

model.eval()
# å¦‚æœæ²¡æœ‰ Tokenizer æ–‡ä»¶ï¼Œä¼šæŠ¥é”™ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨
if os.path.exists(config['vocab_file']):
    tokenizer = TRIE_TOKENIZER(config['vocab_file'])
else:
    print("âŒ Tokenizer vocab file not found.")
    exit()

# ================= è¾…åŠ©å‡½æ•° =================
def sample_top_p(probs, p, temperature):
    if temperature == 0:
        return torch.argmax(probs, dim=-1).unsqueeze(0)
    
    probs = probs.pow(1.0/temperature)
    probs = probs / probs.sum()
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    probs = probs.masked_fill(indices_to_remove, 0.0)
    probs = probs / probs.sum()
    
    return torch.multinomial(probs, 1)

def analyze_token_preferences(history_log):
    """
    åˆ†æ Transformer åˆ°åº•å–œæ¬¢åƒä»€ä¹ˆè¯
    """
    trans_heavy_tokens = []
    trans_light_tokens = []
    
    for item in history_log:
        token = item['token'].strip()
        if not token: continue
        
        # ç»Ÿè®¡ç”¨äº† Trans çš„å±‚æ•°
        trans_layers = len(item['trans_layers'])
        if trans_layers >= 2: # åªè¦æœ‰2å±‚ä»¥ä¸Šç”¨äº† Trans
            trans_heavy_tokens.append(token)
        else:
            trans_light_tokens.append(token)
            
    heavy_counts = Counter(trans_heavy_tokens).most_common(10)
    
    print("\nğŸ§ [AI Analysis] Transformer's Favorite Tokens (Top 10):")
    print(f"è¿™äº›è¯æœ€å®¹æ˜“è§¦å‘ Trans: {heavy_counts}")

def generate_and_visualize(prompt, max_new_tokens=200, temperature=0.85, top_p=0.9):
    input_ids = tokenizer.encode(prompt)
    x = torch.tensor([input_ids], dtype=torch.long).to(DEVICE)
    
    print("\n" + "="*20 + " GENERATION START " + "="*20)
    print(f"Prompt: {prompt}\n")
    print("-" * 50)
    print(prompt, end="", flush=True)
    
    # ç»Ÿè®¡æ•°æ®
    total_generated = 0
    global_trans_count = 0
    layer_trans_counts = {i: 0 for i in range(config['n_layer'])}
    
    # AI åˆ†ææ—¥å¿—åˆ—è¡¨
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            # Padding
            curr_ctx = x[:, -config['ctx_len']:]
            B, T_actual = curr_ctx.shape
            remainder = T_actual % CHUNK_LEN
            if remainder != 0:
                pad_len = CHUNK_LEN - remainder
                x_padded = F.pad(curr_ctx, (0, pad_len), value=0)
            else:
                x_padded = curr_ctx
            
            # Forward
            # step=30000 ç¡®ä¿ Eureka å…³é—­ï¼Œå®Œå…¨çœ‹ Router
            # phase="normal" å¼€å¯ Market
            logits, info = model(x_padded, step=30000, phase="normal") 
            
            # Sampling
            target_idx = T_actual - 1
            next_token_logits = logits[:, target_idx, :]
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = sample_top_p(probs, top_p, temperature)
            
            # è·¯ç”±ç»Ÿè®¡
            active_layers = []
            rwkv_boundary = config['num_rwkv_experts']
            
            for layer_idx, layer_winners in enumerate(info["winners"]):
                # layer_winners: [B, T]
                # å–ç”Ÿæˆä½ç½®çš„èƒœè€…
                winner_id = layer_winners[0, target_idx].item()
                
                # [v12 é€‚é…] ID >= num_rwkv_experts çš„éƒ½æ˜¯ Trans
                if winner_id >= rwkv_boundary:
                    layer_trans_counts[layer_idx] += 1
                    active_layers.append(layer_idx)
            
            trans_layer_count = len(active_layers)
            
            # [å‡çº§ç‰ˆé¢œè‰²é€»è¾‘]
            if trans_layer_count == 0:
                color = 'blue'       # çº¯ç›´è§‰æµ
            elif trans_layer_count <= 3:
                color = 'cyan'
                global_trans_count += 0.3       # è½»é‡çº§æ··åˆ
            elif trans_layer_count <= 5:
                color = 'green'
                global_trans_count += 0.5      # v13 æ ‡å‡†ä¸‰æ˜æ²» (æ”¯æŸ±å±‚ä»‹å…¥)
            elif trans_layer_count <= 8:
                color = 'yellow'
                global_trans_count += 0.8     # é€»è¾‘å¼ºåŒ– (ä¸­é—´å±‚ä¹Ÿä»‹å…¥äº†)
            else:
                color = 'red'        # é«˜å¼ºåº¦æ¨ç† (å…¨çº¿é‡å…µå‹å¢ƒ)
                global_trans_count += 1
            
            total_generated += 1
            
            try:
                word = tokenizer.decode([next_token.item()])
            except:
                word = ""
                
            print(colored(word, color), end="", flush=True)
            
            # è®°å½•åˆ°æ—¥å¿—
            
            x = torch.cat([x, next_token.view(1, 1)], dim=1)
            if next_token.item() == 0: break
    
    print("\n" + "-" * 50)
    
    if total_generated > 0:
        print(f"\nğŸ“Š Global Stats: {total_generated} tokens")
        print(f"ğŸ”µ RWKV Token: {total_generated - global_trans_count}")
        print(f"ğŸ”´ Trans Token: {global_trans_count} ({global_trans_count/total_generated:.1%})")
        
        print("\nğŸ” Layer-wise Transformer Usage:")
        for i in range(config['n_layer']):
            pct = layer_trans_counts[i] / total_generated
            bar_len = int(pct * 20)
            bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
            print(f" L{i:02d} | {pct:.1%} | {bar}")

        
        # å¯é€‰ï¼šæ‰“å°è¯¦ç»†æ—¥å¿—
        # print(json.dumps(analysis_log, indent=2, ensure_ascii=False))

# ================= æµ‹è¯• =================
prompts = [
    
    # ===== 3. ç®€å•å¯¹è¯ (Switchboard é£æ ¼) =====
    "The three main steps to cook rice are: 1. Wash the rice; 2.",
    "The capital of France is Paris, but the capital of Japan is",
    "If x = 5 and y = 3, then x + y equals",
    "The three main steps to cook rice are: 1. Wash the rice; 2.",
    "Although the weather was very cold and the wind was blowing hard, the small bird decided to",
    "Based on the above mentioned analysis, we can conclude that"
]

for p in prompts:
    generate_and_visualize(p)