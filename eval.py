"""
CaMoE å¯è§†åŒ–è¯„æµ‹è„šæœ¬ (å¢žå¼ºç‰ˆ)
åŠŸèƒ½ï¼š
1. ç”Ÿæˆé«˜äº®æ•…äº‹
2. ä¿®å¤ CUDA Padding é—®é¢˜
3. [æ–°å¢ž] æ¯ä¸€å±‚çš„è¯¦ç»†æ¿€æ´»çŽ‡ç»Ÿè®¡
"""

import torch
import torch.nn.functional as F
import os
from termcolor import colored
from camoe import CaMoE_System
from config import CONFIG_01B, CONFIG_04B
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER

# ================= é…ç½® =================
MODEL_PATH = "checkpoints/v10_final.pth"  
SCALE = "0.1b"
DEVICE = "cuda"
ctx_len = 512
CHUNK_LEN = 16  # RWKV-7 Kernel çº¦æŸ

# ================= åŠ è½½ =================
config = CONFIG_01B if SCALE == "0.1b" else CONFIG_04B
config['num_rwkv_experts'] = 3
config['micro_batch_size'] = 1

print(f"ðŸ”„ Loading model from {MODEL_PATH}...")
model = CaMoE_System(config).to(DEVICE)

checkpoint = torch.load(MODEL_PATH, map_location='cpu')
if 'model' in checkpoint:
    model.load_state_dict(checkpoint['model'])
else:
    model.load_state_dict(checkpoint)
model.eval()

tokenizer = TRIE_TOKENIZER(config['vocab_file'])

# ================= è¾…åŠ©å‡½æ•° =================
def sample_top_p(probs, p, temperature):
    probs = probs.pow(1.0/temperature)
    probs = probs / probs.sum()
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    probs = probs.masked_fill(indices_to_remove, 0.0)
    probs = probs / probs.sum()
    
    return torch.multinomial(probs, 1)

def generate_and_visualize(prompt, max_new_tokens=200, temperature=1.0, top_p=0.85):
    input_ids = tokenizer.encode(prompt)
    x = torch.tensor([input_ids], dtype=torch.long).to(DEVICE)
    
    print("\n" + "="*20 + " GENERATION START " + "="*20)
    print(f"Prompt: {prompt}\n")
    print("-" * 50)
    print(prompt, end="", flush=True)
    
    # ç»Ÿè®¡æ•°æ®åˆå§‹åŒ–
    total_generated = 0
    global_trans_count = 0
    global_rwkv_count = 0
    
    # [æ–°å¢ž] æ¯å±‚çš„ Transformer è®¡æ•°
    layer_trans_counts = {i: 0 for i in range(config['n_layer'])}
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            # 1. Padding å¤„ç†
            curr_ctx = x[:, -config['ctx_len']:]
            B, T_actual = curr_ctx.shape
            
            remainder = T_actual % CHUNK_LEN
            if remainder != 0:
                pad_len = CHUNK_LEN - remainder
                x_padded = F.pad(curr_ctx, (0, pad_len), value=0)
            else:
                x_padded = curr_ctx
            
            # 2. Forward
            # è¿™é‡Œ step ä¼  3000 (çœŸå®žæ­¥æ•°) è¿˜æ˜¯ 30000 å¯èƒ½ä¼šå½±å“ Capital é€»è¾‘
            # ä½†åœ¨ eval æ¨¡å¼ä¸‹ Capital é€šå¸¸æ˜¯å†»ç»“çš„ï¼Œä¸»è¦çœ‹ Router è¡Œä¸º
            logits, info = model(x_padded, step=30000, phase="normal") 
            
            # 3. èŽ·å–æ•°æ®
            target_idx = T_actual - 1
            next_token_logits = logits[:, target_idx, :]
            
            # é‡‡æ ·
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = sample_top_p(probs, top_p, temperature)
            
            # 4. [æ–°å¢ž] ç»Ÿè®¡æ¯ä¸€å±‚çš„ Winner
            transformer_id = config['num_rwkv_experts']
            token_uses_transformer = False # åªè¦æœ‰ä¸€å±‚ç”¨äº†å°±ç®—è¿™ä¸ªTokenæ˜¯çº¢çš„
            
            # info["winners"] æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ä¸º n_layer
            # æ¯ä¸ªå…ƒç´ æ˜¯ [B, T] çš„ Tensor
            for layer_idx, layer_winners in enumerate(info["winners"]):
                winner_id = layer_winners[0, target_idx].item()
                
                if winner_id == transformer_id:
                    layer_trans_counts[layer_idx] += 1
                    token_uses_transformer = True
            
            # å…¨å±€ç»Ÿè®¡
            if token_uses_transformer:
                global_trans_count += 1
                color = 'red'
            else:
                global_rwkv_count += 1
                color = 'cyan'
            
            total_generated += 1
            
            # æ‰“å°
            try:
                word = tokenizer.decode([next_token.item()])
            except:
                word = ""
            print(colored(word, color), end="", flush=True)
            
            x = torch.cat([x, next_token.view(1, 1)], dim=1)
            if next_token.item() == 0: break
    
    print("\n" + "-" * 50)
    
    if total_generated > 0:
        print(f"\nðŸ“Š Global Stats: {total_generated} tokens")
        print(f"ðŸ”µ RWKV Token: {global_rwkv_count} ({global_rwkv_count/total_generated:.1%})")
        print(f"ðŸ”´ Trans Token: {global_trans_count} ({global_trans_count/total_generated:.1%})")
        print(f"   (æ³¨: Token åªè¦æœ‰ä¸€å±‚ç”¨äº† Trans å°±ç®—çº¢è‰²)")
        
        print("\nðŸ” Layer-wise Transformer Usage:")
        print("Layer | Usage % | Visualization")
        print("-" * 40)
        for i in range(config['n_layer']):
            count = layer_trans_counts[i]
            pct = count / total_generated
            
            # ç®€å•çš„è¿›åº¦æ¡å¯è§†åŒ–
            bar_len = int(pct * 20)
            bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
            
            # é«˜äº®æ˜¾ç¤ºé«˜é¢‘å±‚
            pct_str = f"{pct:.1%}"
            if pct > 0.5:
                pct_str = colored(pct_str, 'red')
            elif pct > 0.2:
                pct_str = colored(pct_str, 'yellow')
            else:
                pct_str = colored(pct_str, 'green')
                
            print(f" L{i:02d} | {pct_str:>7} | {bar}")

# ================= æµ‹è¯•ç”¨ä¾‹ =================
prompts = [
    "Once upon a time, there was a little girl named Lily.",
    "The king was very sad because he lost his crown.",
]

for p in prompts:
    generate_and_visualize(p)