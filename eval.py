"""
CaMoE v18 å¯è§†åŒ–æ·±åº¦è¯„æµ‹è„šæœ¬ (Sherlock Edition)
é€‚é…: 6 RWKV + 2 Trans (Top-2) æ¶æ„
åŠŸèƒ½ï¼š
1. ç”Ÿæˆå¸¦é¢œè‰²é«˜äº®çš„æ•…äº‹ (äººç±»çœ‹)
2. è‡ªåŠ¨è¯†åˆ« Top-2 è·¯ç”±çŠ¶æ€ (åŒR / æ··åŠ¨ / åŒT)
3. ç»Ÿè®¡å±‚çº§ Transformer æ¸—é€ç‡
"""

import torch
import torch.nn.functional as F
import os
import sys
from termcolor import colored
from collections import Counter

# ç¡®ä¿èƒ½å¯¼å…¥ CaMoE æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from CaMoE.system import CaMoE_System
from CaMoE.config import get_config # ä½¿ç”¨ config.py çš„ getter

# å°è¯•å¯¼å…¥ Rust Tokenizerï¼Œæ²¡æœ‰å°±ç”¨ Python ç‰ˆ
try:
    import pyrwkv_tokenizer
    RUST_TOKENIZER = True
except ImportError:
    from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
    RUST_TOKENIZER = False

# ================= é…ç½® =================
# [è¯·ç¡®è®¤] æ¨¡å‹è·¯å¾„
MODEL_PATH = "checkpoints/v18_0.4b/v18_step10000.pth" # ä½ çš„ Pilot è·¯å¾„
SCALE = "0.4b"  # "0.4b" or "pilot" or "0.1b"
DEVICE = "cuda"

# ================= åŠ è½½é€»è¾‘ =================
config = get_config(SCALE).copy()

# å¼ºåˆ¶æ¨ç†é…ç½®
config['micro_batch_size'] = 1 
config['ctx_len'] = 1024 # æ¨ç†é•¿åº¦
config['dropout'] = 0.0

print(f"ğŸ”„ Loading model from {MODEL_PATH}...")
print(f"âš™ï¸ Config: {config['num_rwkv_experts']}R + {config['num_trans_experts']}T (Top-{config.get('top_k', 2)})")

model = CaMoE_System(config).to(DEVICE)

# åŠ è½½æƒé‡
if os.path.exists(MODEL_PATH):
    checkpoint = torch.load(MODEL_PATH, map_location='cpu')
    # å…¼å®¹ä¸åŒçš„ä¿å­˜æ ¼å¼
    if isinstance(checkpoint, dict):
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
            
    try:
        model.load_state_dict(state_dict, strict=True)
        print("âœ… Full strict load success.")
    except Exception as e:
        print(f"âš ï¸ Strict load failed, trying non-strict... ({str(e)[:100]}...)")
        model.load_state_dict(state_dict, strict=False)
        print("âœ… Non-strict load success.")
else:
    print(f"âŒ Checkpoint not found: {MODEL_PATH}")
    exit()

model.eval()

# åŠ è½½ Tokenizer
print("ğŸ”¤ Loading Tokenizer...")
if RUST_TOKENIZER:
    # Rust ç‰ˆä¸éœ€è¦ vocab æ–‡ä»¶è·¯å¾„ï¼Œå†…ç½®
    tokenizer = pyrwkv_tokenizer.RWKVTokenizer()
    print("âœ… Rust Tokenizer loaded.")
elif os.path.exists(config['vocab_file']):
    tokenizer = TRIE_TOKENIZER(config['vocab_file'])
    print("âœ… Python Trie Tokenizer loaded.")
else:
    print("âŒ Tokenizer vocab file not found.")
    exit()

# ================= è¾…åŠ©å‡½æ•° =================
def sample_top_p(probs: torch.Tensor, p: float, temperature: float) -> torch.Tensor:
    r"""sample_top_p(probs, p, temperature) -> Tensor

    å¯¹æ¦‚ç‡åˆ†å¸ƒæ‰§è¡Œ top-p é‡‡æ ·ã€‚

    Args:
      probs (Tensor): å½¢çŠ¶ ``[B, V]`` çš„æ¦‚ç‡åˆ†å¸ƒã€‚
      p (float): nucleus æˆªæ–­é˜ˆå€¼ã€‚
      temperature (float): é‡‡æ ·æ¸©åº¦ï¼›``0`` è¡¨ç¤ºè´ªå¿ƒã€‚

    Returns:
      Tensor: å½¢çŠ¶ ``[B, 1]`` çš„é‡‡æ · token idã€‚
    """
    if temperature == 0:
        return torch.argmax(probs, dim=-1).unsqueeze(0)
    
    probs = probs.pow(1.0/temperature)
    probs = probs / probs.sum()
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    probs = probs.masked_fill(indices_to_remove, 0.0)
    probs = probs / probs.sum()
    
    return torch.multinomial(probs, 1)


def apply_repetition_penalty(
    logits: torch.Tensor,
    context_ids: torch.Tensor,
    penalty: float = 1.2,
) -> torch.Tensor:
    r"""apply_repetition_penalty(logits, context_ids, penalty=1.2) -> Tensor

    å¯¹ä¸Šä¸‹æ–‡å‡ºç°è¿‡çš„ token æ–½åŠ é‡å¤æƒ©ç½šã€‚

    Args:
      logits (Tensor): å½¢çŠ¶ ``[B, V]``ã€‚
      context_ids (Tensor): å½¢çŠ¶ ``[B, Seq]``ã€‚
      penalty (float, optional): é‡å¤æƒ©ç½šç³»æ•°ã€‚Default: ``1.2``ã€‚

    Returns:
      Tensor: åº”ç”¨æƒ©ç½šåçš„ logitsã€‚
    """
    if penalty == 1.0:
        return logits
    score = torch.gather(logits, 1, context_ids)
    score = torch.where(score < 0, score * penalty, score / penalty)
    logits.scatter_(1, context_ids, score)
    return logits


def format_prompt(user_input: str) -> str:
    r"""format_prompt(user_input) -> str

    å°†ç”¨æˆ·è¾“å…¥åŒ…è£…æˆç®€å•å¯¹è¯æ¨¡æ¿ã€‚

    Args:
      user_input (str): ç”¨æˆ·æ–‡æœ¬ã€‚

    Returns:
      str: æ‹¼æ¥åçš„ promptã€‚
    """
    return f"User: {user_input}\nAssistant:"


def generate_and_visualize(
    prompt: str,
    max_new_tokens: int = 200,
    temperature: float = 1.0,
    top_p: float = 0.9,
    repetition_penalty: float = 1.2,
) -> None:
    r"""generate_and_visualize(prompt, max_new_tokens=200, temperature=1.0, top_p=0.9, repetition_penalty=1.2) -> None

    ç”Ÿæˆæ–‡æœ¬å¹¶è¾“å‡ºæŒ‰è·¯ç”±å¼ºåº¦ç€è‰²çš„å¯è§†åŒ–ç»“æœã€‚

    Args:
      prompt (str): è¾“å…¥æç¤ºè¯ã€‚
      max_new_tokens (int, optional): æœ€å¤§ç”Ÿæˆé•¿åº¦ã€‚Default: ``200``ã€‚
      temperature (float, optional): é‡‡æ ·æ¸©åº¦ã€‚Default: ``1.0``ã€‚
      top_p (float, optional): nucleus é˜ˆå€¼ã€‚Default: ``0.9``ã€‚
      repetition_penalty (float, optional): é‡å¤æƒ©ç½šã€‚Default: ``1.2``ã€‚
    """
    # Tokenize
    if RUST_TOKENIZER:
        input_ids = tokenizer.encode(prompt)
    else:
        input_ids = tokenizer.encode(prompt)
        
    x = torch.tensor([input_ids], dtype=torch.long).to(DEVICE)
    
    print("\n" + "="*20 + " GENERATION START " + "="*20)
    print(f"Prompt: {prompt}\n")
    print("-" * 50)
    print(prompt, end="", flush=True)
    
    # ç»Ÿè®¡æ•°æ®
    total_generated = 0
    
    # ç»Ÿè®¡æ¯ä¸€å±‚ Transformer çš„æ¿€æ´»æ¬¡æ•° (Top-2 ä¸­ä»»ä¸€æ¿€æ´»ç®—ä¸€æ¬¡)
    layer_trans_counts = {i: 0 for i in range(config['n_layer'])}
    
    # ç»Ÿè®¡å…¨å±€çŠ¶æ€ï¼š
    # 0: Pure RWKV (Blue)
    # 1: Mixed (Yellow)
    # 2: Pure Trans (Red)
    state_counts = {0: 0, 1: 0, 2: 0}
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            # v18 ä¸å†å¼ºåˆ¶ paddingï¼Œé™¤éä½¿ç”¨ CUDA Kernel ä¼˜åŒ–
            # ç®€å•èµ·è§ï¼Œè¿™é‡Œç›´æ¥è¾“å…¥
            curr_x = x[:, -config['ctx_len']:]
            
            # Forward
            # step=30000 ç¡®ä¿ Eureka å…³é—­ï¼Œå®Œå…¨çœ‹ Router
            # phase="normal" å¼€å¯ Market
            # å¼€å¯ AMP ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„ç²¾åº¦ (BF16)
            with torch.amp.autocast(device_type=DEVICE, dtype=torch.bfloat16):
                logits, info = model(curr_x, step=30000, phase="normal") 
            
            # Sampling
            target_idx = curr_x.shape[1] - 1
            next_token_logits = logits[:, target_idx, :].clone()
            # é‡å¤æƒ©ç½šï¼šå¯¹å·²å‡ºç°åœ¨ x ä¸­çš„ token é™æƒ
            apply_repetition_penalty(next_token_logits, x, penalty=repetition_penalty)
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = sample_top_p(probs, top_p, temperature)
            
            # === Top-2 è·¯ç”±åˆ†æ ===
            rwkv_boundary = config['num_rwkv_experts'] # e.g. 6
            
            # å½“å‰ Token åœ¨æ‰€æœ‰å±‚çš„ Transformer æ¿€æ´»æ•°
            token_trans_intensity = 0 
            
            for layer_idx, layer_winners in enumerate(info["winners"]):
                # layer_winners: [B, T, 2] -> å–å½“å‰ä½ç½® [2]
                winners = layer_winners[0, target_idx] # tensor([idx1, idx2])
                
                # æ£€æŸ¥ Top-2 ä¸­æœ‰å‡ ä¸ªæ˜¯ Transformer
                # ID >= rwkv_boundary (6) çš„æ˜¯ Trans
                is_trans = (winners >= rwkv_boundary).long().sum().item()
                
                if is_trans > 0:
                    layer_trans_counts[layer_idx] += 1
                    token_trans_intensity += is_trans # è¿™ä¸€å±‚è´¡çŒ®äº† 1 æˆ– 2 ä¸ª Trans å¼ºåº¦
            
            # === é¢œè‰²é€»è¾‘ ===
            # æ€»å…±æœ‰ 16 å±‚ï¼Œæ¯å±‚æœ€å¤š 2 ä¸ª Transï¼Œæ»¡åˆ† 32 åˆ†
            # æˆ‘ä»¬æ ¹æ®å¼ºåº¦å®šè‰²
            
            if token_trans_intensity == 0:
                color = 'blue'       # çº¯ç›´è§‰ (å…¨ RWKV)
                state_counts[0] += 1
            elif token_trans_intensity <= 5:
                color = 'cyan'       # è½»å¾®æ€è€ƒ
                state_counts[1] += 1
            elif token_trans_intensity <= 12:
                color = 'yellow'     # æ··åˆæ¨¡å¼
                state_counts[1] += 1
            else:
                color = 'red'        # æ·±åº¦æ€è€ƒ (å¤§é‡ Transformer ä»‹å…¥)
                state_counts[2] += 1
            
            total_generated += 1
            
            # Decode
            try:
                word = tokenizer.decode([next_token.item()])
            except:
                word = ""
                
            print(colored(word, color), end="", flush=True)
            
            x = torch.cat([x, next_token.view(1, 1)], dim=1)
            if next_token.item() == 0: break # EOS
    
    print("\n" + "-" * 50)
    
    if total_generated > 0:
        print(f"\nğŸ“Š Global Stats: {total_generated} tokens")
        print(f"ğŸ”µ Pure RWKV: {state_counts[0]} ({state_counts[0]/total_generated:.1%})")
        print(f"ğŸŸ¡ Mixed:      {state_counts[1]} ({state_counts[1]/total_generated:.1%})")
        print(f"ğŸ”´ Deep Trans: {state_counts[2]} ({state_counts[2]/total_generated:.1%})")
        
        print("\nğŸ” Layer-wise Transformer Usage (Top-2 Hit Rate):")
        for i in range(config['n_layer']):
            # è¿™ä¸€å±‚åœ¨ Top-2 ä¸­å‘½ä¸­ Trans çš„æ¦‚ç‡
            pct = layer_trans_counts[i] / total_generated
            bar_len = int(pct * 20)
            bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
            print(f" L{i:02d} | {pct:.1%} | {bar}")

# ================= æµ‹è¯• =================
prompts = [
    "Once upon a time, there was a little girl named Lily.",  # Story æ¨¡å¼ä¸ç”¨åŒ…
    format_prompt("The capital of France is Paris, but the capital of Japan is"),
    format_prompt("If x = 5 and y = 3, then x + y equals"),
]

if __name__ == "__main__":
    for p in prompts:
        generate_and_visualize(p,temperature=1.0,top_p=0.5,repetition_penalty=2)
