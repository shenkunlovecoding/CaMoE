"""
CaMoE å¯è§†åŒ–æ·±åº¦è¯„æµ‹è„šæœ¬ (Sherlock Edition)
åŠŸèƒ½ï¼š
1. ç”Ÿæˆå¸¦é¢œè‰²é«˜äº®çš„æ•…äº‹ (äººç±»çœ‹)
2. ç”Ÿæˆ Token çº§çš„è¯¦ç»†å±‚çº§è·¯ç”±æ—¥å¿— (AI åˆ†æžç”¨)
3. è‡ªåŠ¨ç»Ÿè®¡ Transformer çš„â€œå£å‘³åå¥½â€
"""

import torch
import torch.nn.functional as F
import os
import json
from termcolor import colored
from collections import Counter
from camoe import CaMoE_System
from config import CONFIG_01B, CONFIG_04B
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER

# ================= é…ç½® =================
MODEL_PATH = "checkpoints/v10_step10000.pth"  
SCALE = "0.1b"
DEVICE = "cuda"
ctx_len = 512
CHUNK_LEN = 16  

# ================= åŠ è½½é€»è¾‘ =================
config = CONFIG_01B if SCALE == "0.1b" else CONFIG_04B

# [é‡è¦] è¿™é‡Œå¿…é¡»å’Œè®­ç»ƒæ—¶æ„å¤–è¦†ç›–çš„å‚æ•°ä¸€è‡´ï¼
# å¦‚æžœä½ è®­ç»ƒæ—¶ num_rwkv_experts=3 (æ„å‘³ç€æ€»å…±4ä¸“å®¶: 3R+1T)ï¼Œè¿™é‡Œå°±å¾—å¡«3
config['num_rwkv_experts'] = 3  
config['micro_batch_size'] = 1

print(f"ðŸ”„ Loading model from {MODEL_PATH}...")
print(f"âš™ï¸ Config: {config['num_rwkv_experts']} RWKV Experts + 1 Linear Trans")

model = CaMoE_System(config).to(DEVICE)

# å°è¯•åŠ è½½ï¼Œå®¹å¿ä¸€äº›å½¢çŠ¶ä¸åŒ¹é…ï¼ˆå¦‚æžœæ˜¯ä¸“å®¶æ•°å¯¼è‡´çš„ï¼‰
checkpoint = torch.load(MODEL_PATH, map_location='cpu')
state_dict = checkpoint['model'] if 'model' in checkpoint else checkpoint

try:
    model.load_state_dict(state_dict, strict=True)
    print("âœ… Full strict load success.")
except Exception as e:
    print(f"âš ï¸ Strict load failed, trying non-strict... ({str(e)[:100]}...)")
    model.load_state_dict(state_dict, strict=False)
    print("âœ… Non-strict load success (Ignore this if generation works).")

model.eval()
tokenizer = TRIE_TOKENIZER(config['vocab_file'])

# ================= è¾…åŠ©å‡½æ•° =================
def sample_top_p(probs, p, temperature):
    probs = probs.pow(1.0/temperature)
    probs = probs / probs.sum()
    sorted_probs, sorted_indices = torch.sort(probs, descending=True)
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    probs = probs.masked_fill(indices_to_remove, 0.0)
    probs = probs / probs.sum()
    
    return torch.multinomial(probs, 1)

def analyze_token_preferences(history_log):
    """
    åˆ†æž Transformer åˆ°åº•å–œæ¬¢åƒä»€ä¹ˆè¯
    """
    trans_heavy_tokens = []
    trans_light_tokens = []
    
    for item in history_log:
        token = item['token'].strip()
        if not token: continue
        
        # ç»Ÿè®¡ç”¨äº† Trans çš„å±‚æ•°
        trans_layers = len(item['trans_layers'])
        if trans_layers >= 2: # åªè¦æœ‰2å±‚ä»¥ä¸Šç”¨äº† Trans
            trans_heavy_tokens.append(token)
        else:
            trans_light_tokens.append(token)
            
    heavy_counts = Counter(trans_heavy_tokens).most_common(10)
    
    print("\nðŸ§ [AI Analysis] Transformer's Favorite Tokens (Top 10):")
    print(f"è¿™äº›è¯æœ€å®¹æ˜“è§¦å‘ Trans: {heavy_counts}")

def generate_and_visualize(prompt, max_new_tokens=200, temperature=1.0, top_p=0.85):
    input_ids = tokenizer.encode(prompt)
    x = torch.tensor([input_ids], dtype=torch.long).to(DEVICE)
    
    print("\n" + "="*20 + " GENERATION START " + "="*20)
    print(f"Prompt: {prompt}\n")
    print("-" * 50)
    print(prompt, end="", flush=True)
    
    # ç»Ÿè®¡æ•°æ®
    total_generated = 0
    global_trans_count = 0
    layer_trans_counts = {i: 0 for i in range(config['n_layer'])}
    
    # AI åˆ†æžæ—¥å¿—åˆ—è¡¨
    analysis_log = []
    
    with torch.no_grad():
        for _ in range(max_new_tokens):
            # Padding
            curr_ctx = x[:, -config['ctx_len']:]
            B, T_actual = curr_ctx.shape
            remainder = T_actual % CHUNK_LEN
            if remainder != 0:
                pad_len = CHUNK_LEN - remainder
                x_padded = F.pad(curr_ctx, (0, pad_len), value=0)
            else:
                x_padded = curr_ctx
            
            # Forward
            # step=30000 ç¡®ä¿ Eureka å…³é—­ï¼Œå®Œå…¨çœ‹ Router
            logits, info = model(x_padded, step=30000, phase="normal") 
            
            # Sampling
            target_idx = T_actual - 1
            next_token_logits = logits[:, target_idx, :]
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = sample_top_p(probs, top_p, temperature)
            
            # è·¯ç”±ç»Ÿè®¡
            transformer_id = config['num_rwkv_experts'] # æœ€åŽä¸€ä¸ª ID æ˜¯ Trans
            active_layers = []
            
            for layer_idx, layer_winners in enumerate(info["winners"]):
                # layer_winners: [B, T]
                # æ³¨æ„ï¼šå¦‚æžœä½¿ç”¨äº† Paddingï¼Œtarget_idx åº”è¯¥æ˜¯ä¸å« padding çš„ç´¢å¼•
                # ä½†å› ä¸ºæˆ‘ä»¬åªå–æœ€åŽä¸€ä¸ªç”Ÿæˆçš„ï¼Œè¿™é‡Œå– target_idx å³å¯
                # (å¦‚æžœ forward å†…éƒ¨åšäº† padding å¤„ç†ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦å¯¹é½ï¼Œ
                # ä½†æ ¹æ®ä½ çš„ä»£ç ï¼Œinfoè¿”å›žçš„æ˜¯å¯¹é½åŽçš„ï¼Œé€šå¸¸å–æœ€åŽä¸€ä¸ªæœ‰æ•ˆä½)
                
                # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å– info['winners'] çš„å¯¹åº”ä½ç½®
                # å¦‚æžœ padding äº†ï¼Œinfo çš„é•¿åº¦æ˜¯ T_padded
                # æˆ‘ä»¬çš„ target_idx æ˜¯ T_actual - 1
                
                winner_id = layer_winners[0, target_idx].item()
                if winner_id == transformer_id:
                    layer_trans_counts[layer_idx] += 1
                    active_layers.append(layer_idx)
            
            # å¯è§†åŒ–é¢œè‰²
            if len(active_layers) > 0:
                global_trans_count += 1
                color = 'red'
            else:
                color = 'cyan'
            
            total_generated += 1
            
            try:
                word = tokenizer.decode([next_token.item()])
            except:
                word = ""
                
            print(colored(word, color), end="", flush=True)
            
            # è®°å½•åˆ°æ—¥å¿—
            analysis_log.append({
                "token": word,
                "trans_layers": active_layers
            })
            
            x = torch.cat([x, next_token.view(1, 1)], dim=1)
            if next_token.item() == 0: break
    
    print("\n" + "-" * 50)
    
    if total_generated > 0:
        print(f"\nðŸ“Š Global Stats: {total_generated} tokens")
        print(f"ðŸ”µ RWKV Token: {total_generated - global_trans_count}")
        print(f"ðŸ”´ Trans Token: {global_trans_count} ({global_trans_count/total_generated:.1%})")
        
        print("\nðŸ” Layer-wise Transformer Usage:")
        for i in range(config['n_layer']):
            pct = layer_trans_counts[i] / total_generated
            bar_len = int(pct * 20)
            bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
            print(f" L{i:02d} | {pct:.1%} | {bar}")

        # === è¿™é‡Œçš„è¾“å‡ºå‘ç»™æˆ‘ ===
        analyze_token_preferences(analysis_log)
        
        print("\nðŸ“‹ Raw Token Dump (Copy this to Analysis):")
        print("[")
        for i, item in enumerate(analysis_log):
            # åªæ‰“å°æœ‰ Trans ä»‹å…¥çš„ï¼Œæˆ–è€…æ¯éš”å‡ ä¸ªæ‰“å°ä¸€ä¸‹ï¼Œé˜²æ­¢å¤ªé•¿
            # è¿™é‡Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            clean_token = repr(item['token'])
            layers = item['trans_layers']
            if len(layers) > 0:
                print(f"  {{'t': {clean_token:<10}, 'L': {layers}}},")
            else:
                pass # çº¯ RWKV çš„å°±ä¸æ‰“å°äº†ï¼Œçœç©ºé—´ï¼Œé™¤éžä½ æƒ³çœ‹ä¸Šä¸‹æ–‡
        print("]")

# ================= æµ‹è¯• =================
prompts = [
    "Once upon a time, there was a little girl named Lily.",
    "The king was very sad because he lost his crown.",
]

for p in prompts:
    generate_and_visualize(p)