import os
import sys
import multiprocessing
from datasets import load_dataset

# ç¯å¢ƒé…ç½®
sys.setrecursionlimit(10000)
sys.path.append(os.path.join(os.path.dirname(__file__), "tokenizer"))
from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER

# é…ç½®è·¯å¾„
VOCAB_FILE = "tokenizer/rwkv_vocab_v20230424.txt" 
DATA_DIR = "./data/dev" # å­˜æ”¾é‚£ä¸€å † .train æ–‡ä»¶çš„æ–‡ä»¶å¤¹
SAVE_PATH = "./data/dev_processed"
CTX_LEN = 1024 

print(f"ğŸš€ Initializing Tokenizer...")
global_tokenizer = TRIE_TOKENIZER(VOCAB_FILE)

def process_and_pack(examples):
    all_token_ids = []
    for text in examples["text"]:
        # å¤„ç†ç©ºè¡Œ
        if not text or not text.strip():
            continue
        
        # ç¼–ç ã€‚æ¯ä¸ªæ®µè½åé¢å¼ºè¡ŒåŠ ä¸€ä¸ª EOS(0) 
        # è¿™æ ·å³ä½¿æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæ¨¡å‹ä¹Ÿèƒ½é€šè¿‡ 0 çŸ¥é“è¿™æ˜¯ä¸åŒæ®µè½
        ids = global_tokenizer.encode(text) + [0]
        all_token_ids.extend(ids)
    
    # æ ¸å¿ƒï¼šå°†è¶…é•¿åˆ—è¡¨åˆ‡æˆ CTX_LEN çš„å—
    # æ¯”å¦‚ 2500 ä¸ª token ä¼šåˆ‡æˆä¸¤ä¸ª 1024ï¼Œå‰©ä¸‹ 452 ä¸ªä¸¢å¼ƒï¼ˆæˆ–ç•™ç»™ä¸‹ä¸€ä¸ªbatchï¼‰
    # åœ¨ BabyLM è¿™ç§ç¢æ•°æ®ä¸Šï¼Œè¿™èƒ½æå‡ 10 å€è®­ç»ƒæ•ˆç‡
    output = []
    for i in range(0, len(all_token_ids), CTX_LEN):
        chunk = all_token_ids[i : i + CTX_LEN]
        if len(chunk) == CTX_LEN:
            output.append(chunk)
    
    return {"input_ids": output}

def main():
    # [å…³é”®ä¿®æ”¹] è·å–æ–‡ä»¶å¤¹å†…æ‰€æœ‰ .train æ–‡ä»¶
    files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith(".dev")]
    print(f"ğŸ“‚ Found {len(files)} BabyLM files.")

    # å‘Šè¯‰ load_dataset è¿™æ˜¯æ–‡æœ¬æ ¼å¼
    dataset = load_dataset("text", data_files={"train": files}, split="train")
    
    print(f"ğŸ“Š Raw data total lines: {len(dataset)}")

    print("âš™ï¸ Tokenizing & Packing (Multiprocessing)...")
    
    n_proc = min(os.cpu_count(), 8)
    
    # æ‰§è¡Œå¤„ç†
    tokenized_dataset = dataset.map(
        process_and_pack,
        batched=True,
        batch_size=1000, 
        num_proc=n_proc,
        remove_columns=dataset.column_names,
        load_from_cache_file=False 
    )

    print(f"ğŸ’¾ Saving to: {SAVE_PATH}")
    tokenized_dataset.save_to_disk(SAVE_PATH)
    
    # è®¡ç®—æœ€ç»ˆæ•ˆç‡
    final_tokens = len(tokenized_dataset) * CTX_LEN
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆè®­ç»ƒæ ·æœ¬æ•°: {len(tokenized_dataset)}")
    print(f"ğŸ“Š æœ‰æ•ˆ Token æ€»é‡: {final_tokens / 1e6:.2f} M Tokens")

if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()