import os
import sys
import multiprocessing
from datasets import load_dataset

# ==========================================
# [Trick 1] æš´åŠ›æ‹‰é«˜é€’å½’ä¸Šé™ï¼Œé˜²æ­¢ dill æŠ¥é”™
# ==========================================
sys.setrecursionlimit(10000)

# å¯¼å…¥æœ¬åœ° Tokenizer
sys.path.append(os.path.join(os.path.dirname(__file__), "tokenizer"))

try:
    from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
except ImportError:
    try:
        from rwkv_tokenizer import TRIE_TOKENIZER
    except ImportError:
        print("âŒ Error: æ‰¾ä¸åˆ° rwkv_tokenizer.py")
        sys.exit(1)

# é…ç½®
VOCAB_FILE = "tokenizer/rwkv_vocab_v20230424.txt" 
LOCAL_TXT_FILE = "./data/TinyStoriesV2-GPT4-train.txt" # æˆ–è€…æ˜¯ä½ çš„ Parquet æ–‡ä»¶è·¯å¾„
SAVE_PATH = "./data/slimpajama_6b_processed"

# ==========================================
# [Trick 2] æŠŠ Tokenizer åˆå§‹åŒ–æ”¾åˆ°å…¨å±€ä½œç”¨åŸŸ
# è¿™æ · Windows å­è¿›ç¨‹ä¼šè‡ªå·±åŠ è½½å®ƒï¼Œä¸éœ€è¦ pickle ä¼ è¾“
# ==========================================
print(f"ğŸš€ Initializing Tokenizer globally...")
if os.path.exists(VOCAB_FILE):
    global_tokenizer = TRIE_TOKENIZER(VOCAB_FILE)
else:
    print(f"âš ï¸ Warning: æ²¡æ‰¾åˆ° {VOCAB_FILE}ï¼Œå¦‚æœæ˜¯ä¸»è¿›ç¨‹ä¼šæŠ¥é”™")
    global_tokenizer = None

def process(examples):
    # ç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ global_tokenizer
    all_ids = []
    for text in examples["text"]:
        # æ³¨æ„ï¼šä½ çš„ txt å¯èƒ½æœ‰ç©ºè¡Œï¼ŒåŠ ä¸ªåˆ¤æ–­
        if not text.strip():
            continue
            
        try:
            ids = global_tokenizer.encode(text)
        except Exception:
            continue # è·³è¿‡æ— æ³•ç¼–ç çš„è¡Œ
            
        if len(ids) > 1024:
            ids = ids[:1024]
        
        # åªæœ‰éç©ºæ‰åŠ è¿›å»
        if len(ids) > 0:
            all_ids.append(ids)
        
    return {"input_ids": all_ids}

def main():
    if global_tokenizer is None:
        print("âŒ æ— æ³•å¯åŠ¨ï¼šè¯è¡¨æ–‡ä»¶ä¸¢å¤±")
        return

    print(f"ğŸ“‚ Loading Data from: {LOCAL_TXT_FILE}")
    
    # è‡ªåŠ¨è¯†åˆ« txt æˆ– parquet
    if LOCAL_TXT_FILE.endswith(".parquet"):
        dataset = load_dataset("parquet", data_files={"train": LOCAL_TXT_FILE}, split="train")
    else:
        dataset = load_dataset("text", data_files={"train": LOCAL_TXT_FILE}, split="train")
    
    print(f"ğŸ“Š Raw Dataset Size: {len(dataset)} rows")

    print("âš™ï¸ Tokenizing (Multiprocessing)...")
    
    # Windows ä¸‹ n_proc ä¸è¦å¼€å¤ªå¤§ï¼Œå¯åŠ¨å¼€é”€å¤§
    n_proc = min(os.cpu_count(), 8)
    
    tokenized_dataset = dataset.map(
        process,
        batched=True,
        batch_size=1000, 
        num_proc=n_proc, 
        # è‡ªåŠ¨åˆ é™¤æ—§åˆ—
        remove_columns=dataset.column_names 
    )

    print(f"ğŸ’¾ Saving to disk: {SAVE_PATH}")
    tokenized_dataset.save_to_disk(SAVE_PATH)
    print("âœ… Done! é©¬ä¸Šè¿è¡Œ python train.py å§ï¼")

if __name__ == "__main__":
    # Windows å¤šè¿›ç¨‹ä¿æŠ¤
    multiprocessing.freeze_support()
    main()