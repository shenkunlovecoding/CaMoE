import os
import sys
import argparse
import multiprocessing
from datasets import load_dataset, Dataset, DatasetDict

# =================é…ç½®åŒºåŸŸ=================
# å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™ï¼Œé˜²æ­¢ç½‘ç»œæŠ¥é”™
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# =========================================

# å¯¼å…¥ Tokenizer
sys.setrecursionlimit(10000)
try:
    sys.path.append(os.path.join(os.path.dirname(__file__), "tokenizer"))
    from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
except ImportError:
    # å°è¯•ç›´æ¥å¯¼å…¥ï¼ˆé€‚é…ä¸åŒç›®å½•ç»“æ„ï¼‰
    from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER

def get_args():
    parser = argparse.ArgumentParser(description="é€šç”¨æ•°æ®é›†å¤„ç†è„šæœ¬ (MiniPile/SlimPajama)")
    parser.add_argument("--token",type=str,help="HF Token")
    # æ ¸å¿ƒå‚æ•°
    parser.add_argument("--dataset", type=str, default="JeanKaddour/minipile", 
                        help="HuggingFace æ•°æ®é›†åç§° (ä¾‹å¦‚: JeanKaddour/minipile æˆ– Cerebras/SlimPajama-627B)")
    parser.add_argument("--split", type=str, default="train", 
                        help="è¦å¤„ç†çš„åˆ†æ”¯ (train, validation, test)")
    parser.add_argument("--name", type=str, default=None,
                        help="æ•°æ®é›†çš„å­é…ç½®åç§° (ä¾‹å¦‚ SlimPajama å¯èƒ½éœ€è¦)")
    
    # è·¯å¾„å‚æ•°
    parser.add_argument("--save_path", type=str, default="./data/minipile_processed", 
                        help="å¤„ç†åçš„æ•°æ®ä¿å­˜è·¯å¾„")
    parser.add_argument("--vocab", type=str, default="tokenizer/rwkv_vocab_v20230424.txt", 
                        help="è¯è¡¨è·¯å¾„")
    
    # å¤„ç†å‚æ•°
    parser.add_argument("--ctx_len", type=int, default=1024, help="ä¸Šä¸‹æ–‡é•¿åº¦ (Chunk Size)")
    parser.add_argument("--num_proc", type=int, default=16, help="è¿›ç¨‹æ•° (9950X å»ºè®® 16-24)")
    parser.add_argument("--batch_size", type=int, default=1000, help="æ‰¹å¤„ç†å¤§å°")

    return parser.parse_args()

# å…¨å±€å˜é‡ (ç”¨äºå¤šè¿›ç¨‹)
global_tokenizer = None
CTX_LEN = 1024

def init_worker(vocab_path, ctx_len):
    """å¤šè¿›ç¨‹åˆå§‹åŒ–å‡½æ•°"""
    global global_tokenizer, CTX_LEN
    global_tokenizer = TRIE_TOKENIZER(vocab_path)
    CTX_LEN = ctx_len

def process_and_pack(examples):
    # ã€Windows é˜²å‘†è¡¥ä¸ã€‘
    global global_tokenizer, CTX_LEN
    if global_tokenizer is None:
        # å¦‚æœå­è¿›ç¨‹é‡Œæ˜¯ç©ºçš„ï¼Œç°å……ä¸€ä¸ª
        from tokenizer.rwkv_tokenizer import TRIE_TOKENIZER
        # è·¯å¾„å¯èƒ½éœ€è¦å†™æ­»æˆ–è€…ä¼ å‚ï¼Œè¿™é‡Œå‡è®¾åœ¨é»˜è®¤ä½ç½®
        global_tokenizer = TRIE_TOKENIZER("tokenizer/rwkv_vocab_v20230424.txt")
        CTX_LEN = 1024 # é»˜è®¤å€¼
    """
    æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼š
    1. æ‰¾åˆ°æ–‡æœ¬åˆ—
    2. Tokenize + EOS
    3. Packing (æ‹¼æ¥ååˆ‡åˆ†)
    """
    # è‡ªåŠ¨å¯»æ‰¾æ–‡æœ¬åˆ—å (æœ‰çš„æ•°æ®é›†å« text, æœ‰çš„å« content)
    text_column = "text"
    if "text" not in examples:
        if "content" in examples:
            text_column = "content"
        else:
            # ç›²çŒœç¬¬ä¸€ä¸ªå…¨æ˜¯å­—ç¬¦ä¸²çš„åˆ—
            text_column = list(examples.keys())[0]

    all_token_ids = []
    
    for text in examples[text_column]:
        if not text or not isinstance(text, str) or not text.strip():
            continue
        
        # ç¼–ç å¹¶æ·»åŠ  EOS (0)
        # æ³¨æ„ï¼šRWKV Tokenizer åœ¨å¤šè¿›ç¨‹ä¸‹å¯èƒ½éœ€è¦å¼‚å¸¸æ•è·
        try:
            ids = global_tokenizer.encode(text)
            if ids:
                all_token_ids.extend(ids + [0])
        except:
            continue
            
    # Packing: åˆ‡åˆ†æˆå›ºå®šé•¿åº¦
    output = []
    for i in range(0, len(all_token_ids), CTX_LEN):
        chunk = all_token_ids[i : i + CTX_LEN]
        # åªä¿ç•™å®Œæ•´çš„å—ï¼Œä¸¢å¼ƒæœ€åä¸€ç‚¹ç‚¹å°¾å·´ (æ•°æ®é‡å¤Ÿå¤§æ—¶å¯å¿½ç•¥)
        if len(chunk) == CTX_LEN:
            output.append(chunk)
            
    return {"input_ids": output}

def main():
    args = get_args()
    
    print(f"ğŸš€ å‡†å¤‡å¤„ç†æ•°æ®é›†: {args.dataset} [{args.split}]")
    print(f"ğŸ“‚ ä¿å­˜è·¯å¾„: {args.save_path}")
    print(f"ğŸ§µ è¿›ç¨‹æ•°: {args.num_proc} | Context: {args.ctx_len}")

    # 1. åŠ è½½æ•°æ®é›†
    print("â˜ï¸  æ­£åœ¨ä» HuggingFace ä¸‹è½½/åŠ è½½æ•°æ®...")
    try:
        if args.name:
            ds = load_dataset(args.dataset, args.name, split=args.split,token=args.token)
        else:
            ds = load_dataset(args.dataset, split=args.split)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return

    print(f"ğŸ“Š åŸå§‹æ•°æ®é‡: {len(ds)} è¡Œ")

    # 2. å¹¶è¡Œå¤„ç†
    print("âš™ï¸  å¼€å§‹ Tokenizing & Packing...")
    
    # ä½ çš„ 9950X æœ‰ 32 çº¿ç¨‹ï¼Œè¿™é‡Œå¼€ 16-24 ä¸ªæ¯”è¾ƒåˆé€‚ï¼Œç•™ç‚¹ç»™ç³»ç»Ÿ IO
    n_proc = min(os.cpu_count(), args.num_proc)
    
    tokenized_dataset = ds.map(
        process_and_pack,
        batched=True,
        batch_size=args.batch_size,
        num_proc=n_proc,
        remove_columns=ds.column_names, # ç§»é™¤åŸå§‹æ–‡æœ¬åˆ—ï¼Œåªç•™ input_ids
        fn_kwargs={}, # ä¸éœ€è¦ä¼ å‚ï¼Œé€šè¿‡ init_worker åˆå§‹åŒ–å…¨å±€å˜é‡
        load_from_cache_file=True, # å¯ç”¨ç¼“å­˜ï¼Œé˜²æ­¢å´©äº†é‡è·‘
        desc="Processing"
    )

    # 3. ä¿å­˜
    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°ç£ç›˜: {args.save_path}")
    tokenized_dataset.save_to_disk(args.save_path)
    
    # 4. ç»Ÿè®¡
    final_tokens = len(tokenized_dataset) * args.ctx_len
    print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆæ ·æœ¬æ•°: {len(tokenized_dataset)}")
    print(f"ğŸ“Š æœ‰æ•ˆ Token æ€»é‡: {final_tokens / 1e9:.4f} B Tokens")

if __name__ == "__main__":
    multiprocessing.freeze_support()
    
    # è·å–å‚æ•°å¹¶åˆå§‹åŒ–å…¨å±€å˜é‡ (è¿™æ­¥å¾ˆå…³é”®ï¼Œè¦åœ¨ map ä¹‹å‰åš)
    args = get_args()
    
    # Hack: è¿™ç§å†™æ³•åœ¨ Windows spawn æ¨¡å¼ä¸‹å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
    # ä½† datasets çš„ map å‡½æ•°æœ‰è‡ªå·±çš„åˆå§‹åŒ–æœºåˆ¶ï¼Œ
    # æˆ‘ä»¬è¿™é‡Œé€šè¿‡é—­åŒ…æˆ–è€…ç®€å•çš„å…¨å±€è®¾ç½®æ¥åš
    # æœ€ç¨³å¦¥çš„æ–¹å¼æ˜¯æŠŠ init é€»è¾‘æ”¾è¿› map å†…éƒ¨ï¼Œæˆ–è€…åˆ©ç”¨å…¨å±€ä½œç”¨åŸŸ
    
    # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®ä¸€ä¸‹å…¨å±€å˜é‡ï¼Œä¾› Windows å­è¿›ç¨‹ fork/spawn æ—¶ä½¿ç”¨
    # æ³¨æ„ï¼šWindows ä¸‹ spawn ä¼šé‡æ–° import è¿™ä¸ªæ–‡ä»¶ï¼Œæ‰€ä»¥éœ€è¦åœ¨ if __name__ å¤–é¢ä¹Ÿæœ‰ä¸€éƒ¨åˆ†é€»è¾‘
    # æˆ–è€…ç®€å•ç‚¹ï¼Œç›´æ¥åœ¨ process_and_pack é‡Œå®¹é”™ã€‚
    
    # ä¿®æ­£ï¼šdatasets åº“åœ¨ Windows ä¸‹ä¼ é€’ Tokenizer å¯¹è±¡ä¼šå¾ˆéš¾å—ï¼ˆpickle é—®é¢˜ï¼‰ã€‚
    # æœ€å¥½çš„åŠæ³•æ˜¯è®©æ¯ä¸ªå­è¿›ç¨‹è‡ªå·±é‡æ–°åŠ è½½ Tokenizerã€‚
    # æˆ‘ä»¬ä½¿ç”¨ .map çš„ new_fingerprint å‚æ•°æˆ–è€… init æŠ€å·§ï¼Œ
    # ä½†æœ€ç®€å•çš„å°±æ˜¯åˆ©ç”¨ `process_and_pack` é‡Œçš„ global_tokenizerã€‚
    # ä¸ºäº†è®©å®ƒç”Ÿæ•ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª wrapperã€‚
    
    # é‡æ–°å®šä¹‰å¸¦ init çš„ map è°ƒç”¨ï¼š
    # å®é™…ä¸Š datasets åº“ç›®å‰å¤„ç†å¤šè¿›ç¨‹ä¼ å‚æ¯”è¾ƒæ™ºèƒ½ï¼Œåªè¦ global_tokenizer èƒ½è¢« pickle å³å¯ã€‚
    # RWKV çš„ TRIE_TOKENIZER åº”è¯¥æ²¡é—®é¢˜ã€‚
    
    # æ‰‹åŠ¨åˆå§‹åŒ–ä¸»è¿›ç¨‹çš„ tokenizer
    init_worker(args.vocab, args.ctx_len)
    
    main()